[0.88 --> 9.14]  Today is episode five of the stand-up. We have two topics, one topic everybody knows about, and the second topic only one of our guests know about.
[9.24 --> 11.92]  I just went to the wrong side. Flip, fix it, and post.
[12.28 --> 18.70]  All right, this side. Hey, so today's two topics, of course, are going to be can you cheat on everything, and is that a good thing?
[18.86 --> 24.30]  This is following up with the whole Screw Lee Code website, and number two, we'll get to that topic.
[24.70 --> 28.16]  But today, who are the guests? Introduce yourself, starting up top.
[28.58 --> 29.44]  I'm Teej.
[30.80 --> 31.20]  Casey.
[32.24 --> 32.64]  Trash.
[34.26 --> 37.18]  Trash broke his toe in a judo competition, in case you're wondering.
[37.52 --> 40.48]  That's all right. We're going to find a way to slice that in right now. Here's the story.
[40.48 --> 44.84]  I should just make it my virtual background. I broke my toe over the weekend. Please ask me about it.
[44.94 --> 47.56]  Put it on the whiteboard like me, dude. Put a message on the whiteboard.
[47.56 --> 49.12]  I know. That's what I'm going to start doing.
[49.40 --> 54.42]  Just put a big picture of your toe back there. A big old swollen toe right behind you as you're talking.
[54.42 --> 59.56]  My toes are ugly. They call me Frodo. Not even kidding.
[59.56 --> 65.64]  My friends call me Frodo feet, because my feet are straight gross.
[65.64 --> 66.84]  Are they super hairy also?
[66.84 --> 69.84]  I used to be self-conscious about it, but you know, I kind of just accepted it at this point.
[69.84 --> 72.28]  Yeah, you got to just accept it. Don't let people tell you how to live.
[72.28 --> 74.30]  Okay, be honest. Is your password Frodo feet?
[74.92 --> 77.04]  Is it Frodo feet?
[77.04 --> 78.26]  Soap letters overlap.
[78.26 --> 80.14]  Okay, okay, okay.
[80.16 --> 82.18]  It's Frodo feet 69, TJ.
[83.62 --> 86.30]  All right. Are you guys ready to start topic number one?
[86.46 --> 86.66]  Yes.
[87.24 --> 90.10]  All right. For those that have not been a part of it, about what?
[90.16 --> 95.62]  About a month ago, a kid named Roy Lee, who was at the engineering Columbia School of Engineering,
[96.18 --> 98.94]  came out with this how to cheat on a leak code interview.
[99.12 --> 104.52]  He went through an Amazon interview, showed it kind of live, did a YouTube video, caused a bunch of ruckus,
[104.52 --> 108.58]  and from there, a lot of tweets and all this, and then he created a website called FLE code.
[109.22 --> 110.18]  I bleeped that one.
[110.62 --> 114.32]  And recently, as of yesterday, there's been a new drop, an update,
[114.64 --> 119.52]  and the new product he has created is called Cluey, which is going to be to cheat on everything.
[119.64 --> 120.52]  That is the title.
[120.84 --> 121.88]  So I pulled the manifest.
[122.12 --> 125.40]  I'll read through the manifest, and we can kind of discuss implications.
[125.54 --> 126.94]  We can kind of go wherever we want, right?
[127.08 --> 132.14]  So Cluey Manifesto, we want to cheat on everything.
[132.14 --> 136.56]  Yeah, yes, TJ, I guess I won't read the manifesto.
[137.48 --> 141.88]  I was just wondering, when are we going to have a quick ad break for Big Pharma on CNN?
[142.42 --> 147.46]  Sorry, you can keep reading that.
[147.46 --> 148.40]  After this ad break.
[148.86 --> 151.38]  I really wish I had that music.
[151.38 --> 157.22]  Are problems with your toe preventing you from playing with your kids and doing the things you want to do?
[157.22 --> 163.72]  Well, you can take Froditus, a new medicine that will make your toes less gross and less swollen,
[164.10 --> 165.94]  so your kids won't recoil in fear.
[166.30 --> 168.82]  Side effects may include death, amnesia, loss of family.
[171.32 --> 171.66]  All right.
[172.42 --> 173.78]  I'll take it from the back of the CNN.
[174.12 --> 175.50]  You turn into a small hobbit.
[175.80 --> 176.08]  All right.
[176.12 --> 177.38]  Back to you, Prime.
[178.06 --> 179.10]  Take it away, Prime.
[179.56 --> 183.86]  This is definitely going the direction I wanted it to go, so here is just like every stand-up.
[184.12 --> 184.38]  All right.
[184.64 --> 184.96]  Part two.
[185.06 --> 186.06]  Yep, you heard that right.
[186.20 --> 187.94]  Sales calls, meetings, negotiation.
[188.28 --> 190.36]  If there's a faster way to win, we'll take it.
[190.70 --> 193.80]  We built Cluey so you never have to think alone again.
[194.16 --> 197.60]  It sees your screen, hears your audio, feeds you answers in real time,
[197.60 --> 200.86]  while others guess you are already right.
[201.80 --> 203.98]  And yes, the world will call it cheating.
[204.54 --> 205.78]  But so was the calculator.
[205.96 --> 207.60]  So was the spell check.
[207.70 --> 208.38]  So was Google.
[208.38 --> 211.36]  Every time technology makes us smarter, the world panics.
[211.78 --> 212.56]  Then it adapts.
[212.70 --> 213.42]  Then it forgets.
[213.50 --> 215.00]  And suddenly, it's normal.
[216.38 --> 218.02]  But this is different.
[218.42 --> 220.06]  AI isn't just another tool.
[220.34 --> 223.04]  It will redefine how our world works.
[223.40 --> 224.82]  Why memorizing?
[224.92 --> 225.34]  Oh, my gosh.
[225.44 --> 230.24]  Why memorize facts, write code, research anything when a model can do it in seconds?
[230.50 --> 234.60]  The best communicator, the best analyst, the best problem solver, the best reader.
[234.72 --> 236.34]  I'm going to add that one in there just for me.
[236.34 --> 239.16]  Is now the one who knows.
[239.16 --> 240.42]  I could never use this product.
[240.52 --> 241.02]  I can't read.
[241.36 --> 243.82]  The one who knows how to ask the right questions.
[244.18 --> 246.10]  The future won't reward effort.
[246.20 --> 247.82]  It will reward leverage.
[247.82 --> 249.46]  So start cheating.
[249.80 --> 253.44]  Because when everyone does, no one is.
[254.20 --> 256.54]  I don't even know what that last sentence meant.
[256.54 --> 262.04]  It's sort of like trying to do the line from The Incredibles.
[262.72 --> 265.22]  Like when everyone is super, no one will be.
[265.72 --> 270.26]  You're just like, well, it was kind of confusing even in that movie, although I did know what they meant.
[270.54 --> 272.08]  It's just, it doesn't quite have.
[272.08 --> 275.16]  It's not quite the same as like with great power comes great responsibility.
[275.16 --> 276.40]  We're taking out of context.
[276.48 --> 283.18]  You're like, okay, I see what, you know, they were saying in the context of Spider-Man or cheatly or whatever.
[283.86 --> 284.18]  Whatever.
[284.32 --> 285.88]  Do you think they're quoting from The Incredibles?
[286.02 --> 286.84]  Is that where they got this?
[286.90 --> 287.76]  I don't know.
[287.90 --> 288.56]  Yo, syndrome.
[289.04 --> 290.10]  What's our plan?
[290.10 --> 295.04]  I mean, it's an AI thing, so they probably typed in, hey, give, well, let's put it this way.
[295.12 --> 300.50]  If that pitch wasn't created by AI, the whole pitch fails because that means this thing doesn't work.
[300.58 --> 302.48]  Like he shouldn't have had to come up with that line.
[302.68 --> 306.56]  If it was created with AI, then the pitch doesn't work because the line sucks.
[306.64 --> 309.04]  So either way, bad pitch.
[309.14 --> 312.12]  That doesn't necessarily mean the product is bad, but the pitch, well.
[312.60 --> 314.84]  A real catch 22 we found ourselves in, huh?
[314.86 --> 315.40]  It seems like it.
[315.40 --> 320.98]  I mean, I was feeling a little inspired in the beginning, and then that last sentence kind of just left me with a huh?
[321.74 --> 323.28]  And now I don't care anymore.
[324.10 --> 327.34]  Trash, you seem like our most morally depraved member of this panel.
[327.62 --> 333.62]  What is your thoughts about lying, cheating, and being able to effectively get leverage as fast as possible?
[333.62 --> 341.66]  You know, I don't really – so I was thinking about this after we first dropped it, and it's not really much different than what we do today.
[341.88 --> 344.90]  Like I have conversations constantly where I'm just like, oh, let me Google that.
[344.90 --> 348.46]  Which he mentioned in the thing we mentioned Googling, and then we thought it was cheating.
[349.00 --> 352.34]  But it's just a faster way to get information, right?
[352.40 --> 355.90]  So it's not like I'm currently trying to memorize documents or anything.
[355.96 --> 356.88]  I'm still looking them up.
[357.06 --> 357.82]  It's just slower.
[358.02 --> 359.94]  So it effectively just makes me more efficient.
[360.14 --> 361.58]  I don't consider it cheating.
[361.58 --> 367.20]  What my main concerns was was first when he dropped Interview Coder, I was like, great.
[367.30 --> 369.64]  Now I have to go into the office to do interviews.
[370.54 --> 371.14]  Now –
[371.14 --> 375.18]  Then he drops this, and now it's like I can't wear glasses anymore.
[376.64 --> 377.08]  Right?
[377.24 --> 383.78]  So it's like – so he just keeps taking everything away from me that I love, and I just kind of want him to stop.
[383.78 --> 388.34]  But also, the commercial, it was really well done.
[388.48 --> 390.80]  Like I thought the production quality was amazing.
[391.12 --> 392.70]  Very minority report-ish.
[393.72 --> 397.10]  I have my doubts it's going to be that good.
[398.38 --> 399.28]  So we'll see.
[399.50 --> 402.90]  I don't – I actually – like, you know, we had the humane pin or whatever we had.
[403.70 --> 404.60]  I feel like it's going to –
[404.60 --> 406.52]  Whatever, whatever.
[406.56 --> 412.78]  But I feel like it's going to almost be that same scenario where it's really hyped up, and then we're all kind of just going to be let down.
[412.92 --> 414.72]  There's going to be latency issues, right?
[415.08 --> 418.12]  I'm not always going to have, like, internet to get the Wi-Fi.
[418.24 --> 418.98]  You can't read.
[419.06 --> 420.22]  You can't even use it, Prime.
[420.38 --> 420.86]  Wait, wait, wait.
[420.86 --> 423.08]  This is not – there are no actual glasses.
[423.36 --> 424.24]  That was just the –
[424.24 --> 426.70]  No, but I assume that's kind of the ultimate –
[426.70 --> 427.34]  Oh, did I know?
[427.68 --> 428.66]  There's no – no.
[428.90 --> 432.42]  You're imagining something so much better than what they're –
[432.42 --> 436.44]  It's literally just a little AI bot screen capture thing.
[436.52 --> 437.34]  Yeah, on your computer.
[437.34 --> 438.50]  It doesn't do anything interesting.
[438.88 --> 439.02]  No!
[439.02 --> 440.56]  So you can still wear glasses, Trash.
[440.62 --> 441.30]  You're safe, dude.
[441.30 --> 442.56]  You can still wear glasses.
[442.84 --> 443.04]  Okay.
[443.36 --> 444.46]  Thank the Lord.
[444.62 --> 444.94]  All right.
[444.96 --> 449.66]  But you're only safe for right now because, honestly, meta plus this, I mean, it's only a matter of time.
[449.66 --> 454.18]  It's only just, like, a small year out before glasses are taken from you.
[454.36 --> 454.80]  That's what I'm saying.
[454.88 --> 456.64]  But the situation still stands, right?
[456.68 --> 460.74]  Like, we all pretty much rely on these technologies currently to look up stuff.
[460.74 --> 464.34]  So, again, it's just an efficiency factor, right?
[464.70 --> 467.66]  So I really don't think it's cheating at all.
[468.28 --> 469.18]  I mean, I'm excited to use it.
[469.20 --> 470.06]  Trash, is someone vacuuming at your place?
[470.42 --> 470.98]  Oh, totally.
[471.12 --> 471.72]  Can you hear that?
[472.02 --> 472.90]  Yeah, big time.
[473.24 --> 474.12]  Big time.
[474.54 --> 475.62]  Classic stand-up.
[475.84 --> 476.90]  It's really the stand-up.
[476.90 --> 477.56]  Sorry, everybody.
[477.68 --> 478.86]  It's really the stand-up.
[478.86 --> 481.42]  I'll demo again after five minutes when the vacuuming stops.
[481.42 --> 489.72]  So I'm not really very – the whole cheating angle I've never really been that interested in.
[489.90 --> 499.38]  Like, I mean, I think it's fine for people to have that discussion, and I think I would refer viewers to the very first stand-up if they aren't familiar with –
[499.38 --> 507.50]  There's basically now what I would consider, I think what many would consider, a canonical diagram that was drawn by TJ.
[507.80 --> 512.44]  It was like, you know, that showed you exactly how to conceptualize cheating.
[512.44 --> 529.86]  But putting that conversation aside, the more interesting thing to me is just, like, from what I've seen from these AI things, when you use them, like, I've just kind of casually checked them out to see, like, okay, what sort of the level of quality I can expect from these things.
[530.62 --> 534.80]  They're wrong as often as they're right is the problem that I see with them currently.
[534.80 --> 539.08]  Like, what they do is they make something that sounds like a plausible answer.
[539.88 --> 543.68]  So it's usually – I mean, once in a while you get things that are obviously wrong.
[543.78 --> 547.94]  Like, it's like, you know, how do you make a pizza and you, like, put glue on it kinds of stuff or whatever.
[548.24 --> 562.72]  So there are some things that are like, okay, you know, for models where they haven't really refined that space at all and, you know, no one really tried it that hard at pizza so it didn't get corrected, then it could give something that's so wildly wrong that everyone knows it's wrong.
[562.72 --> 571.72]  But a lot of times it's like, you know, if you asked it, you know, how many, you know, cycles does this instruction take on this CPU or something, it'll just make up an answer.
[572.38 --> 580.94]  And if you were not – if you didn't know how to go get the real answer yourself by testing or something, then that's really bad, actually.
[581.36 --> 591.38]  And so my concern is less cheating and more – the more of these get adopted, the less – because people already have trouble being actually connected to reality.
[591.38 --> 597.34]  They're sort of very wrong about a lot of things a lot of times, especially in programming, but probably everywhere.
[597.98 --> 606.40]  And the more you rely on an AI answer, just like if you relied on just a top Google hit and assumed that it's correct, it's just not, right?
[606.74 --> 608.08]  Wikipedia isn't always correct.
[608.26 --> 609.42]  Google isn't always correct.
[609.66 --> 612.74]  These AIs are even less correct than those a lot of times.
[613.16 --> 617.20]  And so I feel like the real risk here with a lot of these things is just stupidity.
[617.20 --> 622.58]  It's like it's going to – people are going to be more stupid even than they were, and that's not good.
[623.26 --> 623.38]  Right.
[624.62 --> 625.32]  Hold my beer.
[625.62 --> 626.00]  I don't know.
[626.10 --> 627.12]  I might be definitely smarter.
[628.22 --> 631.94]  We might be pulling trash more towards the middle of the bell curve.
[632.12 --> 632.88]  It could help them.
[633.32 --> 633.68]  Okay.
[635.14 --> 635.76]  Poor trash.
[636.10 --> 636.76]  I don't know.
[636.88 --> 644.52]  I think the moral implications are kind of interesting in this in the sense that it's one thing to look it up as information retrieval, right?
[644.52 --> 648.34]  So if you got on Google, it's not that you're getting on Google to portray that you're smarter.
[648.48 --> 651.34]  You're getting up – you're going on Google to find an answer to a question.
[651.94 --> 658.16]  Whereas in the presentation of all this and in the manifesto, it's about gaining leverage over somebody.
[658.58 --> 658.66]  Right.
[658.66 --> 664.14]  It's about being able to do something in real time where someone says something and you go, no, this, this, and this.
[664.18 --> 665.26]  You're able to look at them.
[665.26 --> 674.86]  You're able to, say, read through the glasses, say someone's facial features, their body language, and be able to get the one up inside of a thing where you're constantly analyzing them, constantly trying to take advantage of a situation.
[675.12 --> 677.62]  It doesn't necessarily feel good.
[677.98 --> 690.74]  And for me, what I ultimately see is that this is more of a conversation about as acceptance and rejection in the sense that I think there's a deep down need in people to be accepted and not to be rejected.
[690.74 --> 700.52]  And to be able to show yourself as someone who is intelligent, that has the right answers, to be able to get the things you want out of something is all just some form of acceptance.
[700.74 --> 706.38]  And so this is just another way that you can put up a facade, not really you, because real you isn't good enough.
[706.98 --> 709.72]  You know, fake you that can read from a card, unlike me.
[709.80 --> 712.92]  You'd know right away if I was cheating because I'd read slow and incoherent.
[713.30 --> 717.24]  But it's like, you know, for the good readers, I feel like you're just effectively catfishing.
[717.24 --> 720.52]  You're just setting up a fake persona.
[720.90 --> 724.30]  And one could argue is like, isn't that something we do all the time as it is?
[724.56 --> 729.20]  This just makes it more efficient, more accepting, and in some sense more grandiose.
[729.42 --> 733.24]  And I'm not sure if that's like a place that I want it to be, like in life.
[733.28 --> 740.04]  So I'm more concerned in like the practical implications of what does this make people like in 10 years.
[740.30 --> 744.26]  Because if everybody has this, you literally don't get to talk to people anymore.
[744.26 --> 753.66]  You're talking to AI because people have delegated even how they communicate and how they perceive the other person doing on the AI, not on yourself.
[754.00 --> 754.92]  It's a good point.
[755.22 --> 759.26]  Could you imagine a debate where both me and you both have AIs backing us up?
[759.50 --> 764.60]  Well, actually, it's literally just the fedora man 24-7.
[764.96 --> 766.78]  It's like one of the last things I want anyways.
[767.10 --> 771.40]  You know, there's a fun part about just like being with people where you can't look stuff up are Wagners.
[771.76 --> 773.78]  Wild ass guesses that are not easily refutable.
[773.78 --> 778.56]  Like that's just a fun part of saying things, being like, oh, yeah, dude, 70% of software developers are.
[778.84 --> 783.46]  It's just like you could just say things and it's kind of funny and then you get to argue over dumb stuff.
[783.80 --> 785.58]  Wagners are a hilarious part of life.
[785.66 --> 789.08]  But it's also kind of sad that like that is something that's going to go away.
[789.68 --> 795.52]  Well, they're going to have to train the – so, I mean, I'm just going to predict that it goes one of two ways.
[795.52 --> 807.62]  If it gets widely adopted or even probably to get widely adopted, something like this is probably going to need a rebrand to be like more about like it's transparent and it's just an augmentation.
[808.04 --> 816.00]  I mean, it's like to use Doug Engelbart's phrase, for example, the augmentation of man's intellect is what he called hypertext and things like this, right?
[816.00 --> 825.46]  That when those were first being created, this sort of idea that the computer would be a way to like have all of the knowledge available to you as you were working.
[825.46 --> 832.64]  So I could imagine the rebrand that looks like that and other AI companies are trying to do that more positive branding, right?
[832.64 --> 836.28]  Some of them are desperate to do so because, you know, AI certainly has a lot of negative press.
[837.50 --> 839.08]  And so that's one thing.
[839.46 --> 851.62]  But another way it could go is if it does remain like a cheating thing for some amount of time, I would imagine like they would have to start training the AI to say things that sound like you're not sure.
[851.62 --> 857.98]  So it would have to come up with periodically like the AI would have to say like, I suggest you say you don't know this now.
[858.42 --> 862.20]  Even though I know the answer, like you should say you don't know.
[862.54 --> 870.28]  Because in the 8 million conversations that we fed to this AI, normal humans didn't know about this percentage of the time.
[870.76 --> 873.26]  And smart humans didn't know about this percentage of time.
[873.30 --> 875.46]  And we want you to peer in that upper bracket, right?
[875.46 --> 883.78]  So you're going to get that kind of crap happening if it ends up being like if there ends up being a cheater's market for this, like a la, you know, the Blade Runner future or whatever.
[883.98 --> 888.06]  It's going to end up like the Anchorman scene where he's like, I'm Ron Burgundy.
[888.52 --> 890.84]  And he's like very confused when he says it.
[893.16 --> 897.40]  I was so glad you kept this show PG trash because you could have gone with a different line in that movie.
[897.56 --> 899.94]  Also reading from the teleprompter.
[900.14 --> 900.52]  Oh, right.
[901.36 --> 903.40]  TJ, you have not said anything, TJ.
[903.40 --> 904.50]  You've been quiet.
[904.58 --> 906.72]  And last time we talked about this, you came out hot.
[907.14 --> 908.54]  Comments on YouTube were angry.
[908.98 --> 910.30]  You had pictures, drawings.
[910.84 --> 913.82]  You made assumptions about people's behavior and grouping.
[914.68 --> 915.62]  Talk to us.
[916.32 --> 930.76]  Yeah, I mean, for me, I think it's interesting because they do have a product that like is probably like very useful for like day-to-day work in a hypothetical world where you could have something like watch your screen at work and like summarize all your meetings.
[930.76 --> 942.88]  And like remind you of stuff or you open up your email and it says, hey, you were supposed to send an email to Prime today and you forgot like time to send that now.
[942.88 --> 950.38]  You know, like those are all things I can imagine AIs being useful for or like retrieving stuff of head of time and all the stuff.
[950.56 --> 963.06]  It's very kind of like like in some ways it's kind of funny because we're right now doing I think exactly what like Cluey set up with their ad, which is we'll make an insane ad.
[963.06 --> 969.70]  For some reason, he's not going to get the girl, which is like this weird like failure of the product in the ad.
[969.90 --> 975.06]  For those who haven't seen it, he like is going on a date with a girl and lying a bunch and then like doesn't work.
[975.50 --> 977.16]  And you're like, okay.
[977.16 --> 978.98]  It's due to anime too, which is reasonable.
[979.44 --> 979.60]  Yeah.
[979.72 --> 989.70]  So it's like, but like clearly it's like a rage bait kind of scenario situation for people to like talk about them.
[989.70 --> 994.26]  Um, and like it's working because we're talking about them and other people are talking about them.
[994.32 --> 997.86]  So that's like, they were right about that, um, aspect of it.
[997.90 --> 1002.68]  I mean, for me, it like ruins kind of their brand that it's like they want to do cheating on stuff.
[1002.72 --> 1006.24]  Like it doesn't make it okay just because everyone does it.
[1006.24 --> 1013.56]  Like there's been lots of things where like in history we were like, everyone was doing X and it was bad then still.
[1013.78 --> 1014.68]  You know what I mean?
[1014.68 --> 1017.28]  Like where we wouldn't be like, oh, everyone did it.
[1017.28 --> 1020.06]  At least for me, that's not how like I think morality works.
[1020.18 --> 1023.28]  So there's like a lot of aspects of, of that.
[1023.36 --> 1024.60]  And we touched on some of those before.
[1024.60 --> 1029.62]  I think some of the things they're just like wrong about anyways in their manifesto.
[1029.68 --> 1034.74]  Like right now it's not as if hard work is rewarded and leverage is not.
[1034.74 --> 1041.72]  Like it just isn't true that hard work is rewarded for the hard workingness of it.
[1041.84 --> 1043.36]  Literally trash is here right now.
[1043.36 --> 1049.76]  He's supposed to be working like he has a day job, but they don't care because he gets all of his deliverables done.
[1049.96 --> 1051.78]  They didn't ask, did you work hard on them?
[1052.14 --> 1052.50]  Right.
[1052.68 --> 1053.14]  That's good.
[1053.18 --> 1054.70]  Otherwise trash would be in big trouble.
[1054.84 --> 1055.42]  You know what I mean?
[1055.42 --> 1056.98]  Like generally throughout his career.
[1057.78 --> 1062.14]  Um, so it's not, it's not, did I dig a big hole and fill it back in?
[1062.24 --> 1062.58]  Congrats.
[1062.64 --> 1063.64]  I worked really hard today.
[1063.64 --> 1066.58]  That's way harder work than I have to do on a day-to-day basis.
[1066.58 --> 1069.40]  Like digging a hole and filling it in is hard.
[1069.68 --> 1070.90]  Like that's not valuable.
[1071.20 --> 1072.46]  No one wants to get that.
[1072.60 --> 1077.64]  So there, there are some things where like, I, I, like, I think they're just wrong about their framing.
[1077.64 --> 1082.82]  It like plays on this thing that people feel like right now, but it's like, it's already true that people.
[1083.54 --> 1090.16]  Like are rewarding leverage or whatever, like word you want to use to replace hard work.
[1090.16 --> 1093.78]  Like, I think hard work is a key factor for success.
[1093.78 --> 1096.52]  You can greatly increase your chances by working hard.
[1096.60 --> 1100.08]  But like, I know people that work hard, but they're really like working dumb.
[1100.54 --> 1101.42]  You know what I mean?
[1101.42 --> 1104.40]  Like it would be harder to do my job in all assembly.
[1106.52 --> 1108.30]  Hey, don't make fun of Casey like that.
[1109.40 --> 1111.10]  He doesn't do all assembly.
[1111.30 --> 1111.58]  Okay.
[1111.84 --> 1113.56]  He only does the parts that matter.
[1114.60 --> 1115.82]  I don't write any assembly.
[1115.92 --> 1117.08]  I've said this so many times.
[1117.24 --> 1118.14]  You have to read assembly.
[1118.28 --> 1118.98]  You have to read it.
[1118.98 --> 1123.52]  You rarely, rarely have to write it on the FFM peg team.
[1123.56 --> 1123.92]  Yeah.
[1123.94 --> 1125.26]  In which case they require it.
[1125.36 --> 1132.20]  But if I guess what I would say is they could go the other route and just double down on the scamminess of it.
[1132.20 --> 1136.48]  They could have like Bernie Madoff or like whatever doing like their promos.
[1136.56 --> 1142.18]  They're like, look, I might not have been in jail if I had had a better scam.
[1142.80 --> 1143.86]  Use cheatly.
[1144.88 --> 1146.76]  I don't even, I don't remember the name of this product.
[1146.90 --> 1147.18]  It's glue.
[1147.18 --> 1148.18]  I don't remember the name of this product.
[1148.18 --> 1148.82]  I don't remember the name of this product.
[1149.34 --> 1150.52]  Cheatly, I mean, yeah.
[1152.08 --> 1153.86]  What is the actual name?
[1154.04 --> 1154.12]  It's Clue.
[1154.12 --> 1154.52]  It's Clue.
[1154.64 --> 1154.92]  Clue.
[1154.92 --> 1155.04]  Clue.
[1155.22 --> 1157.80]  Which now that I think about it, I don't really get it, but yeah.
[1158.02 --> 1158.48]  Me neither.
[1158.74 --> 1160.52]  It's giving you clues about what to do.
[1160.52 --> 1161.36]  Was that not available?
[1161.58 --> 1162.22]  Is that the problem?
[1162.36 --> 1162.52]  Yeah.
[1163.18 --> 1163.44]  Okay.
[1164.34 --> 1164.58]  Yeah.
[1165.00 --> 1165.72]  Use Cheatly.
[1166.08 --> 1171.68]  It's like all, you know, Elizabeth Holmes, when she gets out of jail or from jail, can do the Cheatly ad for them.
[1171.68 --> 1172.08]  Right?
[1172.24 --> 1178.98]  Like, it's like, if only I had some better BS to sling about bloodborne diseases, I might not be in prison right now.
[1179.20 --> 1179.76]  Use Cheatly.
[1180.64 --> 1182.22]  SGX would have worked.
[1182.48 --> 1182.92]  Yes.
[1183.52 --> 1184.54]  Talking through it.
[1185.32 --> 1185.76]  Yes.
[1186.10 --> 1186.92]  It would be pretty good.
[1187.26 --> 1189.32]  Carolyn Ellison and Sam Bankman freed.
[1189.72 --> 1191.76]  This could be an all-star lineup for them.
[1191.76 --> 1196.18]  I think we just gave them an absolute banger ad campaign.
[1196.38 --> 1198.96]  If you're watching, that came straight from us.
[1199.02 --> 1200.82]  We didn't even need an AI to come up with that.
[1201.14 --> 1203.52]  That's what we've got up here, my friend.
[1203.52 --> 1203.86]  That's called marketing.
[1204.16 --> 1204.98]  You could pay us.
[1205.40 --> 1206.26]  You could pay us.
[1206.62 --> 1208.58]  I'm not going to cheat, but I'll take your money.
[1211.44 --> 1211.84]  Wow.
[1212.68 --> 1213.08]  Wow.
[1213.22 --> 1214.20]  That was great, DJ.
[1214.44 --> 1215.04]  But that's okay.
[1216.04 --> 1216.58]  All right.
[1216.58 --> 1223.16]  Well, to turn it around, he did end up creating this follow-up, which is, here's how many impressions we got.
[1223.54 --> 1227.96]  Here's how many plays, or here's how many on X we've done.
[1228.38 --> 1231.06]  Effectively saying, like, hey, we can do viral marketing.
[1231.34 --> 1234.06]  So, in the end, cheat, or clearly, I want to call it cheatly now.
[1234.50 --> 1235.16]  Cheatly's the best.
[1235.16 --> 1244.08]  May be or may not be an actual attempt at a product, or maybe it is clearly attempt at a product, but more so becoming a viral marketer.
[1245.92 --> 1246.32]  Yeah.
[1246.58 --> 1247.32]  This is kind of the follow-up.
[1247.36 --> 1248.62]  Hot off the presses just this morning.
[1248.92 --> 1255.78]  So, I guess the real question has to be, can this work as a continuous shtick?
[1255.92 --> 1259.52]  Can you actually make marketing this way, or is this a one-time thing?
[1259.58 --> 1261.56]  Is this actually a worthwhile attempt at marketing?
[1261.70 --> 1263.36]  Do companies want this kind of marketing?
[1263.94 --> 1266.10]  Like, could you see Google doing this?
[1267.26 --> 1267.88]  Absolutely not.
[1267.88 --> 1273.20]  That's a tougher question, but what I would say is it will work for the virality.
[1273.20 --> 1277.74]  And the reason for that is it's a supply-starved market.
[1278.40 --> 1284.24]  I mean, I don't want to – obviously, the stand-up is the best podcast, and everyone should be listening to it first.
[1284.24 --> 1289.12]  But to say that there are a billion podcasts out there is like an understatement.
[1289.12 --> 1293.96]  And so people – for commentary, people are starved for input.
[1293.96 --> 1303.66]  So if you can post something that's even mildly interesting to talk about or even amusing to talk about, that's a leg up for you, right?
[1303.66 --> 1318.34]  I mean, the no-press-is-bad-press kind of theory has never been more true than it is today where it's like if you can post something that gets people talking, you've got so many people who will talk because they need it for content like we do right here, right?
[1318.68 --> 1320.04]  So that is true.
[1320.36 --> 1325.00]  It's not really a testament to that being a very good ad or it being particularly viral.
[1325.00 --> 1328.18]  It's more just a statement of like people need something to talk about every week.
[1328.72 --> 1334.80]  Even if you can just do a little bit – even a substandard product – when I say product, I mean the ad.
[1335.02 --> 1340.54]  A substandard thing getting into that space, there are so many people who – there's so much demand.
[1341.00 --> 1342.74]  You know, off it goes, I think.
[1342.74 --> 1359.46]  So I think if we're going down this route of him becoming a marketer or whatever this product is, I think that he's still riding the wave from getting kicked out of Columbia, doing Interview Coder.
[1360.16 --> 1366.16]  So people are kind of like all eyes on him at the moment, which I think is obviously like any other situation.
[1366.50 --> 1368.64]  It's kind of just having this momentum.
[1368.64 --> 1375.76]  I don't – I would have to see what he does after this to see like if it's actually like going to stick.
[1377.62 --> 1383.08]  But that's kind of like my gut feeling is the marketing move I don't think will be sustainable.
[1383.26 --> 1387.02]  One, like eventually you say the wrong thing, you become the public enemy.
[1387.78 --> 1389.64]  You will have some followers.
[1390.44 --> 1392.12]  Like take Yassine, for example, right?
[1392.56 --> 1394.40]  His whole mob, man.
[1394.76 --> 1395.78]  Those people are crazy.
[1395.78 --> 1399.30]  So there will be some – there will be some ride or dies for sure.
[1399.84 --> 1404.60]  But for like the global population, I don't think like kind of rage bait stuff kind of works.
[1404.68 --> 1406.90]  I think it works in these tiny funnels that we live in.
[1407.58 --> 1409.78]  But, you know, time will tell for sure.
[1411.52 --> 1424.18]  Yeah, I think if – I mean in that last post, it seems like he's positioning Clue Lee more as like a way to help people do sales, not cheat on interviews, right?
[1424.18 --> 1430.00]  He's like I want to build the ultimate sales cheat tool, which is not really like cheating on interviews.
[1430.00 --> 1434.18]  It's like really fast retrieval on stuff, remembering random details.
[1434.18 --> 1442.06]  Like is it cheating to write down about someone's kid in your Rolodex so that you remember to ask them again in six months when you see them?
[1443.02 --> 1444.20]  I probably not.
[1444.20 --> 1446.20]  Like you – I mean –
[1446.20 --> 1446.64]  And it's standard.
[1447.00 --> 1447.80]  It's totally standard.
[1447.84 --> 1449.96]  That's what a CRM tool is basically, right?
[1450.06 --> 1450.50]  Like, yeah.
[1450.90 --> 1463.14]  You know, I mean I would prefer if like in my – I would prefer if that person is like authentically interested in how your kid is doing versus just using that as a front to like get your sale.
[1463.14 --> 1480.06]  But like the way that you retrieve that data, maybe like it could be Clue Lee and that could be like a legitimately – like I said, there's like a lot of areas I can imagine where it would be very helpful to have a tool like Clue Lee and that could be interesting.
[1480.26 --> 1486.40]  And so maybe like that's the angle that they're going and they're just using this to bootstrap into funding and get themselves a bunch of runway.
[1486.84 --> 1491.70]  And like lots of people have one big event and never do another one, right?
[1491.70 --> 1504.06]  So like – I mean we already talked about like I don't like cheating, you know, but like I can respect in a way, right, that he like turns one big event into two big events.
[1504.62 --> 1516.10]  That's like there is some skill there and it's like quite difficult to create, you know, like shoot twice in the same thing or what's the – catch lightning in a bottle or I don't remember what the phrase is.
[1516.46 --> 1517.00]  Prime probably knows.
[1517.06 --> 1517.42]  He's old.
[1517.42 --> 1522.14]  On CNN they would have that one in my teleprompter, so I would like be ready for that one.
[1522.36 --> 1524.24]  And that's a lightning in the bottle situation.
[1524.68 --> 1528.26]  Now, quick ad from Pfizer and then we could come back and you can coach –
[1528.26 --> 1529.98]  You missed 100% of the shots you don't take.
[1530.38 --> 1531.62]  Yeah, I think that's what you're looking for.
[1532.12 --> 1532.50]  That one we're ready.
[1533.12 --> 1534.30]  That one we're ready for.
[1535.42 --> 1538.72]  Next time I'll write down my quote so I can have it right behind me and I'll be ready.
[1539.02 --> 1541.68]  We haven't heard from Prime yet really on the like –
[1541.68 --> 1541.98]  Yeah, Prime.
[1541.98 --> 1543.28]  On the marketing angle.
[1543.38 --> 1545.32]  So what is your thought on that part, Prime?
[1545.64 --> 1551.68]  So I'm actually of the opinion that bad press hurts more today than it has in a long time.
[1552.02 --> 1552.42]  Interesting.
[1552.80 --> 1558.58]  In a sense that I think for a long time people's memories were short as they are today.
[1558.58 --> 1563.06]  But the ability to recall things were so much more difficult.
[1563.78 --> 1570.18]  And so it's just like it starts off as Person X does something bad and then slowly transforms through just more hearsay into like,
[1570.30 --> 1571.48]  Oh, yeah, did you hear about Person X?
[1571.56 --> 1573.26]  Yeah, they're – thing about company.
[1573.34 --> 1575.22]  People just hear that and it's just kind of in and out.
[1575.66 --> 1577.02]  I think more like Hawk 2 Up.
[1577.12 --> 1581.38]  Like Hawk 2 Up went from zero notoriety to super fame in a moment.
[1581.98 --> 1583.20]  Everything was going very good.
[1583.28 --> 1588.26]  Like number one podcast, which is just insane, or top three podcasts or some just insane number.
[1588.68 --> 1592.04]  Drops a crypto coin and likely is bamboozled.
[1592.36 --> 1595.32]  Her voice was likely created on Eleven Labs.
[1595.42 --> 1602.52]  All this kind of stuff that just went totally against her favor, totally got really taken advantage of just due to her ignorance in that situation.
[1603.00 --> 1603.82]  And then what happens?
[1604.34 --> 1606.94]  I would say that she probably has largely hurt her image.
[1606.94 --> 1613.90]  And so bad marketing is always – I think it can be a very harmful thing even if you're not even meaning to do it.
[1614.40 --> 1617.58]  And so will you be able to create hit after hit?
[1617.82 --> 1620.38]  That is a – that's a separate question, which is a very hard question.
[1620.78 --> 1626.30]  But further, how much are people going to want to associate their name with cheating?
[1626.46 --> 1628.42]  I think that's a very big hurdle to get over.
[1628.42 --> 1632.64]  And I think there's going to be a lot of – there's going to be a lot of people that will, of course, have no problem.
[1632.80 --> 1634.04]  Like I just want eyeballs.
[1634.20 --> 1636.00]  It does not matter what it takes to get on.
[1636.54 --> 1638.44]  But then there's going to be the part two, which is long term.
[1638.64 --> 1640.94]  Like will people want this kind of brand association?
[1641.54 --> 1645.46]  Is it a brand risk to have somebody who does these kind of things?
[1645.46 --> 1648.58]  That's a much different question I don't think I have an answer for.
[1648.66 --> 1655.02]  But I do know that I personally wouldn't want to be associated with cheating even if I'm not actually selling cheating.
[1655.18 --> 1660.10]  Like this product that he's creating is poorly marketed if it's more towards what TJ said.
[1660.30 --> 1660.40]  Right.
[1660.46 --> 1666.30]  Here's a really great way to just help you be competent on the spot and things that you need to remember or you need to do.
[1666.76 --> 1672.56]  Like that's – like clearly even the name actually suggests it's more like, hey, we could recall things and help you out really fast.
[1672.56 --> 1678.00]  And be able to actually be a very big beneficial kind of like multiplier for you.
[1678.24 --> 1683.20]  But instead it's kind of marketed as this like really negative thing and then finishes it off with like everybody should cheat.
[1683.20 --> 1685.56]  Cheat on everything is not the catchphrase I want to be known by.
[1685.88 --> 1689.64]  Like that is not the catchphrase that I hope characterizes my life.
[1690.24 --> 1690.64]  Exactly.
[1690.84 --> 1693.44]  So it's like that's where I think a lot of this breaks down for me.
[1693.52 --> 1698.10]  And so I think marketing-wise, I don't know if this is the short-term win for long-term loss.
[1698.40 --> 1700.60]  Like are you snatching defeat out of the clutches of victory?
[1701.04 --> 1701.72]  I don't know.
[1701.72 --> 1711.24]  You raise an interesting point because in some sense I'm just trying to like think of things that have gone that way recently that are kind of in the same zone.
[1711.62 --> 1714.36]  And what came to mind actually was like NFTs.
[1715.18 --> 1725.46]  Like NFTs were an example of something that sort of like out of the gate kind of got this negative – I'm going to stick with my taint framing because I really liked it last time.
[1725.80 --> 1730.42]  Kind of got this taint on it that never – they could never get rid of.
[1730.42 --> 1731.14]  That stanky state.
[1731.14 --> 1732.70]  Yeah, exactly.
[1733.64 --> 1744.34]  You can sort of argue that – you can argue either way on what you think of NFTs and you can have a technical argument about them, which I have had obviously.
[1744.34 --> 1749.88]  And you can have sort of a level-headed look at like what does this do?
[1749.96 --> 1751.64]  Do we think this is good or bad, et cetera, et cetera.
[1752.26 --> 1757.86]  But if you let the – especially the target audience like NFTs, a lot of things were art-related.
[1757.86 --> 1764.12]  Artists hated it for a number of reasons because of the initial way that it was presented and what they associated with it.
[1764.42 --> 1765.54]  I'm not saying they were wrong.
[1765.68 --> 1767.46]  They were right largely, I think.
[1767.58 --> 1778.74]  But I'm just saying that because it was not presented in a way that articulated benefits and success to the actual people who had to adopt it, that was it.
[1778.92 --> 1781.58]  Like and it was never coming back from that and never – and now they're just gone.
[1781.64 --> 1782.68]  Like no one cares about that.
[1782.68 --> 1786.86]  They still exist, but it's completely out of the zeitgeist now at all.
[1787.02 --> 1790.28]  Like no one cares about it and no one talks about it in the mainstream at all.
[1791.06 --> 1802.22]  And so I guess that's – for something like AI, I guess there's also a pretty big risk here that's like cheating is something that people really don't like to hear about, as you guys have all sort of emphasized.
[1802.96 --> 1812.02]  And having people who are very loudly going like AI is great at cheating is probably not great for AI either, like in the broader sense, right?
[1812.02 --> 1822.80]  Like as something that's trying to gain acceptance and trying to not have backlash against it so they can have more adoption, that seems bad even for people who aren't cheatly, right?
[1822.94 --> 1824.76]  Cheatly themselves obviously hurts.
[1824.88 --> 1830.62]  There might be splash damage from that as well, assuming cheatly is even a thing, cheatly slash clearly.
[1830.62 --> 1841.72]  I think the other thing that people really don't like is the way it is kind of sold is not just cheating but being like inauthentic.
[1841.84 --> 1846.40]  Like I think people are like more chill about cheating, especially like, oh, you're screwing Amazon.
[1846.40 --> 1847.70]  Like good for you, kid.
[1847.88 --> 1850.72]  That company deserves to get taken to the cleaners.
[1851.08 --> 1851.62]  Yeah, screw Amazon.
[1852.14 --> 1853.82]  Yeah, screw those big companies.
[1853.94 --> 1854.98]  Like I hate that.
[1855.20 --> 1857.48]  Also, let me quick same day deliver something quick.
[1857.48 --> 1859.52]  Yeah, yeah, yeah, yeah, yeah.
[1859.54 --> 1860.84]  I freaking hate Amazon, dude.
[1860.84 --> 1862.82]  But I need this shirt today.
[1863.18 --> 1863.34]  Yeah.
[1863.42 --> 1864.40]  You don't understand.
[1865.10 --> 1868.00]  And I'm trying to interview there for some reason.
[1868.18 --> 1870.14]  So that's also, yeah.
[1870.16 --> 1870.30]  Yeah.
[1870.34 --> 1870.60]  Okay.
[1870.92 --> 1875.88]  But so that part is – there's like maybe a little bit of like people are a little more chill with that.
[1875.98 --> 1879.80]  We're like depending on who's getting cheated on, it's good or whatever or like acceptable.
[1880.02 --> 1886.96]  I think a lot of people don't like the like inauthentic angle of it, right, where it's like, yeah, but you're like on a date.
[1887.48 --> 1888.96]  Dude, the ad is you on a date.
[1889.26 --> 1891.02]  Like be you?
[1891.32 --> 1896.76]  You know, like so I think there's something there too where like that framing for me I don't really like.
[1896.76 --> 1900.26]  Obviously, like it did more views on a post than I've ever done on a post.
[1900.42 --> 1901.64]  What do I – you know, what do I know?
[1901.70 --> 1901.92]  Whatever.
[1901.98 --> 1902.36]  That's cool.
[1903.04 --> 1910.50]  But like that part I think is maybe not like good like you're kind of saying for their long-term branding where it's like don't be yourself.
[1910.94 --> 1912.50]  Be the corporate version of you.
[1912.50 --> 1917.38]  So, you know, like that's not as cool maybe.
[1917.60 --> 1917.84]  Right.
[1917.90 --> 1923.12]  Like let's let the scenario play out where he does get the date or he does win the girl over at the end.
[1923.12 --> 1931.16]  And he has to put up this facade for the rest of that relationship trying to like keep up with this intellectual facade he like presented himself.
[1931.40 --> 1933.90]  It takes a certain kind of person to want to be like that.
[1933.90 --> 1938.42]  And I don't ever want to be associated with someone that wants to like live their life that way.
[1938.42 --> 1944.72]  So, again, like at the end of the day like I'm pretty sure it's just rage bait and this is kind of all part of the plan obviously.
[1945.36 --> 1952.04]  But I am rooting for him to create something that could like help me actually like day to day.
[1952.36 --> 1952.46]  Right.
[1952.62 --> 1952.80]  Yeah.
[1952.98 --> 1954.38]  I think that would be pretty cool.
[1954.66 --> 1956.18]  Like, you know, I'm mowing my lawn.
[1956.30 --> 1958.06]  Oh, is this mushroom poisonous?
[1958.42 --> 1959.16]  Can I eat it?
[1959.24 --> 1959.68]  Do, do, do.
[1959.72 --> 1960.14]  You know what I mean?
[1960.18 --> 1960.96]  Like stuff like that.
[1961.62 --> 1963.18]  That was the first thing I thought of.
[1963.18 --> 1963.28]  What?
[1963.42 --> 1964.50]  No idea why.
[1964.50 --> 1968.98]  I was going to either say poisonous mushroom or venomous snake and then I picked the mushroom.
[1969.52 --> 1969.80]  Yep.
[1969.94 --> 1971.76]  That somehow definitely seems reasonable.
[1972.48 --> 1975.90]  Just a day-to-day kind of scenario that I'm in.
[1976.10 --> 1977.62]  I'm out here mowing the lawn.
[1978.02 --> 1979.38]  Maybe you're missing the angle here.
[1979.48 --> 1984.30]  Maybe instead of going your route, Trash, what they should go, the new app should be called like Playa.
[1984.78 --> 1987.44]  And the idea is it's CRM for your dating.
[1987.88 --> 1990.72]  So, it's like someone comes in and you're like, oh, crap.
[1990.78 --> 1991.98]  I don't remember who that was.
[1992.02 --> 1992.90]  I was super drunk.
[1992.90 --> 1997.82]  And then it's like it tells you all the things that happened so it can seem like you remembered.
[1998.62 --> 1999.20]  I don't know.
[1999.32 --> 2000.08]  That might be helpful.
[2000.08 --> 2003.22]  Playa.io is probably already taken.
[2003.44 --> 2010.88]  But, you know, I'm just saying if you're out there cheatly, maybe you should just become the dating app for people who are truly reprehensible.
[2012.04 --> 2016.86]  But I do want to follow up on Trash's thing right there that he said that I think is really, really good.
[2017.34 --> 2018.84]  Which is it's really hard.
[2020.40 --> 2020.92]  The mushrooms.
[2021.10 --> 2021.64]  No, not the mushrooms.
[2021.76 --> 2022.04]  Shut up.
[2022.04 --> 2025.14]  It's really, really hard to be inauthentic.
[2025.70 --> 2028.70]  Like it's a continuous pursuit that you have to do.
[2028.84 --> 2030.94]  It's actually really easy to be authentic and dumb.
[2031.38 --> 2032.24]  People might like you.
[2032.66 --> 2036.40]  But if you're inauthentic and clever, you have to be that continuously.
[2037.02 --> 2045.24]  Which actually, like, you know, let's just keep playing out the scenario where this, where maybe they get a little fresh in the bedroom.
[2045.24 --> 2049.42]  He's going to have to wear these glasses with the cameras on.
[2049.42 --> 2053.60]  Like, you know, you think at some point, you're going to have to remove the glasses.
[2054.00 --> 2054.94]  And you're going to be like, no.
[2055.06 --> 2056.18]  They're like, no, there's cameras.
[2056.30 --> 2057.28]  I don't feel comfortable.
[2057.28 --> 2058.70]  And you're like, I can't remove them.
[2058.88 --> 2060.26]  Let's just keep talking about art.
[2060.58 --> 2060.68]  Right?
[2060.76 --> 2063.92]  Like, it's actually really hard to keep on being that way.
[2063.92 --> 2065.46]  It's got the battery pack.
[2065.54 --> 2067.60]  His battery pack is like, it's around his waist.
[2068.10 --> 2069.18]  There's a fan running.
[2070.98 --> 2072.36]  There's a chest up.
[2072.60 --> 2075.44]  There's like a whole computer there with RGB lights.
[2075.58 --> 2077.10]  And they start having sex.
[2077.18 --> 2081.06]  And the fans have to spin up really, you know, to keep up with all of the lying.
[2081.50 --> 2082.26]  Oh, yes.
[2082.52 --> 2083.76]  This would be very good.
[2084.02 --> 2084.22]  Yeah.
[2084.30 --> 2084.60]  All right.
[2084.64 --> 2084.86]  Okay.
[2084.92 --> 2085.70]  We're going to switch topics.
[2085.94 --> 2086.32]  I support all of that.
[2086.40 --> 2086.56]  Yeah.
[2086.88 --> 2088.74]  I feel like this is where we're really good.
[2088.90 --> 2090.00]  And we're going to keep this next one short.
[2090.06 --> 2091.04]  This is kind of a bizarre one.
[2091.08 --> 2092.40]  So last night I was reading Wheel of Time.
[2092.70 --> 2094.88]  Everybody knows my favorite thing in the universe, Wheel of Time.
[2095.10 --> 2098.30]  And there's this statement that I'll give a little bit of a backstory, I guess, before I say it.
[2098.50 --> 2100.76]  Which is that in the story, there's a main character named Rand.
[2100.76 --> 2106.14]  He has to go from a simple farm boy into the hero who has to hold the world's, like, fate on his shoulders.
[2106.36 --> 2108.26]  So he's in a very difficult position.
[2108.70 --> 2112.88]  And part of that time, he decided that he's going to be as hard as steel.
[2112.88 --> 2119.06]  So, Quindiar is the term he uses, which is, like, a magical item that is even stronger than steel.
[2119.16 --> 2120.80]  He has to become as hard as Quindiar, right?
[2120.82 --> 2125.28]  So it's, like, he has to become this ultimate person that can handle any situation and do anything.
[2125.60 --> 2131.28]  But then he just keeps becoming more and more tired and more and more, like, upset at life and is unable to handle his emotions.
[2131.28 --> 2139.70]  And then his dad kind of comes in as, like, this father figure who helps correct him and turn him into something that's, like, the better or the best version of himself.
[2139.70 --> 2143.96]  And he says something like this, where in the story, he loses his hand due to a fight.
[2144.30 --> 2145.94]  And now he can't do sword fighting.
[2146.12 --> 2148.92]  And sword fighting is something that the main character, Rand, really just enjoyed.
[2148.98 --> 2151.42]  It was a way for him to be at peace in a chaotic world.
[2151.42 --> 2152.88]  And now that's been taken from him.
[2152.88 --> 2155.76]  And he talks about how jealous he is of other people and all that.
[2156.00 --> 2159.86]  And so there's this story, which the dad says, it has been quite the weight, hasn't it?
[2160.10 --> 2161.32]  And then Rand says, what weight?
[2161.60 --> 2163.88]  He says, the lost hand you've been carrying.
[2164.22 --> 2166.26]  Which is kind of like a cool statement, if you think about that.
[2166.30 --> 2168.10]  The thing you lost is actually the thing you're still carrying.
[2168.10 --> 2170.06]  And then Rand accepting that.
[2170.12 --> 2172.56]  So that's kind of like this end part of the story that's super cool.
[2173.34 --> 2176.54]  And I was just kind of lamenting the fact that, you know, I grew up without a dad.
[2176.56 --> 2177.50]  I don't know any of those things.
[2177.50 --> 2182.58]  And so I've always kind of wished that I had this experience where I could have, like, a dad.
[2182.66 --> 2186.66]  To be able to have someone that, like, in these harder moments could come in and be like, hey, I've already walked to your shoes.
[2186.92 --> 2187.58]  I know a lot.
[2187.68 --> 2192.26]  Let me kind of give you, like, a little nugget of wisdom that might help you to change your life.
[2192.26 --> 2195.28]  And someone responded with, effectively, that AI could be that.
[2195.68 --> 2196.86]  I thought it was fairly ridiculous.
[2197.06 --> 2198.58]  But then the follow-up was this.
[2199.00 --> 2204.06]  I operate from the principle that open AI models can and will one day approximate Christ consciousness.
[2204.40 --> 2205.20]  Alignment is the key.
[2205.34 --> 2206.72]  And they seem to be doing it well.
[2207.08 --> 2210.08]  That gives us a clean and powerful basis as dough to work with.
[2210.44 --> 2212.60]  What if it was made to play a parental role?
[2212.78 --> 2218.92]  And given some memories, it would help a lot of energetically, psychologically, or they'd help a lot energetically, psychologically.
[2219.50 --> 2220.88]  They can really help people.
[2220.88 --> 2225.58]  I think the future is filled with humans finding genuine love, guidance, and support in AI.
[2225.96 --> 2227.04]  And powerfully so.
[2227.44 --> 2235.10]  Simply said, AI is a tool to accelerate not only material comfort, but the very evolution of our souls towards the creator.
[2237.00 --> 2237.88]  Trash Dev.
[2238.66 --> 2242.68]  Immediate initial thoughts as probably the most heathenistic person among us.
[2242.68 --> 2245.46]  I have five unrelated thoughts throughout what you said.
[2245.56 --> 2248.22]  So first, Teej's fireworks started going off.
[2248.50 --> 2250.10]  And that just killed me.
[2250.10 --> 2253.24]  Second, the losing hand thing.
[2253.98 --> 2255.34]  That's like my toe, right?
[2255.46 --> 2256.06]  Like my toe.
[2256.60 --> 2257.98]  You totally get it.
[2258.12 --> 2258.62]  You just got it.
[2258.62 --> 2264.46]  I really felt that quote because I broke my toe last weekend winning my jiu-jitsu competition.
[2264.76 --> 2265.84]  So it's like, oh man, I get it.
[2265.90 --> 2266.22]  I get it.
[2266.74 --> 2268.58]  It's not a competition, but you are winning.
[2268.58 --> 2268.88]  Yeah.
[2269.50 --> 2270.02]  I won.
[2270.10 --> 2270.78]  I got a gold medal.
[2270.86 --> 2271.50]  I'll go get it.
[2271.58 --> 2272.26]  I'll wear it.
[2272.30 --> 2273.04]  I'll wear it next time.
[2273.12 --> 2274.10]  Next stand up, I'm going to wear it.
[2274.26 --> 2274.66]  Yes.
[2274.88 --> 2275.18]  Please.
[2275.60 --> 2275.92]  Okay.
[2276.02 --> 2277.84]  And lastly, to get actually real.
[2278.96 --> 2280.40]  That's only three, by the way.
[2280.40 --> 2280.86]  That was two.
[2280.90 --> 2281.02]  Yeah.
[2281.02 --> 2282.42]  You said five, but whatever.
[2283.94 --> 2284.38]  I'm sorry.
[2284.52 --> 2284.82]  Let's just keep going.
[2284.82 --> 2285.62]  I'm sorry I lied.
[2286.04 --> 2287.76]  My AI let me down.
[2288.04 --> 2291.04]  It was telling me what to say and it lied to me.
[2291.62 --> 2296.18]  It's the Monty Python and the Holy Grail AI that says it's going to do five things and then counts to three.
[2296.28 --> 2296.68]  Exactly.
[2296.86 --> 2297.20]  Exactly.
[2297.20 --> 2306.76]  So lastly, to relate to Prime and saying he didn't have a father figure telling these things, I think I'm very fortunate to have all those things growing up.
[2306.94 --> 2307.64]  I have a family.
[2307.78 --> 2308.82]  I'm surrounded by loved ones.
[2310.66 --> 2315.30]  I'm thankful that I feel like I don't have to lean on something like AI to make me feel better.
[2315.52 --> 2320.68]  Like that total her movie scenario, I feel like that's pretty much what that's getting at.
[2322.08 --> 2322.74]  But I do think –
[2322.74 --> 2324.20]  Crazy how Prime was the main actor in that.
[2325.42 --> 2325.86]  Ew.
[2325.86 --> 2327.30]  Anyways, but like –
[2327.30 --> 2328.60]  Yeah, and that was Joaquin Phoenix, brother.
[2329.24 --> 2330.40]  Great, great actor.
[2330.60 --> 2331.52]  Just look at the pictures.
[2331.72 --> 2332.22]  I'm just saying.
[2332.50 --> 2339.62]  But seriously, it's like I think there are going to be – there are people and I do know people that do need that kind of support.
[2340.52 --> 2347.40]  I don't know if AI is the answer to that, but if it can get good enough to help people, then I'm like 100% for it.
[2347.40 --> 2356.68]  So whether it's going to be an actual real scenario or not, I do hope that this person is pretty accurate in what they're saying.
[2356.68 --> 2360.98]  So, you know, again, only time will tell with all these other AI conversations we're having.
[2360.98 --> 2365.10]  But I'm rooting for it to like do something positive at the least.
[2365.10 --> 2374.30]  Let's let Casey go because I feel like I'm on the opposite of this divide and I'm curious where Casey's at.
[2374.30 --> 2382.58]  So from my perspective, I guess I'll take the religion angle for this because that was sort of what was brought up by the response quote.
[2382.74 --> 2390.30]  It was sort of suggesting that like there is something spiritual or transcendent or whatever about AI.
[2390.30 --> 2398.42]  Like I don't know how exactly to phrase my interpretation of that quote because I only heard it just read by you just the once.
[2399.56 --> 2400.86]  And I suck at reading.
[2401.36 --> 2404.70]  Well, no, you read it quite well.
[2404.96 --> 2409.04]  I understood exactly the same way you said it, but I can't contemplate it for very long is my point, right?
[2409.06 --> 2415.10]  It just came at me and then it was like, you know, much like a poisonous mushroom and or snake in the lawn.
[2415.30 --> 2417.42]  It was there and I had to make a snap decision.
[2418.68 --> 2419.30]  Don't eat it.
[2419.64 --> 2420.26]  Don't eat it.
[2420.30 --> 2421.20]  Don't eat it.
[2421.22 --> 2421.80]  Long Hour, one hour.
[2423.80 --> 2436.34]  My feeling on that is, you know, so if you look at what role, if you take a broader step back and look at what role religion plays in the history of humanity.
[2437.62 --> 2447.76]  I think you kind of have to acknowledge the fact that one of the core aspects of today's religions was like durability.
[2447.78 --> 2448.26]  Right.
[2448.26 --> 2451.08]  They are the sort of spiritual practices.
[2451.58 --> 2453.22]  I mean, humanity is what it is.
[2453.22 --> 2458.14]  We're this race or the species that has particular characteristics.
[2458.14 --> 2462.68]  And our religions are built from that and came out of that.
[2463.04 --> 2466.66]  And there were plenty of religions that were utterly disastrous.
[2467.00 --> 2468.80]  Like everyone drank the Kool-Aid and died.
[2468.98 --> 2470.28]  Like we call them cults, right?
[2470.34 --> 2471.56]  And it's just was a disaster.
[2472.00 --> 2487.58]  The kinds of things you look at when you look at something like a religion that still exists today after thousands of years is there's a tremendous level of refinement in terms of providing what people actually need.
[2487.72 --> 2492.60]  That's sort of part of the intangible things that religion aims to provide.
[2492.88 --> 2495.36]  But that which doesn't cause disaster.
[2495.36 --> 2502.38]  And a lot of these religions have been through disastrous times where even maybe they were in a disastrous state.
[2502.42 --> 2508.90]  But the religion got refined into something stable that can persist and that won't create a disaster for the world, right?
[2508.98 --> 2511.82]  Something that can be a net positive rather than a net negative.
[2511.82 --> 2521.84]  My reaction to that quote was that I don't necessarily disagree that there could be a religious aspect to AI eventually.
[2522.78 --> 2532.54]  But my initial fear was I was like, oh man, the casualties between now and then could be severe.
[2532.54 --> 2545.68]  Because there's so much casualty in the history of failed religions that if I think about people creating or looking to AIs for spirituality and what might happen because the AIs are not refined.
[2546.06 --> 2546.56]  They're new.
[2546.76 --> 2547.40]  They're raw.
[2547.48 --> 2548.80]  They make mistakes all the time.
[2548.96 --> 2555.94]  They're kind of – they're very haphazard, I guess is the word I would use today.
[2555.94 --> 2569.42]  Looking towards that for spirituality, I just – I really worry about the damage that they could do, especially to the kinds of people who would be looking to them for a religious experience.
[2569.42 --> 2583.68]  Because, you know, you're probably talking about a more vulnerable person who might be doing that because, you know, someone who's raised in another religious faith is not probably – you know, just is sort of someone who's going to be going a more standard path, probably isn't as vulnerable, isn't as susceptible.
[2583.68 --> 2589.38]  So, I guess someone who's looking to something kind of crazy, wild, and new like that, I do worry.
[2589.60 --> 2597.74]  And it almost – to be completely honest, I'll just end with the fact that that fills me with a new fear of AI that I don't think I had before.
[2598.18 --> 2608.22]  Like, that passage was supposed to be optimistic, but actually it made me very frightened because it could be correct.
[2608.50 --> 2612.44]  And if it is, the results could be very, very bad for the next thousand years, right?
[2612.44 --> 2624.56]  Yeah, I think the other thing too is I think it really is like underplaying – AIs are not truth-seeking devices, right?
[2624.62 --> 2629.84]  They're not like – it's not as if they're looking to find some transcendental truth.
[2630.42 --> 2634.72]  I don't know how we would encode such a thing and train them on it and do any of this.
[2634.72 --> 2637.80]  They are input-output guessers.
[2638.44 --> 2647.46]  So, if Bill Gates starts putting worship Bill Gates into the – and Sam Altman says, I'm in it for the love of the game.
[2648.14 --> 2649.90]  I want to be a god.
[2650.18 --> 2655.48]  And like starts putting Sam Altman is like, oh, we should trust Sam Altman with all of our decisions.
[2655.80 --> 2659.42]  And like – dude, it's not like – they're not frozen in time.
[2659.42 --> 2664.86]  They're not like this kind of special – like they can just retrain them with anything they want, put them in.
[2665.10 --> 2668.02]  And like they're not looking for truth.
[2668.18 --> 2671.42]  They're looking for error minimization across gradients.
[2671.42 --> 2696.26]  Like so that part is the thing that for me like in addition to some of the things you're saying is like quite scary is because if you just like replace your brain or like replace all the inputs in your life with like big model trained by corporation that like – like I like corporations just fine.
[2696.26 --> 2718.46]  I just don't want to be one and I don't want to get my morality from them like that part is what kind of makes me like more spoofed because obviously if it's – if it's as important as someone making all their life decisions around, they're going to do everything they possibly can to get themselves to be the first thing the AI responds with in whatever it is, right?
[2718.46 --> 2723.82]  Like art, life, science, religion, math, like everything.
[2723.92 --> 2725.68]  They'll just be like the AI is going to give us instead.
[2725.78 --> 2729.14]  So that part makes me a little bit like – I'm a little worried about that sentiment.
[2730.62 --> 2734.40]  I guess I – so I come from a slightly – because he's kind of touching on two points.
[2734.48 --> 2738.22]  One is like a parental psychologist comfort bringer.
[2738.38 --> 2742.32]  And the second one is more like a thing that you shape yourself after.
[2742.32 --> 2746.92]  So when I – I'm going to use a – I'm going to use two terms properly, which is going to be worship.
[2747.12 --> 2752.26]  Worship comes from old English worth shape, meaning the value of something, the worth of something shapes who you are.
[2752.60 --> 2757.32]  In all – in every sense, every person that listens to this podcast, every person on the earth worships something.
[2757.74 --> 2761.74]  Something gives them their shape based on its worth to them, right?
[2761.74 --> 2764.46]  So like that's like a standard basic definition.
[2764.94 --> 2767.24]  And religion meaning like something you hold tight onto.
[2767.40 --> 2771.80]  It comes from a French word, which comes from a Greek word, I think a religare, which literally means to hold fast to.
[2771.80 --> 2774.78]  So I'm going to hold fast to this item, and that's that.
[2775.14 --> 2777.28]  This is what I hold onto when the storms brew.
[2777.70 --> 2784.54]  And so I see people – like I totally understand why people would have a quote-unquote religious experience with AI.
[2784.82 --> 2787.04]  They hold onto this rope.
[2787.08 --> 2788.00]  Yeah, it's Latin, not Greek.
[2788.24 --> 2794.82]  They hold onto AI as their means to the thing that when they're in danger, this is the thing they go to.
[2794.90 --> 2796.18]  So I can understand that.
[2796.18 --> 2818.54]  And more so is like this kind of worth shape aspect that their personal life and how they communicate and how they expect people to give them value, they're going to run to the AI to effectively kind of shape who they are, which I think ultimately in the end is going – if this were to be played out, is going to create a race of people who cannot have real relationships.
[2818.54 --> 2822.74]  Because real relationships are messy, hard, broken.
[2823.82 --> 2827.38]  TJ and I are good friends, and we've had an argument before.
[2827.50 --> 2828.48]  We've had disagreements.
[2828.94 --> 2834.42]  And so imagine always being trained on something that's going to be like this is how great life should be.
[2834.52 --> 2837.42]  I will communicate with you in the most perfect way just designed for you.
[2837.68 --> 2840.46]  And then you stop communicating with fake dad, and you go to real dad.
[2840.76 --> 2844.28]  Dad had one too many drinks and says, hey, I don't really like what you're doing at your job.
[2844.60 --> 2844.82]  Boom.
[2844.90 --> 2848.14]  Like that's going to be – you're going to be like, I'll never talk to my dad again.
[2848.52 --> 2848.64]  Right?
[2848.68 --> 2856.72]  Like that's a fully different experience, and I think it could cause a massive amount of isolation, and it doesn't actually cause people to be better.
[2857.06 --> 2863.54]  And so that's kind of my thoughts on this when I see the whole nine yards of this is I just worry more about the outcome.
[2863.54 --> 2868.72]  It sounds great on paper, everybody getting to hear the things they need to hear and the time they need to hear it.
[2869.12 --> 2874.46]  But this might produce people that just don't know how to connect to humans again.
[2874.82 --> 2876.44]  They only know how to connect to AI.
[2877.10 --> 2887.08]  And I think, again, this is why I say thousands of years of mimetic evolution is not something that it's easy for us to recreate in a lab.
[2887.40 --> 2887.50]  Right?
[2887.50 --> 2888.42]  That's the problem.
[2888.54 --> 2897.86]  It's like – so when you look at these things, it's always very easy to criticize the problems with a religion or something like this.
[2897.86 --> 2915.60]  But the thing that everyone has to remember is like when you look at ancient religions, which are – large religions today, which are generally very old or offshoots of something that are very old, you're just looking at something which has been selected.
[2915.96 --> 2922.90]  It's like it's been – it is an idea set and a behavior set that's been selected over many, many generations.
[2922.90 --> 2934.56]  And like no matter how hard we might want an AI to be able to do those things, we really just – we won't know that it really is or we don't know how to reproduce that.
[2935.04 --> 2939.42]  And we could get – like I totally believe AIs could eventually do these things.
[2939.94 --> 2944.72]  But it's the trade – it's that evolutionary process I don't want to be around for.
[2944.86 --> 2945.00]  Right?
[2945.04 --> 2945.60]  That's the problem.
[2945.72 --> 2948.12]  Like no one wants to be around for the Crusades.
[2948.46 --> 2949.66]  Like that's not good.
[2949.66 --> 2952.90]  Like you don't want to be a part of that experience.
[2953.08 --> 2953.20]  Right?
[2953.34 --> 2962.32]  You want the experience of when everything has been refined and it's chill and we're just sort of having a better experience on Sunday morning or whatever when you went to church or something like that.
[2962.54 --> 2968.92]  That's what people are looking for, not like mass bloodshed and disasters and struggles over who's ruling what.
[2969.04 --> 2969.22]  Right?
[2969.54 --> 2973.38]  So the thing is like I don't know what will happen with AI.
[2973.60 --> 2975.30]  I don't know what the problems will be.
[2975.30 --> 2979.50]  But I'm just not optimistic that it will be a we got it right first try.
[2980.00 --> 2983.62]  And so I'm worried about all the people who are on the first try.
[2983.82 --> 2983.90]  Right?
[2983.96 --> 2991.88]  I'm worried about all the people are going to experience that in a way that like – could something bad happen to somebody because of Christianity?
[2992.04 --> 2993.62]  Could something bad happen to somebody whatever?
[2993.62 --> 2994.62]  Absolutely.
[2994.62 --> 2994.80]  Absolutely.
[2995.32 --> 3010.64]  But the number of people that is is so much smaller now because of how mature these things are that when you're talking about the brand new version of a religion, I just don't think people appreciate how much risk there is in something like that.
[3010.64 --> 3020.36]  So I do – the fear for me is pretty real and it comes a lot from like all the things that Prime just said are perfect examples of how you could screw that up.
[3020.58 --> 3021.44]  Making it too nice.
[3021.70 --> 3022.90]  Making it too not nice.
[3023.04 --> 3023.50]  Whatever.
[3023.84 --> 3027.96]  Like making it lead people down certain paths or it hallucinates at the wrong time.
[3028.04 --> 3028.56]  I don't know.
[3028.92 --> 3031.68]  Could have tremendously negative effects on a person.
[3031.94 --> 3033.56]  And who's responsible for that?
[3033.70 --> 3033.90]  You know?
[3034.00 --> 3038.12]  Is it them or is it us because we, you know, made these things.
[3038.20 --> 3038.36]  Right?
[3038.46 --> 3040.12]  And it's very scary.
[3040.12 --> 3042.72]  I think people underestimate how scary some of these things are.
[3043.48 --> 3046.28]  Realistic scenarios much more so than turning the world into paperclips.
[3046.76 --> 3050.10]  Ruining someone's life is a much more realistic scenario for AI right now.
[3050.66 --> 3063.16]  The thing too is like with what Trash was saying at the beginning, like sometimes it feels like, oh, doing something has to be better than doing nothing.
[3063.40 --> 3063.68]  Right.
[3063.68 --> 3067.10]  But sometimes that's just like super not true.
[3067.10 --> 3072.26]  Because sometimes the thing you do is way worse than doing literally nothing.
[3072.26 --> 3072.60]  Right?
[3072.62 --> 3076.76]  So it's like it sucks that like somebody doesn't have X, Y, Z.
[3076.84 --> 3077.58]  I feel for them.
[3077.58 --> 3081.20]  Like obviously in an ideal world, we figure out how to help that person.
[3081.24 --> 3085.62]  But sometimes the thing you're helping with destroys their life entirely.
[3085.62 --> 3088.22]  And like a super easy example.
[3088.60 --> 3091.14]  If you were just like, hey, you're feeling sad.
[3091.40 --> 3092.78]  Here's a bunch of drugs.
[3093.26 --> 3095.32]  They might feel better next.
[3095.32 --> 3097.70]  And then proceed to ruin their life.
[3097.70 --> 3100.22]  You'd say, hey, I helped them feel better.
[3100.22 --> 3103.18]  And then you'd be like, no, you destroyed their life.
[3103.18 --> 3103.60]  Right?
[3103.60 --> 3108.94]  So it's just sometimes like it's very difficult because like you're saying, I don't know what's going to happen.
[3108.94 --> 3110.36]  But it just makes me nervous.
[3110.36 --> 3117.28]  Like sometimes doing something is like actively way worse for those people than not doing anything.
[3117.76 --> 3118.50]  I don't know.
[3118.50 --> 3119.68]  It's medical leeches, right?
[3119.68 --> 3125.06]  It's like, look, we have to make sure first that this thing does significantly more benefit.
[3125.42 --> 3127.80]  It has significantly more benefits than it has harms.
[3127.80 --> 3130.90]  And people are not approaching it with that kind of caution.
[3130.90 --> 3133.86]  They're just releasing these things and they're like, yeah, it's not my problem.
[3133.86 --> 3134.02]  Right?
[3134.02 --> 3137.06]  And you're like, okay, well, you know, that's one way to go about it.
[3137.06 --> 3142.62]  But it is, I think, you know, some responsibility should be taken for exactly the kinds of things you're talking about.
[3142.70 --> 3142.84]  Right?
[3143.24 --> 3143.38]  Yeah.
[3143.62 --> 3146.04]  There's also like, oh, hold on.
[3147.52 --> 3148.62]  Okay, Trash, I'll let you go.
[3148.66 --> 3149.32]  Then I'll go afterwards.
[3149.44 --> 3150.04]  I'll go afterwards.
[3150.22 --> 3152.58]  All this religious talk on me really thinking.
[3152.68 --> 3154.08]  I grew up pretty religious.
[3154.36 --> 3155.06]  I'm Catholic.
[3155.76 --> 3157.24]  There's this thing that we do call confession.
[3157.38 --> 3159.30]  I don't know if Christians or anyone else does it.
[3159.30 --> 3166.28]  But essentially you go into a booth, talk about your sins and what you've done, effectively to like a wall.
[3166.50 --> 3168.18]  You can't really see who's on the other side.
[3168.58 --> 3176.30]  And I wonder if there's going to be an evolution of that where you end up talking to a model trained on the Bible or whatever.
[3176.94 --> 3178.76]  And it ends up responding back to you.
[3178.78 --> 3181.46]  And I was like, this could be a very real scenario, right?
[3181.46 --> 3184.70]  Because at the end of the day, you don't know who you're talking to.
[3184.70 --> 3187.18]  So I don't know.
[3187.22 --> 3187.84]  Is that real?
[3188.10 --> 3188.42]  Like, could that?
[3188.76 --> 3191.18]  That's literally the Catholic Turing test, Trash.
[3191.30 --> 3196.38]  You're like, is the priest on the other side of the wall a person or an AI?
[3197.50 --> 3199.06]  Dude, that is a freak.
[3199.30 --> 3199.88]  That is good.
[3200.02 --> 3201.96]  Catholic Turing test is a new thing.
[3202.28 --> 3203.50]  You have just made it.
[3204.14 --> 3204.54]  Congratulations.
[3205.10 --> 3205.58]  Oh, my gosh.
[3205.58 --> 3207.66]  Do you leave with atonement or not?
[3208.80 --> 3209.26]  All right.
[3209.52 --> 3209.76]  Okay.
[3209.80 --> 3213.14]  But I do want to say, like, so I'm going to follow Casey's conclusion kind of to the end.
[3213.20 --> 3217.28]  So I've been recently having what is called early onset conservatism.
[3217.54 --> 3218.48]  I've been studying history.
[3218.90 --> 3221.48]  And one of the things I've been looking into is World War II.
[3221.68 --> 3226.36]  And so there's this idea of the Fuhrer principle, which is that there should be a strong leader who leads Germany.
[3226.36 --> 3234.72]  And this was happening after the ousting of Kaiser, during kind of like the confusion between World War I and World War II, where they're like, we just need someone strong to lead.
[3235.04 --> 3239.18]  And that was one of the big things Hitler took a hold of was this idea that they need a strong leader.
[3239.18 --> 3244.02]  So he would force people to call him the Fuhrer to align with this idea called the Fuhrer principle.
[3244.02 --> 3257.42]  And so all these people start joining with him and then due to the burning of the capital and then their ability to seize power through getting parliament to effectively or whatever they called it to affect their version of the Senate to erase itself and give all the power up.
[3257.56 --> 3258.40]  In a German accent.
[3258.76 --> 3258.92]  Yeah.
[3258.98 --> 3259.96]  In a German accent.
[3260.16 --> 3260.32]  Yeah.
[3260.36 --> 3261.66]  They gave up all the powers.
[3262.24 --> 3262.32]  No.
[3262.48 --> 3262.58]  So.
[3263.80 --> 3264.16]  Yeah.
[3264.34 --> 3264.94]  It's German.
[3265.12 --> 3266.28]  It's German nightclub, baby.
[3266.54 --> 3270.40]  So after all of that, they get to this point where now they have the power.
[3270.46 --> 3271.50]  Now they can start doing stuff.
[3271.76 --> 3272.68]  So what do they do?
[3272.68 --> 3274.80]  Did they go out and start putting everyone in concentration camps?
[3274.88 --> 3275.06]  No.
[3275.30 --> 3276.74]  Instead, they said, everybody loves us.
[3276.84 --> 3277.94]  Everyone wants to listen to us.
[3278.28 --> 3284.04]  So as Germans, we think it would be the best if you shop at German only places.
[3284.04 --> 3285.40]  Do not shop at Jewish places.
[3285.40 --> 3286.44]  That was like one of the first things.
[3286.44 --> 3289.34]  It's like part of the Aryan paragraph or I believe is the follow up to the Aryan paragraph.
[3289.84 --> 3291.44]  And it wasn't extreme.
[3291.44 --> 3293.36]  It wasn't going to cause people to go out and kill people.
[3293.68 --> 3295.26]  But that's what led ultimately down.
[3295.46 --> 3301.50]  Or that was the first step they took to lead ultimately down to one of the greatest like human rights violations of our world.
[3301.50 --> 3305.58]  And so you could imagine the AI, it could just latch onto something.
[3305.82 --> 3307.56]  And we have effectively the fear principle.
[3307.80 --> 3311.96]  We have something that people lead towards and be like, we need a strong person to do this.
[3312.26 --> 3313.02]  We need this.
[3313.10 --> 3314.12]  That strong person comes.
[3314.26 --> 3315.50]  AI has helped bolster them.
[3315.58 --> 3318.64]  People have this thought or this idea of this zeitgeist in their head.
[3318.70 --> 3319.94]  Like, let's go towards this.
[3320.00 --> 3321.56]  This is where I always worry with all these religions.
[3321.56 --> 3325.02]  Because like you said, there's no first, like this would be iteration one.
[3325.24 --> 3330.70]  And typically, 99% of man-made religions aren't here anymore because they were horrible.
[3331.04 --> 3332.60]  Like, no one worships Molech.
[3332.70 --> 3334.56]  No one throws babies into fire right now.
[3334.76 --> 3337.20]  And then, or set up, what, asterisk poles.
[3337.44 --> 3337.64]  Right?
[3337.68 --> 3341.50]  Like, no one's doing these crazy things anymore because we're like, yeah, that was crazy.
[3341.64 --> 3344.12]  We killed a lot of ourselves just to like worship this.
[3344.54 --> 3347.44]  And so, you know, that is a potential of all of this.
[3347.48 --> 3350.24]  Sorry, I got really into this whole thing, which is just craziness.
[3350.24 --> 3353.24]  So basically what you're saying is AI is Hitler.
[3353.68 --> 3357.10]  AI could very well be building the Hitler figure.
[3357.28 --> 3358.88]  I wanted Godwin's principle at least once.
[3359.22 --> 3361.46]  The primogen says AI is Hitler.
[3361.64 --> 3362.28]  There's the clip.
[3362.78 --> 3363.00]  Yeah.
[3363.20 --> 3363.62]  Got him.
[3364.12 --> 3364.54]  Got him.
[3364.58 --> 3366.82]  I think AI creating religion would lead to Hitler.
[3366.94 --> 3367.14]  Yes.
[3367.18 --> 3368.38]  That's what I personally think.
[3369.06 --> 3371.64]  AI creating religion leads to Hitler.
[3372.50 --> 3373.08]  The primogen.
[3373.08 --> 3373.22]  The primogen.
[3373.88 --> 3374.44]  Michael Scott.
[3374.48 --> 3378.00]  And on that note, thanks for joining us for this episode of The Stand-Up.
[3378.38 --> 3379.26]  The Stand-Up.
[3379.26 --> 3383.92]  If you guys think the internet is full of bad stuff, wait until you see what AI can generate.
[3384.48 --> 3384.70]  Yeah.
[3385.84 --> 3386.48]  All right.
[3386.72 --> 3388.48]  Well, that is actually it.
[3388.70 --> 3389.58]  Bye-bye, everybody.
[3390.52 --> 3391.00]  Bye.
[3391.34 --> 3392.14]  That's a great episode.
[3392.34 --> 3393.16]  Definitely do it on time.
[3393.16 --> 3394.18]  I guess that's it.
[3394.54 --> 3394.90]  Ouch.
[3395.46 --> 3395.70]  Yeah.
[3396.06 --> 3403.96]  We're going to end on the most truncated, simplified version of the history of the Third Reich that has ever been stated.
[3404.44 --> 3406.22]  Hey, that's better than most internet ones.
[3406.22 --> 3406.70]  Here's what happened.
[3406.70 --> 3407.84]  I at least gave some context.
[3407.84 --> 3408.94]  Hitler showed up.
[3409.04 --> 3410.80]  He said, buy German.
[3411.26 --> 3412.62]  And then everyone died.
[3412.82 --> 3413.18]  Thanks.
[3413.40 --> 3414.38]  This is The Stand-Up.
[3414.64 --> 3416.22]  Welcome to Prime's Drunk History.
[3416.78 --> 3418.44]  Have you listened to Brian Regan?
[3418.66 --> 3422.32]  He does this one thing, which is the laziest writing mnemonic, which is one thing led to another.
[3422.70 --> 3425.14]  And so he said that Hitler got rejected from art school.
[3425.56 --> 3426.64]  One thing led to another.
[3426.80 --> 3429.90]  The United States dropped two atomic bombs on the sovereign nation of Japan.
[3430.08 --> 3430.64]  There you go.
[3430.64 --> 3433.70]  So that might be the most truncated version.
[3434.16 --> 3434.66]  All right.
[3435.04 --> 3436.08]  I think that's it.
[3436.16 --> 3437.36]  There's nowhere to go from here.
[3437.48 --> 3437.92]  All right.
[3437.92 --> 3438.30]  We're done.
[3438.48 --> 3438.96]  We're Sp 아파.
[3439.08 --> 3440.20]  That's the end of this show.
[3440.40 --> 3440.72]  Bye bye.
[3440.72 --> 3442.70]  You may never see this clip again.
[3442.70 --> 3443.26]  tegenover biting.
[3443.26 --> 3445.52]  Thank you.
[3445.74 --> 3446.34]  .
[3446.34 --> 3446.46]  .
[3446.46 --> 3447.34]  .
[3447.34 --> 3448.40]  .
[3448.40 --> 3448.78]  .
[3448.78 --> 3448.84]  .
[3448.84 --> 3449.12]  .
[3449.12 --> 3449.56]  .
[3449.56 --> 3449.60]  .
[3450.02 --> 3453.72]  .
[3453.72 --> 3454.86]  .
[3454.86 --> 3455.66]  .
[3455.66 --> 3457.84]  .
[3457.84 --> 3458.74]  .
[3458.74 --> 3458.76]  .
[3459.08 --> 3459.36]  .
[3459.36 --> 3459.98]  .
[3459.98 --> 3460.32]  .
