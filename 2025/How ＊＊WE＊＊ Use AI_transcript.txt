[0.00 --> 0.50]  Here you guys.
[1.40 --> 2.28]  Hey, hi.
[2.90 --> 3.52]  Chess.
[3.88 --> 4.36]  Yeah, sorry.
[4.58 --> 4.68]  Chess?
[4.78 --> 5.62]  It is chess, yes.
[5.66 --> 7.44]  I was just testing out player versus chat.
[7.80 --> 8.58]  I give up.
[8.82 --> 9.30]  I give up.
[9.46 --> 9.84]  There we go.
[10.02 --> 10.48]  Game over.
[11.06 --> 13.08]  You lost versus chat?
[13.18 --> 14.04]  Yeah, chat won.
[14.12 --> 14.60]  I lost.
[14.72 --> 15.92]  Are you guys good at chess?
[18.44 --> 20.44]  No, but I'm confident I can beat Prime.
[20.66 --> 22.26]  That's all I need to feel.
[22.72 --> 27.82]  So you can type faster and chess better than Prime, is what you're saying to me.
[27.82 --> 30.18]  Are there any other things to be better at than that?
[30.18 --> 30.92]  Nothing worthwhile.
[35.42 --> 37.06]  Alright, hey.
[37.20 --> 38.50]  Welcome to the stand-up.
[38.56 --> 40.30]  Today we have with us Teej.
[40.32 --> 40.80]  Oh, wait, wait, wait.
[41.20 --> 41.58]  Wait, wait.
[42.18 --> 43.60]  Welcome to the stand-up.
[46.58 --> 47.62]  That's so mean.
[47.70 --> 49.26]  He was so on it.
[49.28 --> 50.06]  I was in this one.
[50.06 --> 50.76]  He was right in the middle of it.
[50.78 --> 55.10]  You just interrupted the one good intro we were going to get out of Prime today.
[55.18 --> 56.52]  Dude, you iced the kicker.
[56.52 --> 59.00]  That was the only mean thing I've ever seen Teej do.
[60.00 --> 62.24]  That's why Teej is not known as the nice guy at our company.
[62.40 --> 63.82]  Okay, Dax, you're the nice guy.
[64.08 --> 64.36]  Teej is not the nice guy.
[64.36 --> 65.26]  I'm the blotter wizard.
[65.80 --> 67.60]  I'm the Klarna joke guy.
[67.70 --> 68.52]  The Klarna joke.
[69.96 --> 71.64]  Dax, do you remember how excited Teej is?
[71.68 --> 72.92]  Okay, that's way too inside baseball.
[73.00 --> 73.76]  We're not going to talk about that.
[74.04 --> 75.94]  Alright, hey, welcome to the stand-up.
[75.94 --> 79.14]  Today we have with us Teej, the not nice guy.
[79.60 --> 79.84]  Hi.
[79.84 --> 87.32]  Teej, the Amiratori, commonly known as Casey, the Git lover, and Dax, the nice guy of our company.
[88.20 --> 103.16]  Today we're going to be talking about AI agents because we were going to talk about Git, but we decided that AI agents would be more topical because Dax is going to be releasing open code, a sweet, sweet command line agent for you to be able to do all the greatest things in the universe with.
[103.16 --> 104.32]  And it's model independent, right?
[104.56 --> 104.84]  Yes.
[104.84 --> 105.88]  Ooh, nice.
[106.16 --> 113.14]  Okay, so I guess if we're going to start off this talk, we should first find out who here actually uses agents to do what percentage of their work.
[113.18 --> 119.56]  Because I feel like this is probably going to be our largest disagreement we've ever had on this podcast because typically it goes like this.
[119.78 --> 121.34]  How do you feel about TDD?
[121.74 --> 123.64]  It's universally like, meh.
[123.78 --> 124.86]  It's like, how do you feel about this?
[124.88 --> 125.74]  I said I kind of liked it.
[127.36 --> 130.34]  You kind of like it, but I've literally never seen you do it.
[130.80 --> 132.62]  I write tests all the time.
[132.62 --> 134.30]  Are you joking me?
[134.60 --> 135.96]  Okay, we can't get distracted on this.
[135.96 --> 136.82]  But do you write them first?
[137.42 --> 137.84]  All right.
[137.98 --> 140.00]  I do a lot of red-green cycling, but okay, go ahead.
[140.12 --> 141.32]  Okay, so let's start with Dax.
[141.38 --> 145.26]  Dax, I always see you tweeting up a storm about your use of AI agents.
[145.38 --> 149.62]  At first, Devin was your boy, and then I think at some point you're like, I freaking hate Devin.
[150.04 --> 151.30]  And so what's going on?
[151.32 --> 152.10]  All my homies hate Devin.
[152.10 --> 156.54]  All my homies hate Devin, but for a while, Dax literally has a tweet like, Devin's my boy.
[156.54 --> 156.68]  Okay.
[160.52 --> 163.02]  Some people see a blank terminal, and they get overwhelmed.
[163.52 --> 167.44]  But some people see that empty canvas and know they're going to paint it with productivity.
[167.94 --> 169.54]  They're going to paint it with coffee.
[170.26 --> 171.66]  Why should you drink terminal coffee?
[172.16 --> 172.68]  Here's why.
[172.68 --> 186.74]  Terminal coffee?
[187.10 --> 187.88]  Yeah, I drink it.
[190.66 --> 190.90]  Oh!
[191.98 --> 192.30]  Oh!
[194.60 --> 195.12]  Oh!
[195.12 --> 195.44]  Oh!
[202.68 --> 209.60]  Yeah, so I'm definitely trying to force myself to push these things and see how much I can get out of it.
[210.74 --> 216.46]  Because I think I'm not someone that thinks that it's like going to be a majority of my work,
[216.46 --> 220.82]  but I need to push it to that degree to see what it can actually do and where the boundaries are.
[221.08 --> 224.56]  So I've been trying to do as much as possible.
[224.80 --> 226.84]  I've had very new varying degrees of success.
[226.84 --> 232.50]  Like I think I have a feel on when it works well, when I want to push work there, when I don't.
[232.68 --> 238.90]  I think what I've settled into now is I will I'm actively coding.
[238.98 --> 239.88]  I'm actually doing work.
[239.98 --> 243.28]  I'll reach a point where the work can be split up into two parts.
[243.28 --> 245.52]  Like there's a dumber part and there's a more intense part.
[245.98 --> 251.24]  I'll give the dumber part to the AI, let it do its thing while I do the more intense part.
[251.52 --> 253.82]  Then I'll check in to see, OK, how did it do?
[254.02 --> 258.90]  And sometimes a lot of times, you know, ends up successfully doing it correctly.
[258.90 --> 263.90]  But yeah, I'm like not someone that like lets it drive fully.
[263.90 --> 268.48]  I also do not use them at all in early stages of a code base.
[268.48 --> 271.50]  Like for open code, we basically rewrote it from scratch.
[271.92 --> 274.52]  Almost none of it was built using AI.
[274.70 --> 279.22]  It was all manually created because we're figuring out what abstractions are, what patterns are.
[279.22 --> 286.02]  There's not enough consistency in the code base for an LLM to really like do a good job when we give it a task.
[286.10 --> 290.30]  Dex, would you say that's primarily because you look down on everyone else?
[290.62 --> 293.28]  So like getting code back from the LLM is not going to be good enough?
[293.36 --> 295.22]  Like would you say it's different for Prime or?
[295.22 --> 299.68]  I don't think so.
[299.80 --> 301.34]  I mean, for me, it's just like.
[304.78 --> 305.58]  Good question.
[305.70 --> 306.46]  Good question.
[306.56 --> 307.36]  I'm not going to answer that.
[307.54 --> 309.98]  I'll just let the joke land.
[313.16 --> 314.98]  Prime can respond to that insult.
[315.34 --> 317.02]  Well, we've run into it ourselves, actually.
[317.36 --> 322.62]  I think we probably vibed a little too hard at the beginning of Mordoria when we should have done it at the end.
[322.62 --> 328.40]  Like we would have been a lot better doing like days five and six being harder on vibes.
[328.72 --> 335.42]  Day one through three, harder on smashing our face against the keyboard, trying a bunch of different stuff and getting a feel for a lot of items.
[335.78 --> 335.92]  Yeah.
[336.00 --> 338.64]  In fact, I still look back on that on that time.
[339.04 --> 345.00]  And one of the biggest takeaways I have is that even though I wanted to throw away 100 percent of the code that was generated,
[345.16 --> 348.50]  it also gave me a bunch of ideas about things I did and did not like.
[348.50 --> 354.52]  And a lot of the dangers with like love 2D that I just I would just had to painfully discover them in a much slower process.
[354.52 --> 356.68]  Instead, I'm just like, dude, our tower is always green.
[357.32 --> 357.92]  Oh, yeah.
[358.04 --> 359.94]  Set color is a global state.
[360.04 --> 360.40]  OK, OK.
[360.44 --> 362.64]  I see how this thing intersects or intersects this thing.
[362.90 --> 364.72]  And I just got like a really fast learning thing.
[364.76 --> 368.82]  So I'm like I'm still kind of on the fence about even vibe coding in the beginning when you don't know a problem.
[369.84 --> 370.96]  There's something about that that.
[371.12 --> 371.32]  Yeah.
[371.52 --> 372.20]  Interesting, at least.
[372.20 --> 378.20]  Yeah, I think there are some cases where I still find it useful if I throw away everything.
[378.52 --> 384.82]  There were some cases in OpenCode where I was porting functionality from one language to another and the original code I didn't write.
[385.42 --> 388.32]  So I had it ported, had the LM ported first.
[389.00 --> 393.50]  Then I read through the whole thing and oftentimes it threw away 90 percent of it or like rewrote it entirely.
[393.84 --> 399.62]  But that was a little bit easier than trying to learn it in the other language first and then port it to the next language.
[399.62 --> 404.30]  So, yeah, I find it useful even if I don't end up using the code itself.
[404.90 --> 410.26]  It also is cool that it's at least for me, it seems like it's really pushing down the cost of making a prototype.
[410.64 --> 413.64]  And like prototypes actually can get thrown away.
[413.76 --> 420.04]  Like the problem is most of the time up until now, prototypes so expensive to make that you're like, oh, we cannot throw this away.
[420.26 --> 423.02]  We cannot just like dump this code in the trash bin and move on.
[423.34 --> 424.72]  But now you're like, oh, it could take me a day.
[424.82 --> 426.18]  I'll just like do a bunch of random stuff.
[426.44 --> 427.74]  And then you get a feel for it.
[427.74 --> 428.82]  And they say, OK, cool.
[428.82 --> 430.98]  It's a good idea or it's a bad idea.
[431.16 --> 432.88]  And then you can make a decision based on that.
[434.38 --> 441.50]  Yeah, I everything I ever built, like in every internal tool I've built throughout my entire career has been a hey, let's try something out.
[441.86 --> 442.80]  Mash a bunch of code.
[443.02 --> 444.50]  And then people are like, oh, I like that.
[444.52 --> 445.76]  Let's we're going to use that now.
[445.78 --> 448.36]  And then just like, but I I thought we were just testing.
[448.68 --> 453.06]  OK, and then it's just like now it's just like the scramble to meet people's new expectations.
[453.06 --> 458.12]  And then on top of it, trying to fix up this kind of crappy experience that you have built.
[460.68 --> 466.42]  But Dax, what percentage are you hitting right now on your open?
[466.42 --> 467.48]  I have no idea.
[468.40 --> 469.88]  To be honest, you're not taking a metric.
[470.04 --> 470.98]  Do you guys have a sense of it?
[471.52 --> 471.98]  No, I'm not.
[471.98 --> 472.22]  I'm not.
[472.32 --> 474.24]  I'm not trying to get any of it for some projects.
[474.38 --> 475.60]  I mean, I've done 100 percent.
[475.98 --> 477.66]  I just did one that was 100 percent.
[478.36 --> 478.62]  Yeah.
[478.70 --> 480.68]  And people discover what that is.
[480.68 --> 483.90]  They will discover it actually this week.
[483.96 --> 486.66]  Not because it's so bad that they're going to find out.
[488.30 --> 491.94]  But but yeah, it was just like a standard website.
[492.16 --> 495.56]  You know, I mean, just like literally the most crud of websites.
[496.00 --> 496.86]  So it was perfect.
[496.98 --> 499.16]  I just typed the things I wanted and then it did that.
[499.18 --> 502.22]  And I'm like, can you make it so that I can change the colors?
[502.32 --> 503.86]  And it's like, yeah, I can do that.
[503.92 --> 506.08]  You know, it's I was like, oh, this was easy.
[506.58 --> 507.36]  I don't know any.
[507.36 --> 511.12]  I do not know close to enough tailwind to make this look like this.
[511.20 --> 512.84]  Adam would be yelling at me for days.
[515.48 --> 516.00]  All right.
[516.06 --> 516.92]  So give us the percent.
[517.10 --> 517.92]  Dax, just say it.
[518.04 --> 518.60]  You got to guess.
[518.76 --> 519.24]  You got to guess.
[520.94 --> 521.68]  I don't know.
[522.24 --> 526.04]  In terms of like completely hands off, like letting it like build features.
[528.70 --> 534.36]  I think I have to split it into two kinds of work because a lot of my work is building tools.
[534.36 --> 537.74]  And that kind of work is like too weird for an LLM.
[537.86 --> 539.68]  Like it's not like a standard web app or anything.
[539.90 --> 541.98]  So unfortunately, I can't use it that much.
[542.24 --> 551.60]  But then on more mature things like our terminal code base, we need to add like, for example, like Stripe was like we need you to add a terms of service page, whatever.
[552.10 --> 553.06]  That was like 100%.
[553.06 --> 560.30]  So I think for the more mature projects, I can like maybe half the functionality I can probably do with these tools.
[560.38 --> 564.78]  Again, given it's like a very standard web application that isn't isn't too weird.
[565.82 --> 571.60]  Follow up to that one with the even with the terms of service, would it even matter if it was a mature code base or not?
[571.60 --> 574.90]  Like that's just something that you're not super engaged in.
[574.96 --> 577.04]  And you don't really care if the code is great or bad.
[577.04 --> 580.72]  You just want to be able to give it a big, gigantic text and have someone scroll.
[580.82 --> 582.54]  So it's like, I don't care how you accomplish this.
[582.56 --> 584.44]  Just get it done because we just need it.
[585.22 --> 586.06]  Quick question.
[586.40 --> 586.58]  Yeah.
[586.66 --> 589.62]  I mean, to some degree, yes.
[589.76 --> 591.82]  But then there's always some patterns.
[591.82 --> 597.68]  Like, you know, if you use CSS or certain patterns in CSS, you don't want this thing to look like totally random.
[597.68 --> 602.60]  Like you don't want it to use the most extreme cases like it uses tailwind just for one part of your your app.
[603.78 --> 605.60]  So I still am.
[605.70 --> 611.36]  I don't feel like there's never going to come a day where someone is need to like look into this, change this or manually touch it.
[612.08 --> 616.60]  So I'm still pretty concerned about about what that actual output is.
[616.66 --> 621.10]  As tired as this metaphor is, it does feel like it keeps showing up.
[621.10 --> 623.38]  If you treat it like a dumb intern.
[623.68 --> 623.92]  I hate it.
[624.04 --> 629.92]  You would never let the dumb intern be in charge in the beginning of a project, like establishing the code base.
[630.38 --> 633.92]  You probably want to put them in an environment where there's patterns, things they can look at.
[634.10 --> 636.66]  You probably give them guidance on like, oh, it's like this.
[636.72 --> 639.92]  But then, you know, just modify it to for this use case.
[639.92 --> 643.12]  And then you'd make sure what they're pushing like matches everything else.
[643.58 --> 648.50]  So I'd like to treat it like there's a dumb intern sitting next to me with their own laptop.
[648.50 --> 651.32]  And I can give them tats and look over their shoulder every once in a while.
[651.90 --> 653.98]  I think that flow has probably worked best for me.
[654.16 --> 656.18]  Before Prime goes on a rant about hating this analogy.
[656.18 --> 656.44]  I'm not going to.
[656.64 --> 657.18]  I'm not going to.
[657.26 --> 657.56]  Oh, OK.
[658.16 --> 658.76]  You can, though.
[658.80 --> 659.40]  I think it's good.
[659.70 --> 664.14]  Or, you know, but I just wanted a quick clarification on our terms of service for Terminal.shop.
[664.14 --> 669.82]  But you're not telling me we vibe quoted the actual terms of service, though, right?
[669.86 --> 672.80]  That was a real document that we used.
[673.22 --> 674.26]  Have you read it yet?
[674.98 --> 677.18]  Well, so what I asked.
[677.18 --> 677.70]  Oh, no.
[678.48 --> 679.88]  No, it was generated.
[680.00 --> 680.84]  Josh, take it out.
[681.10 --> 682.18]  Josh, take it out.
[682.60 --> 683.04]  Oh, no.
[683.04 --> 687.64]  I took what Stripe said we needed.
[687.86 --> 688.86]  I gave the chat to APT.
[688.94 --> 691.60]  Then I asked it, don't generate this blindly.
[692.26 --> 695.88]  Ask me for questions step by step until you have enough information to fill this out.
[696.24 --> 700.82]  So then it asked me questions like, OK, under this scenario, do you offer a return?
[700.94 --> 701.48]  I say, yes.
[701.50 --> 702.46]  Under this scenario, blah, blah.
[702.56 --> 705.38]  And then I went through that process and then it generated a full term of service.
[705.84 --> 708.30]  Did anyone read this full term of service afterwards?
[709.28 --> 710.70]  Yeah, it wasn't that long.
[711.30 --> 712.02]  It wasn't anything crazy.
[712.44 --> 712.62]  Yeah.
[713.04 --> 713.80]  Just making sure.
[713.94 --> 714.54]  But it was generated.
[714.54 --> 718.20]  Because I want to go read it now and see if there's any great hallucinations in there.
[718.60 --> 721.66]  How many MDashes are in our terms of service?
[721.84 --> 723.88]  That's what I need to know right now.
[724.70 --> 726.28]  You know what the worst thing is?
[726.34 --> 729.02]  I use normal hyphens in my posts all the time.
[729.08 --> 733.54]  And people keep accusing me of generating it with chat GPT because they can't tell that's a normal hyphen.
[733.54 --> 734.08]  No, that's not the reason.
[734.22 --> 736.52]  It's because your posts are so mediocre, Dax.
[736.52 --> 738.00]  That's what's going on.
[738.30 --> 738.74]  Wow.
[744.90 --> 745.68]  I should just leave now.
[745.82 --> 747.46]  Oh, well, let's pass it over to Casey.
[747.56 --> 748.30]  I want to hear Casey.
[748.48 --> 751.94]  Have you used AI at all or do you use it a lot or do you hate it?
[752.16 --> 752.90]  That's what I want to know.
[752.98 --> 753.46]  Let's hear it.
[753.88 --> 755.54]  So it depends on what you mean.
[755.70 --> 758.44]  I have not used any AI coding stuff.
[758.44 --> 770.48]  Probably that may be partially Prime's fault, which is that you had me come on one time and look at the code that Devin had generated for your game.
[770.88 --> 772.88]  And it was horrible.
[772.88 --> 780.50]  To be fair, that was pure vibe coding, meaning that I let Twitch control it for like 8, 10 hours and we just vibe coded for 10 hours and Twitch just drove it.
[780.50 --> 782.22]  Omega vibe coding.
[782.22 --> 786.00]  Yeah, that's like we're talking about is like pinnacle vibe coding.
[786.00 --> 789.82]  And after I saw that, I was like, well, this technology is going nowhere.
[790.28 --> 804.34]  So that may be Prime's fault because maybe my first look at AI coding should have been in a more favorable environment where like somebody who'd had a lot of experience knowing how to put the prompts in or something.
[804.84 --> 805.36]  Something.
[805.36 --> 806.56]  I don't know.
[806.88 --> 807.08]  Right.
[807.22 --> 811.84]  But I definitely was like, well, this this is it was some of the worst code I'd ever seen.
[812.20 --> 817.24]  Like I actually think that if you do a pure vibe coding, it would it would it would generally come out near the same.
[817.54 --> 817.64]  Right.
[817.66 --> 825.42]  No matter how good your prompts are, you're going to get the things that you hated were the exact reasons what make LLMs weak when it comes to large scale projects.
[826.08 --> 833.72]  I got to say, there's no way the average vibe coding one is going to be as bad as having Twitch chat run vibe coding for 10 hours.
[833.72 --> 835.40]  The ideas will be better.
[835.54 --> 835.82]  True.
[836.04 --> 844.36]  But if you don't know what a vector is and you've like never built a game and you're like, build me a game, you're not going to know that it's going to just inline a bunch of vector operations all over the place.
[844.36 --> 847.58]  Like you just won't know those things because you don't have the foundation to know them.
[848.12 --> 849.16]  And that part I agree.
[849.32 --> 850.30]  But that's what we're talking about.
[850.34 --> 851.58]  That's what me and Casey are talking about.
[851.58 --> 857.46]  The idea of anime waifus and chussies, which is cheese, gold mines, demonetized.
[858.86 --> 861.04]  Trust me, YouTube does not know what that means.
[861.04 --> 864.12]  And so nobody knows what that means.
[864.24 --> 865.12]  That's the problem.
[865.26 --> 866.10]  It's provocative.
[867.00 --> 867.88]  Sorry, Casey.
[867.98 --> 868.38]  Keep going.
[869.26 --> 879.78]  So, you know, I mean that I obviously I know that maybe that wasn't the best possible introduction to AI coding tools.
[879.78 --> 889.84]  But I guess from my perspective, I don't really have much motivation to use them because mostly it's it's really about what are you trying to do when you're programming to me.
[889.84 --> 909.54]  And if your goal when you're programming is I just want to get something like I just want this thing to work like that's my goal when programming, then I can understand the attraction of these AI things because it's like, well, you know, I I just want the code to work and do whatever it is that I asked it to do.
[909.54 --> 920.32]  And they do seem to be able to sort of do that eventually, like even the thing that that prime was doing, you could never ship that code, but it sort of did something vaguely like what the prompts were.
[920.32 --> 933.72]  So if that's like your idea of what programming is to begin with, then not only does that make some sense, but also, you know, these things are going to get better or have gotten better since prime use them, I should say, like, who knows how good they'll eventually get.
[933.80 --> 937.20]  But, you know, they've gotten better since then anyway, at least a little bit.
[937.72 --> 945.78]  And so, you know, maybe that code quality improves for me, like the entire point of programming was that if something already had been done, I don't have to do it again.
[945.94 --> 946.38]  Right.
[946.38 --> 967.60]  And so there's sort of a fundamental underlying mismatch with AI and my whole idea of what programming is, which is like, if what this thing is doing is it's going to mush together a bunch of weird code that's going to feed into some preprocessor that's going to feed into react, that's going to feed into the DOM, it's going to feed into the web browser that's eventually going to make this button.
[967.60 --> 981.70]  To me, that's just like stacking another piece of failure, which is whatever garbage the AI did onto a giant pile of failure, which is all of these terrible systems that are like, why couldn't I just place the frickin button there in the first place?
[981.98 --> 982.08]  Right.
[982.12 --> 985.30]  Like, why did I need an AI to do all these things?
[985.72 --> 986.96]  Why is it so bad?
[987.08 --> 987.30]  Right.
[987.54 --> 993.22]  Because that's been my attitude on web stuff for this whole, like, for the entire time I've seen it sort of happening off to the side.
[993.22 --> 995.72]  But I'm just like, why is this stuff so bad?
[995.90 --> 998.00]  You know, I've programmed web pages.
[998.92 --> 1000.02]  I've hand coded some.
[1000.14 --> 1001.66]  I also made my own generator for one.
[1001.74 --> 1007.32]  We used to have a website that was fancier than our current website until I decided I don't really care about having a website much anymore.
[1007.84 --> 1008.60]  SSH, by the way.
[1008.74 --> 1009.50]  You should just use that.
[1009.60 --> 1011.78]  Yeah, just use SSH to order my coffee and everything else.
[1011.78 --> 1011.90]  SSH.
[1012.38 --> 1014.44]  And so, like, I've done a bunch of work with it.
[1014.76 --> 1018.50]  And, you know, our website was very, very fast, right?
[1018.50 --> 1020.06]  Because I wanted it to come up quickly.
[1020.32 --> 1023.70]  And so I interacted with a lot of the different stuff that you need to do to do that.
[1023.78 --> 1026.68]  Like, for example, source sets and images and things like this.
[1027.20 --> 1032.78]  And if you're a graphics programmer and you interact with that stuff, you're just immediately like, this is so bad.
[1032.84 --> 1041.36]  Like, the people who designed it had no idea how this kind of system needs to work in order to actually be performant and scalable, right?
[1041.36 --> 1043.64]  They just had no idea, right?
[1043.86 --> 1050.50]  Because they made source sets based on media queries, which is exactly the opposite of what you want to do, because they wanted to be able to do prefetches early.
[1050.66 --> 1057.54]  But you can't really do that because the way that images tend to be used, you don't know exactly what the pixel ratio is going to be versus the display size.
[1057.76 --> 1060.06]  And so it's the exact wrong thing.
[1060.22 --> 1068.22]  And you literally can't use source sets the way a graphics programmer would to get the exact right image resolution for the use case because it's just designed poorly.
[1068.22 --> 1071.78]  And you hit that over and over and over again.
[1071.98 --> 1077.28]  The positioning in CSS is the same way, clearly designed by people who had no idea how to do a good layout tool.
[1077.90 --> 1086.44]  And so when I look at the AI stuff, a lot of times I was looking at it and going like, okay, these things that people are talking about, a lot of people say it brings the fun back to programming.
[1086.56 --> 1089.04]  Yeah, but you guys took the fun out of it in the first place.
[1089.28 --> 1091.98]  So why don't you just stop doing that?
[1092.42 --> 1094.86]  And then you wouldn't need the AI to put the fun back in.
[1094.92 --> 1097.22]  You just have a pleasant to use API.
[1097.22 --> 1101.16]  But that's like so far from anyone's conceptualization.
[1101.38 --> 1109.54]  Like they don't realize that they pooped the turd and now they're so happy that they have an AI to clean the turd off of their floor for them.
[1109.64 --> 1109.72]  Right?
[1109.86 --> 1111.58]  Just stop crapping in the house.
[1112.20 --> 1113.18]  Guys, build a toilet.
[1113.56 --> 1114.44]  That's all it took.
[1114.52 --> 1114.64]  Right?
[1115.04 --> 1117.62]  So AI to me just kind of looks like that right now.
[1117.62 --> 1121.62]  And it's not to say that it might not get to a point where it looks like something else to me.
[1121.68 --> 1125.30]  Because I could see like the reinforcement learning stuff is better now.
[1125.56 --> 1127.34]  I like that approach to AI better.
[1127.54 --> 1130.70]  I like the idea that it's going to refine ideas and work on things.
[1130.84 --> 1135.66]  That sounds more like something that could produce usable code in like a novel environment maybe.
[1135.66 --> 1141.12]  So it's not just slopping from like, you know, it's not just serving as my stack of overflow cut and paste.
[1141.88 --> 1144.58]  But, you know, currently I'm just like annoyed with the whole thing.
[1144.72 --> 1148.14]  It's like a giant waste of CPU design.
[1148.86 --> 1152.98]  Because like Tensor cores are pretty much useless for anything in my opinion other than doing this.
[1152.98 --> 1157.18]  It's this huge silicon investment, data center investment, all this stuff.
[1157.34 --> 1163.82]  And so far the big thing that people are talking about is like, well, it does all this crap code we didn't want to have to write.
[1163.86 --> 1166.74]  But the only reason for that crap code is we didn't do a good job with everything else.
[1166.78 --> 1166.90]  Right?
[1167.90 --> 1171.46]  If you told me the AI was going to really improve that stack.
[1171.70 --> 1175.22]  Like, hey guys, the AI is way smarter than humans.
[1175.36 --> 1176.64]  It's going to fix this thing.
[1176.72 --> 1179.64]  It's going to be as if people who knew what they were doing had made it.
[1179.64 --> 1183.48]  Then I'm like, yes, the AI is here to save us.
[1183.54 --> 1184.08]  Thank God.
[1184.20 --> 1186.62]  But so far no one has demonstrated anything like that.
[1186.70 --> 1190.54]  So I'm still waiting for something for me to get excited about.
[1190.72 --> 1191.52]  So that's my rant.
[1192.36 --> 1196.88]  It might help you to know, Dax used AI to help us make Terminal Feud.
[1197.22 --> 1198.60]  So, I mean, that's kind of like something.
[1198.82 --> 1199.08]  What's Feud?
[1199.98 --> 1205.06]  The Family Feud game show, loosely inspired, not trademarked at all.
[1205.52 --> 1206.14]  To be clear.
[1206.74 --> 1209.14]  It does sound somewhat close to a trademark.
[1210.24 --> 1210.38]  No.
[1210.38 --> 1212.32]  I said loosely inspired by.
[1213.36 --> 1214.70]  Loose, loosely.
[1215.30 --> 1218.46]  So, I mean, that's kind of like something that is good for humanity.
[1219.16 --> 1219.34]  No.
[1219.58 --> 1219.90]  Not that.
[1219.98 --> 1220.34]  Not that.
[1220.36 --> 1220.76]  Not that.
[1220.84 --> 1221.02]  Not that.
[1222.30 --> 1224.20]  Oh, we got a big buzzer.
[1224.36 --> 1224.86]  Not this one.
[1224.92 --> 1226.66]  Did we get that for the big buzzer, Dax?
[1226.66 --> 1228.54]  This is an example of what this was not like.
[1228.54 --> 1229.34]  Yeah, it wasn't like that.
[1229.52 --> 1230.82]  We did not wear a red suit.
[1230.82 --> 1231.92]  I don't know why I have this.
[1232.08 --> 1233.64]  I don't know who gave this to me.
[1233.70 --> 1234.70]  I think it was like a gift.
[1235.66 --> 1237.16]  Someone thought I was really into Family Feud.
[1237.18 --> 1237.68]  Are you not?
[1237.68 --> 1238.28]  That's me.
[1238.68 --> 1240.30]  I'm the one who watched all the episodes.
[1240.44 --> 1242.68]  I'm into Terminal Feud, which is a totally different thing.
[1242.70 --> 1243.20]  Completely unrelated.
[1243.46 --> 1246.48]  I just happened to think of it when you said Family Feud for no real reason.
[1246.76 --> 1248.24]  It just popped into my head randomly.
[1249.40 --> 1249.70]  Yes.
[1249.86 --> 1250.72]  It's so different.
[1250.90 --> 1255.56]  Like, in that game show, they ask questions and give answers.
[1255.56 --> 1258.48]  And ours, it's kind of like that too, but different.
[1258.82 --> 1258.92]  Yeah.
[1258.92 --> 1260.44]  They don't even have a terminal in theirs.
[1260.60 --> 1260.80]  Yeah.
[1260.90 --> 1261.34]  Exactly.
[1261.92 --> 1262.74]  So what is that?
[1262.92 --> 1264.40]  Also, our guy doesn't look like this guy.
[1264.80 --> 1265.54]  There's like a key difference.
[1265.74 --> 1266.98]  And there's two hosts for ours.
[1267.00 --> 1267.56]  That's true.
[1267.64 --> 1268.14]  That's true.
[1269.24 --> 1270.00]  So that's different.
[1271.42 --> 1271.88]  We were green suits.
[1271.88 --> 1274.68]  I can't remember who the original host was way, way back in the day.
[1274.72 --> 1275.56]  We're not supposed to talk about that.
[1275.56 --> 1277.48]  It's some guy who loved kissing contestants.
[1277.72 --> 1277.96]  Yeah.
[1278.34 --> 1279.36]  Yes, that's right.
[1279.78 --> 1280.16]  You did.
[1280.16 --> 1281.16]  I only remember.
[1281.16 --> 1281.70]  I only remember.
[1281.70 --> 1282.32]  Beacon is the same.
[1282.76 --> 1283.16]  Richard.
[1283.48 --> 1283.88]  Richard.
[1284.32 --> 1284.72]  Yeah.
[1284.88 --> 1285.14]  What was it?
[1285.14 --> 1286.58]  All I know is we had no idea.
[1286.76 --> 1290.02]  And Beacon's like, okay, here's the thing is, every time there's a girl contestant,
[1290.02 --> 1290.78]  you need to kiss her.
[1290.90 --> 1292.36]  I'm like, we're not doing it.
[1292.44 --> 1293.72]  That doesn't sound like a good idea.
[1293.84 --> 1294.86]  TJ and I are not going to do that.
[1295.64 --> 1295.96]  Yeah.
[1296.04 --> 1302.06]  Beacon was like, guys, it's a well-known meme from the show is people kiss people.
[1302.18 --> 1303.42]  I was like, first off.
[1303.42 --> 1304.60]  For people over 40, maybe.
[1304.86 --> 1305.18]  Yeah.
[1305.18 --> 1307.12]  I'm like 30 years ago.
[1307.40 --> 1307.58]  Yeah.
[1307.74 --> 1309.28]  Secondly, no.
[1310.20 --> 1310.76]  No.
[1311.04 --> 1314.06]  Even if it was a good meme, I'm still not doing it.
[1314.44 --> 1317.70]  He really tried for 10 minutes to convince us it would be a good idea.
[1317.70 --> 1318.14]  Yeah.
[1319.96 --> 1320.32]  Yeah.
[1320.32 --> 1321.90]  It was actually an excessive period of time.
[1322.02 --> 1322.48]  Dax was there.
[1322.68 --> 1323.34]  He just kept on.
[1323.38 --> 1324.92]  And it was like multiple times throughout the day.
[1324.96 --> 1327.02]  But seriously, like you really should.
[1327.60 --> 1328.34]  You should just kiss them.
[1328.34 --> 1329.14]  And then he's like pulling up.
[1329.20 --> 1332.66]  He's pulling up like highlight compilations from it where the host is like kissing all
[1332.66 --> 1333.34]  these random girls.
[1333.38 --> 1334.38]  And you're like, that's crazy.
[1334.62 --> 1335.26]  That's insane.
[1335.26 --> 1337.14]  I can't believe they got away with this on TV.
[1337.78 --> 1338.30]  It would be.
[1338.80 --> 1343.08]  It would have been an amazing reference for the people that got it, but it would have been
[1343.08 --> 1346.14]  horrifying for everyone else, which would have been 90% of the people.
[1346.24 --> 1346.48]  Yes.
[1347.34 --> 1350.00]  But saying it like that makes it sound funnier for me.
[1350.30 --> 1354.18]  So you're not even allowed to say that because that's TJ's humor in a nutshell.
[1355.02 --> 1357.44]  TJ's like, oh, a bunch of people wouldn't get the joke.
[1357.58 --> 1358.50]  I'm in.
[1359.02 --> 1359.84]  What's the joke?
[1361.84 --> 1362.44]  All right.
[1362.72 --> 1363.12]  TJ.
[1363.66 --> 1366.28]  So Casey, your percentage is effectively zero then.
[1367.08 --> 1367.96]  It's not effectively zero.
[1368.04 --> 1368.68]  It's literally zero.
[1368.68 --> 1369.62]  Okay.
[1369.70 --> 1373.52]  So have you ever just tried to like, just, I don't want to, I don't care about getting
[1373.52 --> 1375.98]  this server or something out that you just don't care about.
[1376.02 --> 1380.82]  And you're just like, Chad GPT, write me a Python thing to serve out this thing and add
[1380.82 --> 1381.68]  in Twitch login.
[1382.08 --> 1383.14]  You've never done anything like that?
[1383.72 --> 1385.24]  Well, no, because my experience.
[1385.50 --> 1388.26]  So one of the reasons I don't ship web stuff hardly ever.
[1388.26 --> 1388.60]  Right.
[1388.64 --> 1389.38]  I mean, I do.
[1389.54 --> 1394.24]  So I wrote a mailing list thing in C that still runs our mailing list to this day.
[1394.90 --> 1398.64]  It's, it's a little, you know, if you go on like the, um,
[1399.00 --> 1399.92]  Molly rocket.com.
[1399.96 --> 1400.16]  Right.
[1400.32 --> 1401.48]  And there's a thing at the bottom.
[1401.48 --> 1404.10]  That's like typing your email address to subscribe that thing.
[1404.98 --> 1408.80]  So, I mean, my experience with web stuff is there's only two things.
[1408.92 --> 1413.36]  There's one that I write myself in C and that I run on a server that I control.
[1413.46 --> 1415.28]  And that thing will run forever.
[1415.52 --> 1417.04]  This thing has run forever.
[1417.32 --> 1417.72]  Basically.
[1417.72 --> 1417.98]  Right.
[1417.98 --> 1424.06]  The only thing that I have to do for maintenance is because in the galaxy brain, astronomically
[1424.06 --> 1431.06]  ridiculous world of the web, you can't even get a signed certificate that's valid for longer
[1431.06 --> 1431.66]  than a year.
[1431.66 --> 1434.58]  So either you run Lex, let's encrypt.
[1434.72 --> 1435.08]  Right.
[1435.74 --> 1441.10]  Or you actually have to manually go and update this certificate once a year.
[1441.18 --> 1443.62]  So there is one small maintenance thing that has to happen.
[1444.34 --> 1448.92]  But either that, or I use something like you said, where, you know, forget vibe coding,
[1448.98 --> 1451.40]  but just, you know, you, you use something even as Python, whatever.
[1451.40 --> 1456.14]  And that thing will be broken in, in six months because of some updates where you will not
[1456.14 --> 1460.16]  be able to deploy it on some new machine for whatever reason, without recompiling it or
[1460.16 --> 1462.04]  changing a few things or whatever else.
[1462.04 --> 1466.12]  So I just, I don't like things that you can't do once and have them run forever.
[1466.32 --> 1468.82]  So I rarely deploy web things in that way.
[1468.88 --> 1472.62]  If I'm going to actually do it, I will do it that way.
[1472.92 --> 1477.92]  And for example, that thing also had a, that, that particular little piece that I built,
[1477.92 --> 1482.86]  it also had a thing that would do Twitch to see if I was live.
[1482.96 --> 1484.32]  Cause this was back in the days when I did Twitch.
[1484.40 --> 1485.76]  That's how long ago I shipped this thing.
[1486.38 --> 1490.28]  Um, and that part of course, eventually broke and I can't use it anymore.
[1490.52 --> 1490.92]  Why?
[1491.06 --> 1494.14]  Because Twitch changed their API because that's the nature of the web.
[1494.16 --> 1496.96]  Like the web has no idea about the idea.
[1497.08 --> 1500.46]  Like they literally can't conceive of something that could run forever.
[1500.48 --> 1502.16]  They have no idea what that looks like.
[1502.26 --> 1505.46]  They're like, things are just, they're only going to work for a few weeks, guys.
[1505.60 --> 1506.36]  That's what we ship.
[1506.36 --> 1511.40]  And everyone thinks it's fine to just completely change their architecture every, you know, month or whatever.
[1511.78 --> 1517.10]  And if everyone's software breaks, it's okay because they were all busy fixing all these other things that broke that week.
[1517.18 --> 1519.82]  They'll never even notice that we pushed this completely different thing.
[1519.82 --> 1520.02]  Right.
[1520.38 --> 1525.26]  And so, uh, I just don't do that sort of stuff for that reason.
[1525.62 --> 1532.90]  Like I've said before about AI, if I did that kind of code, I could totally, like, I probably would use it because it's so annoying.
[1532.96 --> 1534.38]  I don't want to learn this stuff.
[1534.38 --> 1536.08]  It's, it's so ephemeral, right?
[1536.08 --> 1538.46]  You would learn how to do this one thing.
[1538.46 --> 1544.14]  And then that knowledge would be worthless in five weeks because they'll have completely changed how that thing worked.
[1544.14 --> 1544.50]  Right.
[1544.60 --> 1545.76]  Can I pause you for a second?
[1545.86 --> 1546.02]  Yeah.
[1546.32 --> 1547.54]  Dax, you got to get off Slack.
[1547.86 --> 1548.84]  Not only can we hear that.
[1549.82 --> 1550.38]  Wait, what?
[1550.50 --> 1551.46]  We can hear when you get to Slack.
[1551.46 --> 1553.30]  And we can watch your eyes going up and down.
[1553.38 --> 1554.70]  You're looking so serious.
[1554.70 --> 1556.82]  I'm not.
[1556.88 --> 1557.64]  I was just looking at the screen.
[1557.72 --> 1558.30]  I wasn't looking at Slack.
[1558.30 --> 1559.80]  That's the wrong screen for Slack, dude.
[1559.84 --> 1561.02]  He doesn't put it on this monitor.
[1561.48 --> 1561.80]  Okay.
[1562.54 --> 1563.52]  Something's going on over there.
[1563.56 --> 1564.26]  We hear that Slack.
[1564.26 --> 1564.66]  I wanted to.
[1565.04 --> 1565.76]  I got executed.
[1565.76 --> 1566.50]  I interrupted.
[1566.50 --> 1567.84]  I interrupted Casey's rant.
[1567.84 --> 1570.16]  I was just wondering, Dax, how's the open code rewrite going?
[1572.66 --> 1573.06]  Yeah.
[1573.28 --> 1576.30]  I mean, Casey, I know about your thing about how APIs change.
[1576.68 --> 1578.90]  Dax, I'm super happy you got rid of some of your APIs.
[1579.36 --> 1581.34]  Like, that was such a good move.
[1582.40 --> 1582.96]  It's not.
[1583.10 --> 1584.22]  Stop doing this, guys.
[1584.36 --> 1585.86]  APIs are supposed to work.
[1586.04 --> 1587.92]  It wasn't an API, though, to be fair.
[1588.06 --> 1588.32]  It's just.
[1588.52 --> 1589.98]  I don't know what open code is, so I'm not criticizing it.
[1590.06 --> 1590.66]  But I'm just saying.
[1590.76 --> 1590.88]  Yeah.
[1591.70 --> 1592.84]  No, I just wanted to get that.
[1592.84 --> 1594.44]  Yeah, it was just a config file location.
[1594.72 --> 1594.96]  Yeah.
[1595.08 --> 1595.86]  And name, potentially.
[1597.32 --> 1602.30]  Yeah, we, like, when we did the character animation stuff I worked on at Rad,
[1602.74 --> 1604.68]  we changed the API one time.
[1605.36 --> 1609.08]  And it was only because we had very few customers at that point, right?
[1610.40 --> 1611.44]  All that I can relate to.
[1611.72 --> 1615.20]  And from then on, like, I just made sure the API worked forever.
[1615.46 --> 1616.32]  Like, it never broke.
[1616.46 --> 1617.20]  Even the files.
[1617.54 --> 1620.10]  In fact, we were forwards and backwards compatible on files.
[1620.10 --> 1622.76]  So, like, basically newer versions could load older files.
[1623.08 --> 1624.54]  Older versions could load newer files.
[1624.74 --> 1627.88]  And they just wouldn't understand, like, the parts that were new or whatever,
[1628.06 --> 1628.68]  and that sort of thing.
[1629.06 --> 1631.50]  And we took that stuff very seriously because we're like, look,
[1631.56 --> 1632.54]  we don't want to break.
[1632.68 --> 1635.06]  Like, people are supposed to learn this thing once,
[1635.12 --> 1637.04]  and it's supposed to be easy to use, right?
[1637.08 --> 1637.84]  You're supposed to just.
[1638.28 --> 1640.06]  Humans are supposed to walk up to this thing and be able to use it.
[1640.08 --> 1641.14]  And we took that very seriously.
[1641.60 --> 1645.80]  And, you know, the parts I think were failings of, you know,
[1645.80 --> 1648.22]  some of those API designs were anything that wasn't easy.
[1648.22 --> 1651.28]  Like, I know exactly which parts we screwed up because I'm like,
[1651.36 --> 1654.70]  that part wasn't as straightforward as it could have been.
[1654.74 --> 1657.02]  Or, like, there's ways we could have done this part better, right?
[1658.18 --> 1663.42]  And so, you know, nowadays people don't take any of that seriously, right?
[1663.74 --> 1665.20]  They're just like, it doesn't matter.
[1665.42 --> 1666.60]  We just, we do whatever.
[1666.98 --> 1670.14]  We, I think, develop in public is kind of this idea, right?
[1670.14 --> 1673.84]  We just put it out there, and it's just completely fluid.
[1674.08 --> 1675.52]  It could change at any time.
[1675.64 --> 1679.06]  There's no concept of, you know, what it's supposed to do or any of that sort of stuff.
[1679.70 --> 1684.84]  And, yeah, I mean, I don't blame anyone for using an AI in that environment
[1684.84 --> 1686.74]  because, like I said, what else are you going to do?
[1686.86 --> 1688.52]  Like, it's not satisfying.
[1688.66 --> 1689.36]  It's not fun.
[1689.52 --> 1693.08]  But I wish there would be some recognition that this problem was created.
[1693.60 --> 1696.36]  Like, it does not exist in programming normally.
[1696.36 --> 1702.02]  It exists when you have decided to create layers of abstraction, and those layers are bad.
[1702.40 --> 1703.74]  That's when this happens.
[1703.94 --> 1706.92]  Because when those layers are good, it's pleasant to use.
[1707.18 --> 1710.76]  And you're not wondering all the time, how would I change the color of this thing?
[1710.86 --> 1712.26]  Or how would I center this button?
[1712.60 --> 1716.74]  But that's become so normalized in WebNow that even the act of centering a button
[1716.74 --> 1721.12]  could, in certain circumstances, be so complicated that I don't even know how to do it
[1721.12 --> 1723.92]  by a straightforward, like, one-line thing I might type in.
[1723.92 --> 1728.10]  That's become so normalized because of such horrifically bad design.
[1728.50 --> 1729.82]  I mean, it's worth underscoring.
[1731.06 --> 1736.62]  You have to be so bad at what you're doing to make a system where centering is difficult.
[1736.96 --> 1741.82]  Like, you have to be exceptionally terrible at what you've done to get to that point,
[1741.86 --> 1744.54]  and nobody seems to think this is a critical issue, right?
[1744.64 --> 1746.70]  They're just like, yeah, it's been that way forever.
[1746.86 --> 1748.92]  We've tried to fix centering, like, 17 times.
[1748.92 --> 1752.76]  We've sort of half succeeded by now, but not really.
[1752.96 --> 1754.16]  Like, it still actually kind of sucks.
[1754.30 --> 1757.74]  And you would much rather just write a single line of JavaScript code that could actually
[1757.74 --> 1759.98]  center something because that's trivial to do, right?
[1760.18 --> 1764.08]  Whereas in CSS, it's this enormous annoyance to do these sorts of things.
[1764.76 --> 1768.42]  So I feel like that's just, it's very, very frustrating, is what I would say.
[1768.46 --> 1769.28]  It's very, very frustrating.
[1769.28 --> 1771.12]  And I wish that part would get acknowledged.
[1771.76 --> 1775.10]  And the AI part is not really, so to me, the AI part is not that interesting.
[1775.10 --> 1781.24]  But I totally understand why it's valuable because, yeah, I would want it too if I was
[1781.24 --> 1781.98]  forced to use it.
[1782.28 --> 1784.62]  If I was forced to use that sort of stuff, I would 100% want it.
[1787.38 --> 1789.60]  I love that that's somehow tied into AI.
[1791.42 --> 1792.28]  That's what it's doing.
[1792.70 --> 1794.20]  That's what I see everyone doing with it.
[1794.26 --> 1796.34]  Like, you go look at, like, what's this demo of this AI?
[1796.58 --> 1797.22]  That's what it did.
[1797.84 --> 1802.92]  They never show an AI, like, oh, this AI, like, recreated a browser that's way better
[1802.92 --> 1807.20]  with, you know, with a better design that's really easy to program web pages in or something,
[1807.28 --> 1807.46]  right?
[1807.76 --> 1808.94]  They never show that.
[1809.20 --> 1813.62]  What they show is, hey, it put forth 30,000 lines of code that's accomplishing something
[1813.62 --> 1815.34]  incredibly simple.
[1815.84 --> 1819.84]  Like, so incredibly simple that if you had written it from scratch, it would probably
[1819.84 --> 1823.32]  take less than 30 lines in the first place, less than 30,000 lines in the first place.
[1823.60 --> 1828.22]  But, like, because they're sitting on this gigantic stack, it took 30,000 lines of code.
[1828.22 --> 1831.76]  And maybe that's not even that unreasonable for this thing at that point.
[1832.06 --> 1833.24]  It's so nuts, guys.
[1834.00 --> 1836.76]  Anyway, this stuff, you know, I mean, people like me have been ranting about this stuff
[1836.76 --> 1837.06]  forever.
[1837.28 --> 1838.70]  We've been ranting about it for 20 years.
[1839.42 --> 1842.28]  We're never going to have a different opinion about it because that's just how we feel about
[1842.28 --> 1842.40]  it.
[1842.48 --> 1843.26]  I'm sure everyone disagrees.
[1843.34 --> 1845.48]  I'm sure everyone in the YouTube comments can be like, hey, this isn't what he's talking
[1845.48 --> 1845.82]  about.
[1846.14 --> 1847.06]  This stuff is amazing.
[1848.12 --> 1848.80]  Okay, maybe.
[1849.10 --> 1849.54]  I'm old.
[1849.56 --> 1850.28]  I don't have to care.
[1850.76 --> 1851.82]  You guys do what you want.
[1852.42 --> 1852.78]  Right?
[1852.78 --> 1858.82]  But at least for someone who's worked on API design a lot in their career, I look at
[1858.82 --> 1862.88]  this stuff and I'm just like, you're just piling another disaster on top of a giant disaster.
[1863.20 --> 1865.94]  And I don't think it bodes well for security.
[1866.48 --> 1866.68]  Right?
[1866.82 --> 1868.36]  I don't think it bodes well for performance.
[1868.72 --> 1872.94]  I don't think it bodes well for maintainability unless these AIs get better, to be honest,
[1873.62 --> 1875.10]  which, you know, I certainly hope they do.
[1876.12 --> 1877.10]  And anyway, rant off.
[1877.16 --> 1877.56]  There you go.
[1878.02 --> 1878.36]  All right.
[1878.42 --> 1879.58]  Zero percent from Casey.
[1879.80 --> 1880.94]  Dax with an I don't know.
[1880.94 --> 1881.96]  TJ, where are you at?
[1882.78 --> 1887.82]  I'll go the it depends answer to like Dax.
[1887.96 --> 1892.92]  I've been trying it in a variety of different levels, like on different projects.
[1894.18 --> 1898.82]  Like I said, I just did one where 100% I wanted to see how far I could get.
[1900.00 --> 1903.58]  It's like a simple site overall, but it was cool that I didn't have to do anything.
[1903.74 --> 1904.38]  I can click a button.
[1904.48 --> 1904.92]  It deploys.
[1905.24 --> 1906.00]  Very exciting.
[1906.00 --> 1908.94]  And like pushed.
[1909.64 --> 1917.88]  I could see that enabling a bunch of people to get to a like MVP version of an idea that
[1917.88 --> 1920.86]  then maybe like they can validate and make sure it makes sense.
[1920.90 --> 1922.86]  And then maybe they throw away the whole site.
[1922.94 --> 1924.16]  Maybe they throw away a bunch of it.
[1924.50 --> 1925.62]  Maybe they keep it because it's fine.
[1925.68 --> 1927.36]  Like, I don't know if you guys have seen Craigslist.
[1927.36 --> 1927.84]  Yeah.
[1928.66 --> 1929.36]  Billion dollars.
[1929.58 --> 1936.32]  Like, it's a successful company and it's like raw HTML links and squares.
[1936.66 --> 1938.42]  So like it could it could work.
[1938.50 --> 1941.52]  Like there's a bunch of companies that have made way more money than me on like terrible
[1941.52 --> 1942.10]  tech stacks.
[1942.14 --> 1942.68]  And that's fine.
[1942.68 --> 1944.34]  And like depends what you're shooting for.
[1944.40 --> 1945.48]  Depends what you're wanting to do.
[1945.48 --> 1951.66]  So I think in like those verticals, I can see a lot of these being like very powerful,
[1951.66 --> 1955.58]  especially if you you know, if you're like Prime, you're more of an ideas guy.
[1955.90 --> 1957.72]  Like it'd be really helpful.
[1959.54 --> 1960.46]  I promise.
[1960.60 --> 1961.66]  You weren't supposed to laugh.
[1961.82 --> 1963.62]  You made me break on that one.
[1964.18 --> 1966.42]  You said it in the terminal documentary.
[1966.54 --> 1968.38]  That's the only reason I said that.
[1968.88 --> 1970.54]  But I prompt you to build stuff.
[1970.86 --> 1971.10]  TJ.
[1971.26 --> 1972.68]  I know.
[1973.02 --> 1974.28]  I'm like an LLM.
[1974.28 --> 1975.94]  Actually, Adams are LLM.
[1975.94 --> 1976.96]  Yeah, that's fine.
[1978.06 --> 1979.04]  Adam intelligence.
[1979.04 --> 1980.18]  That's what AI stands for.
[1980.72 --> 1987.40]  So so I think there there are some verticals like in that arena or like Dax is saying where
[1987.40 --> 1989.60]  there's like an offshoot of an existing project.
[1989.60 --> 1991.76]  You have a bunch of pretty standard stuff.
[1991.94 --> 1993.58]  You want to do a new standard thing.
[1993.98 --> 1998.54]  I can totally see like if you're on a long running project, you have seen this.
[1998.54 --> 2003.80]  You have a bunch of tickets that are relatively straightforward and nobody has time.
[2003.80 --> 2005.02]  You close them.
[2005.80 --> 2007.32]  It's just like I've been there.
[2007.52 --> 2009.00]  I've been I've been on these things.
[2009.00 --> 2009.20]  Right.
[2009.50 --> 2011.42]  So it's like, OK, cool.
[2011.74 --> 2014.50]  What if we could like send an LLM to go try them?
[2014.72 --> 2015.60]  It can add a test.
[2015.68 --> 2017.70]  We can see we have a good test suite.
[2017.82 --> 2018.92]  We're happy with our CI.
[2019.26 --> 2020.60]  We're good with all this other stuff.
[2021.14 --> 2021.50]  Sweet.
[2021.50 --> 2027.74]  We just turned one hour of my time into 40 hours of my time for clearing out a bunch
[2027.74 --> 2030.10]  of these dumb little annoyances or fixing stuff.
[2030.50 --> 2035.56]  So there's a whole bunch of areas there where I can see already we don't need like a 10x
[2035.56 --> 2041.04]  improvement in LLM performance that I think would be valuable and like probably will get
[2041.04 --> 2044.84]  integrated in lots of places in like better and better ways.
[2044.84 --> 2051.92]  I'm a big fan of like not writing log statements manually like Prime and I were just talking
[2051.92 --> 2052.40]  about this.
[2052.64 --> 2057.06]  Like there's lots of little things where as I'm coding where if I have like an autocomplete
[2057.06 --> 2059.98]  tab, it's very obvious what my intent is.
[2060.40 --> 2062.54]  Like it doesn't take a super genius to figure out.
[2062.80 --> 2063.84]  I just wrote a variable.
[2064.36 --> 2066.84]  Now I started taking logging dot info.
[2066.84 --> 2069.90]  And it's like in blogging dot info.
[2070.12 --> 2071.80]  And now it says describe the thing.
[2071.98 --> 2072.40]  Here it is.
[2072.48 --> 2072.94]  Print it out.
[2073.04 --> 2073.38]  Format.
[2073.50 --> 2073.82]  Done.
[2074.26 --> 2075.82]  And those things like they add up.
[2075.88 --> 2077.20]  They really like they do.
[2077.30 --> 2081.46]  There's a bunch of stuff like this or, you know, you're writing Golang and it can complete
[2081.46 --> 2084.76]  the if statement with the correct error wrapping and a bunch of other stuff like that.
[2084.80 --> 2089.34]  Not just returning the error, but like the proper thing that you would normally skip because
[2089.34 --> 2091.70]  you're annoyed by writing the error handling.
[2091.70 --> 2100.04]  So so depending on the project, like I would have a I would feel kind of dumb for most projects
[2100.04 --> 2106.74]  right now, not using like an LLM tab complete at least because I just feel like it's a net
[2106.74 --> 2112.76]  benefit and I can skip it whenever I don't want to use it up to for some projects.
[2112.88 --> 2115.08]  I think like, yeah, I want to make a goofy website.
[2115.20 --> 2116.30]  I want to do something fast.
[2116.46 --> 2117.84]  I want it's very simple, straightforward.
[2117.84 --> 2121.78]  I could like completely see vibe coding that to 100 percent.
[2122.20 --> 2126.22]  And I also was like, to Casey's point, I don't need the website to work in six months.
[2126.66 --> 2127.52]  It's going to be gone.
[2127.96 --> 2128.32]  Right.
[2128.32 --> 2129.04]  So it's like it's fine.
[2129.14 --> 2132.92]  It's for a short thing or it's for a one day thing or promotional or whatever.
[2133.16 --> 2133.96]  Like we can do that.
[2134.04 --> 2138.02]  So, yeah, like I would say 25 percent to 100 percent.
[2138.14 --> 2138.50]  You know what I'm saying?
[2138.54 --> 2139.40]  Like in that range.
[2139.40 --> 2144.68]  But I'm also part of it, too, like Dax saying, I'm also pushing it to explore because I want
[2144.68 --> 2149.66]  to be like informed about about those things as we're like trying out different stuff.
[2150.84 --> 2150.98]  Yeah.
[2151.84 --> 2152.74]  Prime, what about you?
[2152.88 --> 2154.62]  Oh, hey, thank you, TJ, for asking.
[2155.04 --> 2155.88]  So I got you.
[2155.92 --> 2159.78]  I kind of do I do the same cycle every single time, which is OK.
[2160.04 --> 2164.02]  I'm going to just like really lean into this vibe coding thing.
[2164.08 --> 2165.08]  I'm just going to do it.
[2165.56 --> 2166.56]  I'm just going to start going.
[2166.78 --> 2168.30]  And I've got I've gotten a lot better.
[2168.30 --> 2172.18]  Hey, you know, one thing I do now in Cursor is I ask it to tell me first, come up with
[2172.18 --> 2173.82]  the plan where it would change what files.
[2174.00 --> 2177.66]  Why would it change them and really break it down and then ask it to complete it after
[2177.66 --> 2179.08]  I've corrected it like five times.
[2179.64 --> 2182.36]  But I even do no mistakes at the end, which is huge.
[2182.42 --> 2183.02]  No mistakes.
[2183.32 --> 2184.10]  Grandmother's in danger.
[2184.32 --> 2187.80]  And so it's like I do all these things like really, you know, I'm pretty good at it now.
[2188.14 --> 2190.02]  I am officially like the machine.
[2190.66 --> 2194.56]  What's called the machine spirits whisper from Warhammer 40K.
[2194.64 --> 2196.12]  Like I am a high priest at this point.
[2196.14 --> 2197.28]  I'm feeling pretty good about it.
[2197.28 --> 2203.72]  And I always I've come to the same conclusion multiple times thus far, which is that if
[2203.72 --> 2208.88]  it's a project I care about, I could have accomplished the same thing when I reach a
[2208.88 --> 2213.02]  certain level of complexity and made it better and then be able to add things faster.
[2213.44 --> 2216.64]  Eventually, they're just like there is an inflection point that exists somewhere that's
[2216.64 --> 2218.00]  really painful and kind of annoying.
[2218.00 --> 2222.76]  Once you get into like the once you once you get into the, you know, like the nitty gritty
[2222.76 --> 2223.14]  of things.
[2223.40 --> 2226.50]  But at the same time, there's a whole bunch of projects that I just want to create and
[2226.50 --> 2227.04]  just get them done.
[2227.12 --> 2230.42]  Like a good example of this is that Chessie project we're creating right now where we're
[2230.42 --> 2233.42]  going to have chat versus chat and chat versus a streamer.
[2233.42 --> 2238.14]  And it's just like, OK, I just want to get something up because like, is this even fun?
[2238.58 --> 2239.60]  I want to make sure like this.
[2239.66 --> 2242.22]  The idea is fun first before we do it.
[2242.24 --> 2244.94]  And I've already seen, OK, we need to add an additional notation.
[2245.46 --> 2250.88]  People cannot understand when I say from this space dash this space, people like capital
[2250.88 --> 2251.50]  letters, this space.
[2251.54 --> 2252.80]  I'm like, no, it's lowercase.
[2252.82 --> 2256.56]  And they're like, OK, this space, space, dash, space.
[2256.66 --> 2257.00]  This one.
[2257.06 --> 2257.80]  I'm like, no, I do.
[2257.86 --> 2259.04]  I gave you the exact format.
[2259.14 --> 2260.36]  How are you still screwing this up?
[2260.36 --> 2263.70]  And so it's like, OK, I can see all these things that I do incorrectly that would just
[2263.70 --> 2265.76]  make life way easier for people.
[2265.86 --> 2271.18]  If people can just be like pawn H4, it's just like, OK, can I even do that one?
[2271.24 --> 2274.90]  Probably I can't do that one just due to the fact that there's eight pawns.
[2274.90 --> 2277.08]  And I don't know if you can move it that way and how that would work.
[2277.10 --> 2280.74]  But you get the idea where it would it would be pretty fun to kind of work on.
[2281.10 --> 2284.10]  And so I like them in that case.
[2284.12 --> 2285.36]  And I've been using in more and more.
[2285.60 --> 2289.68]  But still, at the end of the day, it's like whenever I care about a project, what I want to
[2289.68 --> 2291.34]  do is I want to learn the problem space.
[2291.48 --> 2298.40]  And I find myself really struggling to learn the problem space if I don't code it myself.
[2298.56 --> 2303.02]  If I vibe code it, I have this like kind of like this really large void that exists where
[2303.02 --> 2306.84]  it's like I have to read the code, understand what the code does, and then attempt to understand
[2306.84 --> 2309.12]  the edge conditions, which I never programmed around.
[2309.42 --> 2312.42]  All the if statements, all that never were formulated in my head and understood it.
[2312.86 --> 2315.16]  So it's just like Dax was talking about with the rewrite to open code.
[2315.16 --> 2319.54]  The reason why you could rewrite open code was because you went through the process of
[2319.54 --> 2323.06]  writing open code, like you would not have started on whatever design you achieved.
[2323.22 --> 2324.76]  You started on the previous design.
[2324.86 --> 2328.28]  That's why you're changing, you know, the hidden file in the config, whatever, you know,
[2328.30 --> 2330.58]  like all these things you're changing due to that.
[2330.64 --> 2334.18]  And so there's like this, I have to gauge what side of this.
[2334.54 --> 2338.24]  And if that's the case, if it's something I don't care about, I don't care if it's 80%,
[2338.24 --> 2342.04]  90%, whatever, it's just a process of me getting out an idea.
[2342.24 --> 2348.44]  But when I do care, it seems like I go almost towards 0% because it's like I want the long
[2348.44 --> 2349.38]  term vision on this thing.
[2349.50 --> 2354.10]  And then I don't really consider something like Super Maven or autocomplete to be AI space
[2354.10 --> 2358.48]  when I say AI, just because it's like, did it correctly guess my for loop?
[2358.60 --> 2358.72]  Sure.
[2358.84 --> 2360.68]  Okay, that's just like a slightly better snippets.
[2360.80 --> 2361.46]  That's all that is.
[2361.46 --> 2362.84]  It's like, oh, did it guess my snippet?
[2362.92 --> 2363.08]  Correct.
[2363.22 --> 2363.50]  Awesome.
[2363.62 --> 2364.54]  Oh, it didn't guess my snippet.
[2364.58 --> 2364.74]  Correct.
[2365.02 --> 2367.22]  I'll just type five more characters and it will probably catch up.
[2367.22 --> 2368.58]  Or I'll just type the whole thing myself.
[2369.10 --> 2371.18]  So I don't really like consider a snippet.
[2371.18 --> 2374.14]  Better autocomplete than just search the text, right?
[2374.60 --> 2374.92]  Exactly.
[2375.10 --> 2378.84]  And so that one feels like I feel like I've said this for a long time, which is that like
[2378.84 --> 2382.06]  the small, I look at AI kind of like a weatherman.
[2382.26 --> 2386.44]  If you ask someone that's, or a meteorologist, if you ask a meteorologist, what's the weather
[2386.44 --> 2387.24]  going to be in 30 minutes?
[2387.28 --> 2389.42]  I know you wouldn't actually do this, but let's just pretend you didn't.
[2389.54 --> 2390.10]  There's a real person.
[2390.10 --> 2392.46]  The guys who do like Cancer and Gemini stuff?
[2392.60 --> 2393.64]  Yeah, yeah, exactly.
[2394.66 --> 2396.12]  You'd be like, what's the weather in 30 minutes?
[2396.12 --> 2398.34]  They'd probably give you a fairly accurate weather.
[2398.46 --> 2400.76]  It'd only be off by maybe one degree centigrade.
[2401.66 --> 2404.18]  Overall, it'd be a rare case that they're off by more than that.
[2404.64 --> 2410.14]  And so then as you go like one day or two days or five days, that prediction is effectively
[2410.14 --> 2411.54]  just insanity by the end.
[2411.90 --> 2414.72]  You know, what's going to happen in five days is just a made up number on my phone that
[2414.72 --> 2417.66]  I look at every single time and I'm disappointed afterwards when it's not that thing.
[2418.06 --> 2419.96]  And so it's like, okay, that's how I treat it.
[2419.98 --> 2423.32]  So if I can get it just online, then it's just like so accurate because I'm asking for
[2423.32 --> 2427.08]  the weather in 10 minutes, it's like, yeah, I can get like 95% of it almost every single
[2427.08 --> 2427.34]  time.
[2427.50 --> 2429.44]  And so that feels right.
[2429.54 --> 2432.94]  I'm willing to go back and give it another shot and really lean into it.
[2433.30 --> 2434.02]  Anyway, so there you go.
[2434.04 --> 2437.14]  There's my long answer, which is it's either zero or I'm close to full.
[2439.22 --> 2442.44]  I think that kind of brings up another point.
[2442.56 --> 2445.40]  I think a lot of the discussion and even products.
[2445.54 --> 2447.62]  Oh my God, of course, the vacuum turns on right when I start talking.
[2447.72 --> 2448.90]  Your name is Daxum for a reason.
[2448.90 --> 2449.90]  Daxum now, yeah.
[2452.00 --> 2452.96]  A lot of the, yeah.
[2453.02 --> 2456.56]  So a lot of these products are so focused on the marketing to our focus on the zero to
[2456.56 --> 2457.06]  one, right?
[2457.08 --> 2461.92]  Like you have nothing but an idea and here's how you get to the first version of it.
[2462.46 --> 2464.12]  And there's definitely use cases for that.
[2464.12 --> 2469.04]  Like, you know, TJ, both like TJ and prime brought up some cases where that is super valuable.
[2469.56 --> 2472.06]  I don't find myself in too many of those cases.
[2472.06 --> 2478.52]  Most of what I have are either existing projects or things that I'm kicking off that I know
[2478.52 --> 2480.60]  I'm like investing into the long term.
[2480.68 --> 2482.28]  I need to maintain for the long term.
[2482.88 --> 2488.02]  So I find it frustrating that the discussion gets stuck on the zero to one, like to do the
[2488.02 --> 2488.94]  zero to one really well.
[2489.22 --> 2492.10]  It's like, I don't really care about that when I'm going from zero to one for something
[2492.10 --> 2495.52]  serious, like prime just said, I'm going to build it myself because I'm discovering
[2495.52 --> 2498.52]  the patterns, the things that we need, like what, what makes sense.
[2498.52 --> 2504.20]  Um, and I'm more interested in bringing AI in once I'm at a good place there and it's
[2504.20 --> 2508.02]  now like an iterative project or like a maintenance project.
[2508.64 --> 2513.58]  Um, and I don't think a lot of the discussion or even the tools are, are focused in that space.
[2513.58 --> 2517.54]  Like all the big flashy tools that have like, you know, done quote unquote really well and
[2517.54 --> 2521.18]  raise a bunch of money all around, Hey, bring your idea to life.
[2521.30 --> 2525.76]  But I think we're going to go through this phase where we'll see where this lands.
[2525.76 --> 2531.00]  But, uh, I think oftentimes people imagine that, Oh, when you make something more accessible,
[2531.84 --> 2534.54]  everyone's doing it more like you've made creating software more accessible.
[2534.64 --> 2537.98]  That means every single person's gonna be putting software, but it could also do this
[2537.98 --> 2541.46]  much more boring thing where a lot of people try it out.
[2541.84 --> 2545.48]  And then there's like a trend where everyone's making their own software and then just goes
[2545.48 --> 2549.54]  back to how it was where everyone realizes, yeah, this, I don't actually want to be doing
[2549.54 --> 2549.86]  this.
[2549.94 --> 2554.46]  You know, I, I, I feel like that we kind of already saw like a micro version of this happen
[2554.46 --> 2555.58]  with like the game thing.
[2555.88 --> 2559.56]  Cause there was this phase where everyone was like vibe coding games.
[2559.56 --> 2561.92]  Like everyone's going to be vibe coding a game and all these games are going to be
[2561.92 --> 2562.38]  vibe coded.
[2562.54 --> 2564.54]  And we saw like a trend where that was cool for a while.
[2564.60 --> 2565.40]  People like enjoyed it.
[2565.42 --> 2570.16]  It was fun, but like, you know, not a large percentage of people ended up sticking with
[2570.16 --> 2570.40]  it.
[2570.68 --> 2572.38]  So I don't know anybody that has stuck with it.
[2572.38 --> 2573.78]  I think we're in this phase where it feels like to be fair.
[2573.86 --> 2574.40]  Us prime.
[2574.66 --> 2574.84]  Yeah.
[2574.96 --> 2575.24]  Us.
[2575.36 --> 2576.00]  We're still working on.
[2576.18 --> 2576.76]  That's true.
[2576.96 --> 2580.06]  We, we were kind of, we weren't true vibe coding.
[2580.06 --> 2580.46]  Yeah.
[2580.46 --> 2585.68]  We've spent many hours making it nice now doing a proper layout system, doing proper
[2585.68 --> 2587.36]  card system, all that stuff.
[2587.54 --> 2587.72]  Yeah.
[2588.24 --> 2588.52]  Yeah.
[2588.86 --> 2592.72]  I mean, I'll still count that as a win because you know, it brought you, it like gave you
[2592.72 --> 2595.54]  the motivation to do the initial part and that you stuck with it to the end.
[2595.58 --> 2595.66]  Right.
[2595.66 --> 2600.06]  So you can say the tool effectively converted you guys there, but yeah, it might just be
[2600.06 --> 2601.20]  a really, really small percentage.
[2601.20 --> 2604.92]  So I feel like this whole zero to one focus of being like, well, we're like one shot my
[2604.92 --> 2605.28]  idea.
[2606.24 --> 2612.18]  It's cool for like demos and marketing, but practically day to day, most people are iterating
[2612.18 --> 2614.06]  on old existing code bases.
[2614.26 --> 2616.72]  I assume that's part of the funding drive.
[2617.02 --> 2620.12]  Part of it is that like, look, it's not as big of a sale to say like, Hey, we're just
[2620.12 --> 2623.92]  making this thing for existing engineers that makes them incrementally better.
[2624.06 --> 2627.04]  It's way better to be like everyone in the world will be able to create software.
[2627.18 --> 2628.36]  Like that's probably a better pitch.
[2628.56 --> 2630.30]  Is that, you think that's why they're focusing on it?
[2630.38 --> 2631.84]  The TAM is huge.
[2632.04 --> 2632.20]  Yeah.
[2632.82 --> 2633.88]  Or is that not why?
[2633.96 --> 2634.42]  I don't know.
[2634.42 --> 2635.08]  I'm just asking.
[2635.62 --> 2636.74]  No, I think it makes sense.
[2636.78 --> 2638.56]  And I generally am in favor of stuff like that.
[2638.62 --> 2639.96]  Like they should try that.
[2640.04 --> 2643.96]  Like go ahead and see how accessible you can make it and see if you can, is this a thing
[2643.96 --> 2646.24]  that's in everyone's pockets that people do every single day?
[2646.78 --> 2647.04]  Sure.
[2647.10 --> 2647.80]  Go explore that.
[2647.88 --> 2652.20]  But to me, that's the, my only annoyance is that that's occupying like almost a hundred
[2652.20 --> 2653.02]  percent of the conversation.
[2654.14 --> 2658.32]  There's like, and we've talked about like working on open code.
[2659.52 --> 2663.28]  We're more gearing it towards like bigger established companies.
[2663.28 --> 2668.64]  They all tell us like, yeah, nobody is like doing the boring version of this, which is
[2668.64 --> 2672.32]  we have an established code base and we just need to be able to iterate.
[2672.50 --> 2673.06]  Like exactly.
[2673.16 --> 2674.20]  It's the case that TJ brought up.
[2674.44 --> 2676.40]  There's a backlog of a hundred things.
[2676.40 --> 2677.32]  That's never a priority.
[2677.76 --> 2680.42]  How do we build a tool that helps, helps get that down?
[2680.42 --> 2687.04]  Um, a lot more boring than the other thing, but practically that might just end up being
[2687.04 --> 2689.18]  where this stuff gets used.
[2689.18 --> 2694.02]  And I could honestly see most of software engineering, not totally changing besides that.
[2694.14 --> 2699.08]  The thing I would add with that too, is like, if you make it a lot easier to make prototypes,
[2699.08 --> 2704.70]  then like that does change how like professional software developers can iterate on things.
[2704.70 --> 2709.18]  And that's like also can change the game for stuff without making it so that like, it's
[2709.18 --> 2711.14]  useless to know programming skills.
[2711.48 --> 2711.92]  Right.
[2711.96 --> 2716.48]  It's like a bunch of things are way more accessible now than they've ever been before.
[2716.48 --> 2721.80]  And I still don't do any of them like, like woodworking or something like, you know what
[2721.80 --> 2721.94]  I mean?
[2721.94 --> 2727.80]  Like they have tools now for you to make crazy things with woodworking or like 3d printing
[2727.80 --> 2728.22]  or whatever.
[2728.22 --> 2728.42]  Right.
[2728.42 --> 2732.38]  Like everyone's like, everyone's going to 3d print every piece of furniture in their house.
[2732.38 --> 2734.70]  They're going to 3d print all of their forks.
[2734.76 --> 2736.20]  You're never going to buy a fork again.
[2736.38 --> 2738.06]  And I'm like, I don't want to do that.
[2738.08 --> 2740.44]  I just want to go to the store and buy new forks.
[2740.46 --> 2741.38]  Like that's fine.
[2741.48 --> 2743.66]  I hate the eight hours to wait till your fork's done.
[2743.74 --> 2745.88]  And then you realize it's too small and you're like, dang it.
[2746.02 --> 2748.56]  Oh no, I put, I uploaded spoon.
[2749.10 --> 2750.22]  Oh, shoot.
[2750.96 --> 2751.32]  Spork.
[2751.54 --> 2752.70]  And it's kind of stair steps.
[2752.86 --> 2755.90]  So it like catches on your mouth when you're, it's like not quite smooth.
[2756.84 --> 2757.20]  Yes.
[2758.36 --> 2761.96]  But so like, that's the other thing I feel like is missing a lot from the
[2761.96 --> 2762.46]  conversations.
[2762.60 --> 2764.46]  People like, Oh, it's so easy to go zero to one now.
[2764.50 --> 2765.78]  So like programming's useless.
[2765.78 --> 2771.14]  And you're like, dude, if I could go zero to one on a bunch of stuff that I can use for
[2771.14 --> 2776.18]  myself or like to test or to verify an idea or to do a bunch of other stuff.
[2776.60 --> 2777.34]  That's awesome.
[2777.34 --> 2781.70]  That can be like a huge change in the way that I do stuff without making it.
[2781.78 --> 2786.42]  So like, it's useless to code actually coding a thousand X's me on all of those
[2786.42 --> 2790.64]  categories compared to random person who doesn't know the words they're typing
[2790.64 --> 2792.68]  saying, please make website.
[2792.80 --> 2793.08]  Right.
[2793.08 --> 2796.28]  If I can be like, I need a website with these characteristics, make sure we have a
[2796.28 --> 2796.62]  database.
[2796.62 --> 2798.10]  It should have indexes on these columns.
[2798.10 --> 2801.32]  I'm going to be querying in these kinds of ways with this kind of traffic.
[2801.32 --> 2803.72]  It needs to be able to handle this sort of login flow.
[2803.94 --> 2805.16]  Please use OAuth.
[2805.28 --> 2810.66]  Don't you like my website will turn out better vibe coded or not.
[2810.66 --> 2810.90]  Right.
[2810.90 --> 2812.92]  So it's like the skills are still useful.
[2812.92 --> 2815.38]  That's the thing I'm like, guys, it's still good to know things.
[2816.10 --> 2816.38]  Yeah.
[2816.96 --> 2817.28]  Yeah.
[2818.08 --> 2823.18]  The other thing you mentioned, like, I think some of these headlines around, I
[2823.18 --> 2824.78]  think, I think YC is guilty of this.
[2824.78 --> 2829.18]  They'll say stuff like we interviewed all of our founders and like, they're
[2829.18 --> 2832.60]  saying 95% of their code bases entirely vibe coded.
[2833.04 --> 2837.08]  But a very simple explanation for that is these are early stage companies.
[2837.08 --> 2841.90]  When you go through YC, the process is roughly talk to some customers, spend a
[2841.90 --> 2844.24]  week building something, go show it to them.
[2845.24 --> 2847.18]  You're going to realize, oh, we built the wrong thing.
[2847.52 --> 2848.16]  Go back.
[2848.52 --> 2849.08]  Try it again.
[2849.20 --> 2849.64]  Go talk.
[2849.70 --> 2851.68]  And then eventually you would kind of maybe land on something.
[2852.20 --> 2856.96]  You can see why in that environment, a large percentage of stuff is vibe coded.
[2857.68 --> 2859.44]  And they go faster because of it.
[2859.90 --> 2860.40]  Which is good.
[2860.50 --> 2860.84]  You know, like.
[2860.94 --> 2861.90]  So it's like perfect for that.
[2862.00 --> 2862.12]  Yeah.
[2862.12 --> 2867.78]  But yeah, I think those headlines confuse people because they think, oh, they're
[2867.78 --> 2871.96]  claiming that all companies are like all systems are going to have this split.
[2873.06 --> 2873.38]  Awesome.
[2873.64 --> 2873.86]  Well, Dax.
[2873.86 --> 2874.40]  Are we losing Dax?
[2874.40 --> 2875.58]  I know you're out of time.
[2875.72 --> 2876.04]  We're losing Dax.
[2876.76 --> 2877.52]  Dax, you're out of time?
[2877.54 --> 2878.16]  You're out of time.
[2878.60 --> 2880.46]  Because he's got to go to a doctor's appointment, he said.
[2881.32 --> 2881.66]  I do.
[2881.66 --> 2883.94]  So I want to have Dax back on though, Prime, if you're up for it.
[2884.02 --> 2886.02]  I want to talk about building an agent next time.
[2886.02 --> 2886.14]  Yeah.
[2886.18 --> 2886.46]  I want to do.
[2886.54 --> 2887.66]  I want to talk about building an agent.
[2887.76 --> 2888.98]  Can we do that here soon, Dax?
[2888.98 --> 2892.70]  Because I'm actually very curious what it takes because I heard a really weird argument.
[2892.80 --> 2894.60]  Someone's like, oh, you can just build an agent over the weekend.
[2894.84 --> 2896.00]  And I thought, okay, yeah, like.
[2896.38 --> 2896.94]  It's very easy.
[2897.10 --> 2897.34]  Yeah.
[2897.72 --> 2898.04]  Okay.
[2898.04 --> 2899.94]  I assume you can like do some stuff.
[2899.94 --> 2902.56]  But what does it take to build like a real agent that's doing that?
[2902.68 --> 2903.28]  A good one.
[2903.36 --> 2907.82]  I don't mean like, yes, it can query open AI to be like, what's what file should I use?
[2907.82 --> 2908.00]  Right.
[2908.00 --> 2909.16]  Like, what's the actual.
[2910.58 --> 2915.24]  Well, I spent so much time building this thing and I'm just like, what the hell is an agent?
[2915.24 --> 2918.38]  Like everyone keeps saying agent and I like have built this thing.
[2918.38 --> 2921.02]  And I don't understand what people mean when they say agent.
[2921.56 --> 2923.22]  I think they just mean calling.
[2923.66 --> 2924.02]  Yes.
[2928.20 --> 2929.16]  Very easy to build.
[2929.16 --> 2929.96]  Well, don't spoil it.
[2929.96 --> 2930.84]  Yeah, don't spoil it.
[2931.48 --> 2932.76]  Josh, bleep that out.
[2932.94 --> 2933.92]  But there's a secret to it.
[2933.92 --> 2934.96]  There's a secret to it.
[2935.02 --> 2935.94]  Not to the secret next time.
[2936.18 --> 2937.18]  Just bleep it out.
[2937.86 --> 2939.22]  And they'll find out next time.
[2939.22 --> 2940.68]  And putting a last thing over his mouth like this little square.
[2940.68 --> 2941.24]  Exactly.
[2941.24 --> 2942.10]  Can't see what he's saying.
[2943.06 --> 2944.42]  All right, Dax, how about you leave?
[2944.42 --> 2946.18]  And I'm going to play our thing, TJ.
[2947.04 --> 2948.44]  Yes, that's what I was going to say, too.
[2948.56 --> 2949.02]  OK, Dax.
[2949.12 --> 2949.64]  Thanks, Dax.
[2949.72 --> 2950.24]  You get out of here.
[2950.36 --> 2950.76]  Thanks, everybody.
[2950.96 --> 2952.52]  We'll see you on the next episode of The Stand Up.
[2952.52 --> 2954.06]  Boot up the day.
[2955.76 --> 2958.88]  Vibe cult and errors on my screen.
[2960.24 --> 2963.72]  Terminal coffee and hair.
[2964.72 --> 2966.72]  Live in the dream.
