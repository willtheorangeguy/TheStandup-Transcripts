[0.00 → 4.48] Today is the stand-up episode four where we're going to be talking about all the things that
[4.48 → 8.14] are important to devs. My name is The Primo gen, the host, and today with me, we have...
[8.14 → 8.66] My name's TJ.
[9.42 → 10.40] Ha ha, yes!
[10.78 → 12.02] Nice. And who else?
[12.70 → 13.14] Casey.
[13.76 → 14.16] Trash.
[14.60 → 15.28] Trash, you're frozen.
[15.28 → 15.80] Let's go.
[16.02 → 17.84] But you're good enough, and so that is good.
[18.10 → 21.18] All right, frozen trash is pretty much the same thing as a normal trash.
[21.42 → 22.40] So today's first...
[22.40 → 22.60] Wait, I'm frozen?
[23.08 → 23.76] Yeah, well, you were.
[23.76 → 31.62] Today's first hot topic is going to be a court case that was recently tweeted by Casey himself
[31.62 → 38.64] describing that AI and the training data for AI may or may not actually be legal to use.
[39.44 → 41.56] And so what are the implications coming on down?
[41.64 → 45.32] This is with Ghibli Studio, which Casey specifically pinged in it.
[45.88 → 48.38] And so what are going to be the outcomes?
[48.54 → 52.64] So Casey, since you opened to this can of worms, why don't you guide us somewhere with this conversation?
[52.64 → 57.30] Wow, you're going right into heavy-duty stuff here, Prime.
[57.46 → 59.64] This is the stand-up. This is not the mess around, okay?
[59.74 → 61.12] We don't talk about our dogs here.
[61.44 → 62.30] All right, all right.
[62.76 → 65.34] So no, this was not about Studio Ghibli.
[65.68 → 73.54] This is actually from a case, its Thomson Reuters are the people who are filing this particular lawsuit.
[73.66 → 79.60] Or I should say the people who are worried that their copyrighted work is being infringed unfairly.
[79.60 → 90.70] And in this lawsuit, so just to give you sort of the big overview, everyone agrees that AI training is copyright infringement,
[90.80 → 94.46] at least in the sense that they are taking copyrighted works without licensing them.
[95.06 → 96.96] So there's no argument there.
[97.20 → 101.00] Like, AI companies aren't going into court and saying, like, what?
[101.16 → 103.22] No, we didn't use any copyrighted work.
[103.30 → 104.44] Like, no one's saying that, right?
[104.44 → 109.82] They're all saying, yes, we did, but their argument is that somehow this is okay, right?
[110.22 → 115.04] And so what happens when you go into court on that footing, right?
[115.08 → 120.60] Your legal footing is we're violating all these copyrights, but it's okay for us to do it.
[120.94 → 123.58] You have to provide what's called a fair use rationale.
[123.58 → 128.32] And just to be clear about what this is, copyright is a worldwide thing.
[128.42 → 129.18] It's international.
[129.64 → 137.66] The U.S. is a party to something called the Bern Convention, which is like a uniform agreement that copyright is respected everywhere, right?
[137.80 → 140.22] So it's not some weird, like, only U.S. thing.
[140.34 → 144.22] It's not some, like, finicky thing that only Congress decided or anything like that.
[144.22 → 154.68] This is a worldwide agreement that when someone writes something, they have the copyright for it, and they have this right to exclude other people from using it in ways that they don't like, right?
[155.70 → 162.74] Now, in the Bern Convention, it allows countries to make exceptions to this on very limited grounds.
[162.74 → 171.24] So basically, a country is allowed to carve out some special uses where you can make a copy without the original author's permission.
[171.24 → 177.64] And in the U.S., what we have carved out, you know, over the years for that is called fair use, right?
[177.90 → 181.40] Now, this doesn't just come from the Bern Convention because copyright existed before that.
[181.52 → 184.24] But the Bern Convention is now kind of the backstop, right?
[184.32 → 185.32] It was in the 70s.
[185.56 → 190.34] There's been – people keep ratifying it, so it gets – you know, more and more countries get into it until now.
[190.46 → 191.38] It's basically everybody.
[191.56 → 198.42] So in order to have a fair use exception, in order to say I can, you know, take these copyrighted works and use them without a license,
[198.42 → 206.34] you need to demonstrate that your use doesn't, you know, doesn't violate – there's basically this four-pronged test.
[206.96 → 214.82] And the most important of the prongs is essentially that you are not doing any economic damage to the rights' holder, right?
[214.90 → 216.46] That's the very most important one.
[216.68 → 218.60] The Supreme Court has said it's the most important one.
[218.60 → 231.54] All the courts have consistently followed that guidance that, like, if what you are doing is creating an economic negative outcome for the original rights' holder, it's probably not fair use, right?
[231.60 → 243.50] Like, it's highly unlikely that it's going to be fair use because that's the main thing that copyright was designed to do is to give money to the people who made the works so that they will continue to make them.
[244.14 → 245.54] So hopefully that all makes sense.
[246.06 → 246.62] I think it does.
[246.62 → 246.88] I have a question.
[247.44 → 247.80] Oh, yes.
[249.40 → 254.22] So you said the burn convention, that's the thing in the desert that people go to every evening?
[254.48 → 255.20] Burning Man?
[255.84 → 257.40] TJ, how long have you been planning this joke?
[257.78 → 258.88] Like, how long has it been?
[258.88 → 259.76] That's immediately after you said burn convention.
[259.76 → 262.96] He thought about it as soon as he said, I saw his light bulb just go off.
[263.54 → 269.32] I was just waiting for a pause so I couldn't make that joke, but I was going to pretend I didn't know for real.
[269.42 → 269.70] Sorry.
[269.94 → 271.70] Brian, you – okay, Casey, sorry.
[271.86 → 273.22] No, that's how it works.
[273.22 → 276.16] All the countries have to send their ambassador out into the desert.
[276.44 → 276.74] That's what they said.
[276.74 → 281.30] They all make a huge, weird thing like sculptures and people serve drinks.
[281.48 → 282.22] Everyone's high.
[282.64 → 284.00] And then they sign this treaty.
[284.14 → 285.14] They don't know what it says.
[285.22 → 289.16] It's probably written in some foreign pictograph-like language thing that hasn't existed.
[289.30 → 289.74] You know, Sanskrit.
[289.74 → 293.10] And then – and that's how we do copyright now.
[293.32 → 294.08] And it's fine.
[294.14 → 295.32] Sometimes they get stranded there.
[295.54 → 299.94] We hope there's not a windstorm because then there'll be no copyrighted works the following year.
[300.46 → 300.68] Yeah.
[300.92 → 301.70] I mean, it makes sense.
[301.82 → 302.74] That seems to be on par with government.
[302.74 → 308.26] That's the only reason that the AI companies would think they could just steal everyone's stuff and then make money off of it.
[308.26 → 314.08] So to get into – this is why I said you're opening a huge can of worms here because, like, I love studying law.
[314.14 → 316.20] It's one of the things I do in my spare time.
[316.54 → 316.72] Me too.
[316.72 → 318.42] So, you know, I just – I'm into it.
[318.42 → 327.30] So anyway, the thing that I've been saying for quite some time, actually, is I don't really understand AI companies' defence.
[327.70 → 339.06] And the reason for that is their primary defence has been that this is fair use because the thing that comes out of the AI is not going to economically compete with the things going into it.
[339.56 → 342.08] Now, that's kind of tenuous on its own.
[342.42 → 345.30] And we don't really need to go into that to talk about this particular tweet.
[345.30 → 358.54] I think that's a pretty tenuous defence to begin with because they rely on a thing called Authors Guild v. Google, which if you read it, seems like it would have come out the other way had Google had their AI summaries at the time, the case, right?
[359.58 → 363.30] At that time, that case was litigated.
[363.80 → 365.60] Google just had search results, right?
[365.68 → 367.64] And the little snippet from the web page.
[367.64 → 375.22] It didn't have the thing that basically makes it unnecessary for you to go to, say, Box Office Mojo to find out what the box office is.
[375.52 → 380.26] So I think actually Google would have lost Authors Guild v. Google today.
[380.48 → 382.94] And I think AI companies aren't appreciating that.
[383.06 → 386.42] So I think that defence is also very tenuous.
[386.54 → 388.98] But you can put that aside because maybe you don't think it is.
[389.06 → 391.86] Maybe you read Authors Guild v. Google differently than I do.
[392.02 → 392.38] Whatever.
[392.64 → 395.46] The thing that I've been saying is who cares about that?
[395.54 → 397.70] The market is AI training data.
[397.70 → 398.98] We know that's a market.
[399.16 → 400.66] I know you guys have received this too.
[400.82 → 401.88] I have a YouTube channel.
[402.00 → 404.46] It's not even remotely as popular as Primes, for example.
[404.46 → 412.54] I've got emails from people offering to pay me money to license my YouTube videos for AI training.
[413.10 → 417.08] So we don't even need to hypothesize that the market exists.
[417.26 → 418.40] It already does.
[418.58 → 421.52] People are already paying money for this content.
[421.60 → 424.20] So I'm like, look, it's an open and shut case.
[424.68 → 427.02] The market is the market for AI training data.
[427.02 → 436.36] They're not paying us, and they owe us because we could be generating revenue from our content by licensing it to AI companies to put it into their algorithms.
[437.02 → 439.42] And so I've been saying that for quite some time.
[439.76 → 441.60] That's been my general position on what this is.
[441.82 → 445.08] We don't know what the courts will find, but other people have made this argument.
[445.30 → 446.88] New York Times made this argument in court.
[446.88 → 448.02] We don't know.
[448.34 → 450.00] Well, made this argument in the initial filing.
[450.14 → 453.66] Their case hasn't percolated through the courts enough to get an opinion on that yet.
[454.38 → 462.32] But this first opinion that came out in February that I missed, I only read it yesterday, actually holds exactly that.
[462.54 → 471.80] In this opinion, finding for Thomson Reuters that their copyright was violated, and it was not fair use, the court said literally this.
[471.80 → 476.04] It said there is potentially a market for this training data.
[476.92 → 478.76] You are stealing their copyright.
[479.04 → 481.78] You're infringing their copyright in that market.
[482.18 → 485.58] And that could have negative ramifications for that was literally in the opinion.
[486.24 → 491.04] And so I thought this was a very positive sign because this suggests to me that courts are thinking that way.
[491.52 → 492.82] Lawyers are making that argument.
[493.04 → 498.20] And it's very possible that it will be found to be that way as more of these cases go through.
[498.20 → 503.74] So to me, this was a very positive outcome because I think companies should have to pay for their training data.
[504.16 → 506.92] And that's the entire long-winded thing.
[507.02 → 508.22] So hopefully everyone now has context.
[508.58 → 509.50] Take it away, team.
[510.10 → 510.50] Wow.
[511.02 → 512.64] That was definitely some context.
[512.96 → 514.86] I feel like, TJ, I can just see you.
[514.94 → 516.86] The whole time you were bobbing, you were ready.
[517.70 → 518.68] I missed the whole thing.
[518.96 → 520.68] Because I already have a lot of feelings on it.
[520.96 → 522.56] I'm feeling a lot of things here.
[523.38 → 525.46] I have a funny question and a serious question.
[525.52 → 526.36] Which one do you want, Prime?
[526.36 → 529.82] I'll obviously start off with TJ Funny, which is the best funny.
[531.52 → 535.58] If they trained on Trash's code, does Trash have to pay them or do they have to pay Trash?
[535.80 → 537.02] Ooh, sick bird.
[537.06 → 537.46] No, no, no.
[537.46 → 537.76] God of Trash.
[537.76 → 539.00] For your code, Trash.
[539.14 → 539.78] Yeah, yeah.
[539.84 → 540.52] No, they pay me.
[540.52 → 541.06] For liability.
[541.38 → 541.78] For liability.
[541.78 → 543.46] Because you damaged them, brother.
[543.46 → 545.92] Okay, they pay you, but you are liable, Trash.
[546.04 → 548.08] You are liable for the code, but they pay you.
[548.54 → 550.16] Only I can debug that code, baby.
[550.78 → 551.62] Job security.
[551.62 → 555.16] When the AI company gets sued for the output, right?
[555.30 → 558.42] Like when it's like, look, this thing, you know, deleted all of our data.
[558.76 → 563.36] And they're like, well, I mean, it's just, we can trace it back to this training set from Trash.
[563.46 → 564.84] And I don't know what to tell you.
[564.94 → 565.50] Talk to him.
[565.66 → 567.16] Trash has been selling all of his data.
[567.36 → 568.50] He just cannot wait for this.
[568.58 → 569.90] He's just, come on, get all my code off.
[569.90 → 570.88] I'm kind of sabotaging AI.
[570.88 → 575.20] Okay, my serious question is though, like, so let's say codes on GitHub.
[576.58 → 578.12] Did I sign something?
[578.30 → 581.16] I mean, obviously I read the TOS, so we can put that away.
[581.36 → 581.98] Obviously I read it.
[582.06 → 583.08] I'm just testing you, Casey.
[583.30 → 589.14] Did I sign something to let GitHub let people do this without my permission?
[590.38 → 594.04] So that's going to be a perfect question and a really difficult question.
[594.04 → 608.94] I haven't tried to look into that because the reason for that is there's such a thing as a contract of adhesion, which is what you are effectively sort of quote unquote signing when you agree to use a service, right?
[609.00 → 610.86] In a way, it depends on how you want to look at it.
[610.90 → 613.44] Terms of service and contract of adhesion are technically a little bit different.
[613.68 → 620.18] Like a contract of adhesion is more like the thing you sign when you go to get a cell phone, you know, or something like that.
[620.48 → 622.06] But it's the same kind of principle.
[622.06 → 625.60] There are limits to what you can give away.
[626.06 → 633.80] You know if the cell phone company had buried a little tiny clause in that giant contract that said, and also we can human traffic all your kids or whatever, right?
[633.82 → 636.90] It's like it doesn't suddenly, I mean, that's going to be criminal as well.
[636.98 → 637.76] But I mean, you know what I'm saying?
[637.88 → 649.40] Like they can't just put something arbitrarily egregious that would, you know, quote unquote, offend the conscience into one of these boilerplate things and then actually expect that to get them out of jail free when they go to court.
[649.40 → 653.90] And so I would have to say that's a much harder question.
[653.90 → 666.68] I would say you probably I would imagine you wouldn't have to worry too much about Microsoft's competitors being able to do it because the only people you probably gave an agreement to was them.
[666.68 → 673.08] So it might be training co-pilot or maybe an open AI through some agreement.
[673.08 → 679.04] But Google would probably never be allowed to do it because Microsoft wants the competitive advantage of all the source code on GitHub.
[679.62 → 685.40] My recommendation, and I have been thinking about doing this for a while, does I think everyone should probably be going dark these days.
[685.58 → 691.56] Everyone should be pulling stuff and putting it on their own website with a little notice that says if you want to use this for AI training, you have to pay me.
[691.56 → 693.74] So that's that's I think the best practice.
[693.92 → 700.20] You know, if like let's say you gentlemen are distinguished web devs, and you know a lot of distinguished web devs.
[700.42 → 707.20] If you put up the AI safe GitHub tomorrow, I would move everything that I have to it literally as soon as it was up.
[708.00 → 709.86] And I think a lot of other people would too.
[710.06 → 712.10] And I would also encourage everyone else to do so.
[712.10 → 716.08] So that that would be a great competitor to GitHub if anyone wanted to do that.
[716.64 → 728.36] I would even pay a fee to do that, meaning I would pay for a good GitHub clone out there that would get me away from licensing Microsoft implicitly.
[728.48 → 735.06] Because, like I said, I don't know if that it will turn out to be something that they're allowed to do by slipping it in a TOS.
[735.56 → 736.78] But it might be.
[736.86 → 738.48] And until a court case, we wouldn't know.
[738.56 → 740.50] So the safer thing to do is to just not use it.
[740.50 → 752.42] So could you use an AI, an LLM to generate a GitHub like interface trained off of code on GitHub to then launch a private AI free service?
[752.56 → 753.58] Is that what you're trying to tell me?
[755.04 → 757.18] It would be an interesting thing to do.
[757.28 → 763.24] But again, based on what we have seen, you may very well be liable for copyright infringement because of the way these court cases are going.
[763.24 → 782.74] Because if you weren't Microsoft and don't have the effective, like, you know, in the GitHub terms of service, there are probably some clauses that say, like, you grant Microsoft, or, well, GitHub, but, you know, as a subsidiary of Microsoft, worldwide, you know, license to reproduce this work or whatever it is, right?
[782.74 → 784.92] Because they need to for operating the service.
[785.12 → 788.42] And they'll just argue, well, co-pilot is part of the service, so we train it, right?
[789.06 → 790.52] Other people don't have that out.
[790.70 → 793.72] They can't go to court and say, like, well, you agreed to it, right?
[794.22 → 796.10] And so, eh?
[796.10 → 806.18] So that's kind of a yeah, I mean, that's going to be a gray area, even if things go the way this court case did and the way that I think they should, which is the companies need a proper license.
[807.32 → 808.64] You know, who knows?
[808.84 → 810.64] Who knows what that looks like?
[810.74 → 821.12] Will anyone using AI, is there any sort of, like, trickle down to this where if people who are using AI, will they become also, like, tangentially liable for what they have generated?
[821.12 → 828.78] Yeah, so that's very possible, but also I think that really depends on people.
[829.76 → 835.76] So this will transition nicely into another thing I want to talk about, and you just tell me when we're ready to transition.
[836.00 → 836.86] Tell me when.
[837.08 → 838.42] I have it.
[838.52 → 839.00] I'm ready.
[839.62 → 840.04] Hold on.
[840.12 → 841.04] I want to say something.
[841.20 → 841.42] Okay.
[841.58 → 844.12] So we're going to, let's get through all this first, and then we'll transition.
[844.26 → 844.88] So to say like that.
[845.20 → 846.00] Let me talk.
[846.50 → 846.86] Trash.
[848.18 → 848.58] Okay.
[848.58 → 854.06] I'm going to bring this back to the Studio Ghibli scenario, because that's what everyone's probably familiar with, right?
[854.42 → 855.54] Where that was a whole big debacle.
[855.70 → 859.08] Yeah, Trash isn't checking out the laws, but he did see the pictures on the internet.
[859.30 → 861.30] He saw Sam Olsen looking quite wibbly.
[861.30 → 862.84] I'm the last person that you want to talk to about law, okay?
[863.10 → 864.20] No, you said you study it.
[864.24 → 864.90] You said you study it.
[864.98 → 866.48] He knows what he's talking about.
[866.54 → 868.22] I studied it when I fired it all night.
[868.22 → 869.02] That was not real.
[871.46 → 871.82] Okay.
[871.96 → 872.98] Riddle me this scenario.
[873.24 → 873.44] Okay.
[873.44 → 874.70] So we're going to bring this back to art.
[874.84 → 877.54] And I think art is, like, where it gets really, really muddy.
[877.54 → 883.48] It's probably not as cut and dry as, like, this Reuters scenario, which is, like, a very niche kind of area, I feel like.
[884.06 → 885.56] So riddle me this.
[885.62 → 886.24] I'm an artist.
[886.78 → 888.88] I don't really care if people use my work.
[888.94 → 889.94] I just want to be well-known.
[890.16 → 891.42] I just want people to see my work.
[891.98 → 894.56] But as a human, I have inspirations from other people.
[894.64 → 895.66] Maybe it's Studio Ghibli.
[895.66 → 900.96] Maybe it's a number of other very popular artists where it's very obvious if I'm stealing their work.
[901.84 → 904.86] So when I draw, you're probably going to see a little bit of those details come out.
[905.56 → 905.72] All right?
[905.74 → 910.42] But I allow you to use my work for your training data because I don't care.
[910.52 → 912.04] I have a full-time job doing something else.
[912.46 → 913.84] You know, drawing is just my hobby.
[914.56 → 918.08] But when you're trading on my data, some of those little qualities come out.
[918.14 → 919.84] Maybe I like Studio Ghibli eyes, you know?
[920.20 → 921.72] Maybe I like Dragon Ball Z abs.
[921.90 → 922.54] I don't know.
[922.96 → 928.18] So what happens if that comes out and then someone else notices it, and you're like, wait a minute.
[928.90 → 930.80] That's Good's six-pack, right?
[932.12 → 933.30] Will I be in trouble?
[933.46 → 934.36] Do you get in trouble?
[934.36 → 941.16] Or is it fair use because, I don't know, I put like an extra shadow on one of the abs or something.
[941.26 → 941.56] I don't know.
[942.28 → 943.20] So how does that work?
[943.56 → 949.62] You're basically talking about is there a scenario where there's effectively copyright laundering?
[950.02 → 951.72] So we take Studio Ghibli.
[951.94 → 954.58] Some other artist draws something sort of like it.
[954.70 → 958.82] And then that artist gives them permission to train, gives an AI permission to train.
[959.10 → 960.32] That's what we're talking about, just to be clear?
[960.40 → 960.52] Yeah.
[960.52 → 967.22] My assumption, A, that is the reason we have courts.
[967.56 → 974.32] In other words, for weird corner cases like this, if someone really cared and was worried about it, they would go to court.
[974.46 → 982.42] And a judge or a jury would look at that evidence and say, do we think this was really against the spirit of copyright law or not?
[983.18 → 985.84] My gut feeling is that would be fine.
[985.84 → 994.98] Meaning that, you know, there isn't, Studio Ghibli is not marketing educational materials for humans right now.
[996.18 → 999.60] And they are not marketing materials for AI training right now.
[999.80 → 1004.20] If things go the way I assume, they will do the latter only.
[1004.32 → 1008.10] They will say, okay, here is our pricing for AI training.
[1008.10 → 1010.42] Because now that's a market, and we understand that.
[1010.66 → 1013.10] So AI companies have to pay that if they want to.
[1013.10 → 1019.52] If they don't, then say, also, if humans want to train on it, you have to pay us this, right?
[1019.62 → 1022.20] And therefore, these are educational materials.
[1022.84 → 1023.62] Then no.
[1023.94 → 1028.38] And also, that thing, again, all of this stuff is based on licensing.
[1028.70 → 1036.32] If they provided a license with their training course that says, also, you agree not to draw things too close to our style.
[1036.32 → 1042.18] Well, right, if they don't do that, then there has been no, like, that causal chain is not broken.
[1042.86 → 1050.16] A lot of people get confused because they think for some reason humans and machines have to be treated identically or something like this.
[1050.16 → 1058.04] But actually, not only do humans and machines not have to be treated equally under copyright law, but humans and humans don't have to be treated equally under copyright law.
[1058.30 → 1064.26] You can sell something for one price to professionals with one license that says you can do anything you want to it.
[1064.50 → 1065.68] And by the way, it's $5,000.
[1066.40 → 1071.40] And another one to students that says it's 50 bucks, and you can't use this for anything that makes money.
[1071.40 → 1072.72] That's completely legal.
[1073.20 → 1080.16] So, again, you can copyright law is supposed to allow the person who made the thing to determine how it is used.
[1080.22 → 1086.68] And if Studio Ghibli wants to make special screenings of their movies where a big thing comes up on the screen that says no one can draw like this.
[1086.76 → 1094.48] If you are going to look at this movie and try to draw like this, leave the theatre now because that is not allowed, and your ticket price will be refunded.
[1094.82 → 1095.58] They can do that.
[1096.04 → 1099.12] Is that contract of adhesion that we talked about earlier?
[1100.06 → 1100.46] Yes.
[1100.46 → 1113.34] Do you guys remember when on Facebook, like probably 10 years ago, when everyone started posting that Facebook cannot use my posts for like anything and everyone just kept posting it as if it did anything, and it was the funniest thing ever?
[1114.24 → 1116.76] I'm pretty sure my grandparents have been doing that recently.
[1117.08 → 1119.24] Actually, these posts are not allowed to be used by AI.
[1119.24 → 1121.60] My parents are just like, you do not have my permission.
[1121.76 → 1122.62] I'm just like, oh my God.
[1122.66 → 1123.90] It's too late, brother.
[1124.42 → 1126.60] Your parents need to put a price tag on it.
[1126.68 → 1127.40] That's what they're missing.
[1127.74 → 1128.64] Put a price tag on there.
[1128.64 → 1131.40] Say like, in order to use this for training, it costs $1,000.
[1132.32 → 1133.04] Put it on there.
[1133.46 → 1134.12] Put it on there.
[1134.32 → 1135.88] It might help you out someday.
[1137.06 → 1138.36] Here's my next question.
[1138.36 → 1152.78] Suppose they find out, okay, couldn't train on the GitHub stuff or like couldn't train because people had some GPL license on their code, and it was in clear violation of the license as stated, the way that they used to train it.
[1153.28 → 1153.34] Right.
[1153.34 → 1157.24] Do they have to delete the models?
[1157.96 → 1160.34] Like what is the next step?
[1160.40 → 1161.72] Let's say they find it out, right?
[1161.94 → 1167.60] Is it like we got to roll these back, and we're starting back at GPT 2.0?
[1167.60 → 1168.00] Yeah.
[1168.16 → 1169.16] Like, yeah.
[1169.32 → 1176.16] So, again, this is, and it's important for everyone to remember all the things we're talking about are somewhat hypothetical.
[1176.44 → 1178.18] We know what happened in the Reuters case.
[1178.42 → 1181.76] It's a district court, which means there are two more levels to go.
[1181.88 → 1182.02] Right.
[1182.04 → 1186.06] Because AI companies are going to have to fight this because, you know, they're in a lot of trouble potentially.
[1186.06 → 1191.46] So, there's district court that gets appealed to like an appellate circuit, right?
[1191.50 → 1193.44] You always feel like the Ninth Circuit or something like that.
[1193.52 → 1195.12] Then that gets appealed to the Supreme Court.
[1195.22 → 1195.88] So, we've got –
[1195.88 → 1196.60] We all knew this.
[1196.68 → 1199.16] We don't know what we're talking about right now, right?
[1199.20 → 1205.64] So, it's just important everyone's aware that we're just saying hypothetically if this stuff happens, here's what's going to go.
[1205.64 → 1206.96] But we don't know that.
[1207.62 → 1217.04] So, anyway, to your hypothetical, right, usually in copyright cases, the first thing you get is injunctive relief.
[1217.94 → 1221.58] They have to stop selling whatever the thing is that infringed, right?
[1221.84 → 1222.04] Right.
[1222.32 → 1228.08] So, my assumption is that at the very least, they would have to literally rerun the training round.
[1228.62 → 1230.30] They would have to pull that content.
[1230.30 → 1238.74] Like, let's say Studio Ghibli won it, they'd have to pull all the frames of all the Studio Ghibli things and rerun the $50 million training round.
[1238.82 → 1247.74] I don't know what they cost these days to run, but, you know, they're in the tens, or I don't know if they're in the hundreds ever, but they're in the tens of millions oftentimes is my understanding.
[1247.76 → 1248.98] What about like distillations?
[1249.42 → 1258.80] Like things that like – you know, like R1, one of its secret sauces was that, you know, it could verify some of its data against open AI and be like, okay, so this is like a great way to predict what's coming up next.
[1258.80 → 1269.72] Does that also – excluding if China were to listen to it – does that also mean that all derivative things that used O1 as part of their training set also have to then fall under the same copyright problem?
[1271.54 → 1274.22] Potentially, but again, maybe it's too attenuated at that point.
[1274.30 → 1274.88] I don't know.
[1275.10 → 1287.84] And again, this is why I say like this is going to be something courts are going to be dealing with for a while because it's like that, you know, just Studio Ghibli see that, you know – well, and the other thing too is people always say like, oh, well, you know, that'll just ruin U.S.
[1287.84 → 1289.36] It's like, no, this will apply to everybody.
[1289.96 → 1298.26] If China can't come into the U.S. market and sell something they trained somewhere else, if it was based on copyrighted data, they'll get the same exact injunction slapped on them, right?
[1298.26 → 1300.18] They can be taken to court just like anybody else.
[1301.06 → 1305.86] And again, also, none of this stuff really applies to research, right, because that is actual fair use.
[1305.92 → 1316.78] So if you weren't commercializing this AI, you're not providing it to the public, you're just doing it in some lab somewhere for research and publishing papers on it, that would probably be totally fine fair use in all these scenarios.
[1316.78 → 1323.00] If you're a government agency who's doing this for national security purposes, like OpenAI, you should try to use a national security thing.
[1323.08 → 1326.22] It's like as if generating Studio Ghibli images is national security.
[1326.36 → 1326.66] It is.
[1326.72 → 1328.62] That stuff is separate work, right?
[1328.70 → 1332.60] Those are separate things, and they can be cabined off, and they can use copyrighted works.
[1332.72 → 1339.92] But the instant you say, oh, hey, we're OpenAI, and you can come pay to use this, Mr. Public, that's a commercial product, and you're on the hook.
[1339.92 → 1347.74] So if the court cases keep going the way they're going, all the things that we're talking about are plausible things that courts will be asked.
[1348.18 → 1348.62] Absolutely.
[1349.18 → 1357.56] And the reason I asked in particular about rolling it back is my understanding is, generally speaking, they are using a lot of transfer learning, right?
[1357.60 → 1361.64] They're taking weights that they already did before.
[1361.98 → 1365.30] They're using that as the beginning of the next round of training.
[1365.30 → 1373.42] Now, I don't know for, like, new models, whatever, my understanding is, though, like, when you do these things, you do a lot of transfer learning.
[1373.50 → 1381.00] That was one of the things that they found out that was crazy about neural nets is, like, yo, we can train it over here, and we can train on something else later, and it does better.
[1381.24 → 1382.20] And you're like, what the heck?
[1382.24 → 1383.24] We don't understand why.
[1383.56 → 1385.58] And they're like, okay, because of math.
[1385.74 → 1385.90] Sure.
[1386.40 → 1386.66] Sure.
[1386.90 → 1387.26] Math.
[1387.52 → 1388.76] Like, magnets are real.
[1391.32 → 1391.94] It's true.
[1392.12 → 1392.34] Sorry.
[1392.34 → 1393.82] That's for fun.
[1393.82 → 1397.22] TJ learned about ICP just, like, two weeks ago and learned about magnets.
[1397.66 → 1399.36] And so he's just so stoked about it right now.
[1399.36 → 1399.94] He's so stoked.
[1399.96 → 1402.36] Just the magnets are – that joke's too good for me.
[1402.36 → 1403.18] I love that joke.
[1403.42 → 1405.68] I have made it many times, and everyone's like, what are you talking about?
[1405.80 → 1406.94] I'm like, insane clown posse?
[1407.04 → 1407.26] Anyone?
[1407.62 → 1407.84] No?
[1408.02 → 1408.24] Okay.
[1408.30 → 1411.94] TJ literally just discovered it last week, so he's very stoked right now.
[1413.16 → 1420.00] But so that's kind of what, like, the transfer learning aspect is really – because I, like, I wonder how far they would try and pull that back, right?
[1420.00 → 1423.58] Because, like, initially they were just literally reading all the code on GitHub for sure.
[1423.72 → 1432.46] We know because they would be, like, start typing out this GPL code, and it completes the rest, including source comments and saying it's GPL.
[1432.78 → 1437.56] Rio was like, okay, well, that's definitely against the license.
[1437.96 → 1439.68] So that part is very – I don't know.
[1439.72 → 1441.38] It would be interesting to see how that unfolds.
[1441.38 → 1451.16] Well, we also have Facebook admitting in public filings that they went to, like – I mean, they effectively went to Pirate Bay to download stuff for their training.
[1451.48 → 1458.74] Like, they knowingly violated, like, literally international law to get their training data in the first place.
[1459.22 → 1463.52] And so you kind of have this situation where these AI companies are completely on the hook.
[1463.52 → 1467.94] So the only question is what are courts going to decide about whether any of this is fair?
[1468.14 → 1471.72] And like I said, I'm optimistic because of the Reuters case.
[1471.94 → 1473.36] I don't know what will happen in the future.
[1473.46 → 1476.52] I'm optimistic that they won't – you know, can I get Kat on stream?
[1476.70 → 1477.02] Can I –
[1477.02 → 1477.50] There you go.
[1477.52 → 1478.56] Yeah, Kat on stream is fine.
[1479.38 → 1480.30] Allergic to cats.
[1480.54 → 1481.20] Oh, I'm sorry.
[1481.56 → 1481.88] I'm sorry.
[1482.02 → 1482.38] Good.
[1482.62 → 1483.04] Sorry, Trash.
[1483.80 → 1484.44] But no, no.
[1484.80 → 1490.30] By the way, the fun part about the Facebook one is that there's an actual email from one of the like, leads that's just like,
[1490.30 → 1494.72] I probably shouldn't do this on my work computer, sweating smiley face.
[1495.06 → 1496.82] It's just like, oh, my gosh.
[1496.82 → 1497.40] It's amazing.
[1497.78 → 1498.42] It's amazing.
[1498.92 → 1499.86] It's nuts.
[1500.16 → 1503.72] I looked at that stuff, and I'm like, what is happening, right?
[1503.84 → 1505.60] Like, they put that in an email.
[1505.60 → 1506.08] It's like all the way to Zuckerberg, too.
[1506.32 → 1509.48] They're like, yeah, Zuckerberg's like, yep, that's probably a bad idea, isn't it?
[1509.96 → 1510.22] Yeah.
[1510.62 → 1511.52] So anyway, you know what?
[1511.64 → 1513.00] Can we transition now?
[1513.00 → 1514.16] Because now this is the thing.
[1514.28 → 1516.36] I assume that you want to do the Shopify one, right?
[1516.66 → 1517.02] No.
[1517.02 → 1521.76] I wanted to talk about this because, and you guys, you basically got there without me.
[1521.94 → 1522.60] That's the thing.
[1523.54 → 1531.38] I have this concept for the AI future that people haven't been thinking about, and it's all the things that you all just said, right?
[1531.46 → 1533.00] You were just hypothesizing.
[1533.24 → 1539.78] And I came up with a term for it that I want to try and make, you know, the way we sort of conceptualize this.
[1539.92 → 1541.96] I call it the taint.
[1542.46 → 1542.76] Okay.
[1543.62 → 1544.18] I like the joke.
[1544.30 → 1545.78] React already does use this.
[1545.78 → 1548.20] So just so you know, the taint already does exist.
[1548.60 → 1549.72] Tercel might come after you.
[1549.80 → 1551.02] I think they came up with taint.
[1551.02 → 1554.26] Yeah, Tercel, which might be the taint, will come at you for copyright.
[1554.80 → 1559.82] Maybe we can work out some license agreement so I can use the taint for this.
[1559.92 → 1560.16] Okay.
[1561.04 → 1563.38] TJ is not participating in this conversation.
[1564.14 → 1572.46] It's possible that we are heading into a future where everyone is going to need to know if their AI has the taint, right?
[1572.46 → 1578.44] If your AI has this taint because it's had the copyrighted work into it, right?
[1578.56 → 1581.88] And you can have, like, a lot of taint or a little taint as well, right?
[1581.90 → 1586.14] So there's a certain amount of taint when we look at this AI that we're going to see, right?
[1586.28 → 1586.88] Taint girth.
[1586.88 → 1587.28] Exactly.
[1587.28 → 1587.68] Exactly.
[1588.86 → 1597.42] And so it's like, will AI companies have to pivot to demonstrating that they have the least taint, effectively?
[1597.76 → 1603.60] Or they are unlikely to have taint when you use it, right?
[1603.60 → 1604.46] I'm watching Teen.
[1604.52 → 1606.56] Teen has, like, 20 jokes lined up right now.
[1606.70 → 1607.32] I'm not going to go.
[1607.46 → 1608.36] I'm literally not going to say anything.
[1608.36 → 1609.48] I'm staring at his square.
[1609.48 → 1612.10] TJ wants the joke so bad, but he's just, like, not going to do it.
[1612.14 → 1613.68] No girth jokes, no length jokes.
[1613.68 → 1616.58] That's 100% of the shots I'm not taking right now, let me tell you.
[1617.28 → 1624.86] So I would like everyone to think about this possibility, that there could be, like, a future where everyone's concerned about the taint of AI.
[1625.08 → 1625.82] So there's a taint score.
[1626.00 → 1627.70] Oh, I bet you there will be, like, some sort of judgment.
[1627.70 → 1637.24] Like, if you can prove in some reasonable amount that your taint score is below 5%, it's kind of like how you could – there's, like, a certain amount of rat feces that can exist within edible products.
[1637.66 → 1640.22] That they're just, like, that's unavoidable percentage.
[1640.78 → 1641.30] The end.
[1641.52 → 1643.58] There are body parts and such in it, and that's it.
[1643.68 → 1647.94] So, therefore, you can also have some unknown amount of taint factor.
[1649.76 → 1650.56] That's disgusting.
[1651.02 → 1653.14] By the way, Casey has just disappeared on us.
[1653.22 → 1659.74] Apparently, he spoke a little too authoritatively on the taint, and now he's out.
[1659.82 → 1660.04] Taint AI.
[1660.42 → 1660.70] What do you do?
[1660.70 → 1661.58] I'm building it after this.
[1662.30 → 1662.62] Amazing.
[1663.10 → 1663.42] Amazing.
[1663.60 → 1664.20] Can you hear me?
[1664.40 → 1665.24] Yeah, now we can hear you.
[1665.24 → 1665.64] Here we go.
[1665.64 → 1666.06] We can hear you again.
[1666.12 → 1666.24] Hey.
[1666.76 → 1667.82] The AI is fighting you.
[1667.94 → 1668.30] It hurt what you said.
[1668.30 → 1668.66] It is.
[1668.76 → 1668.98] It is.
[1668.98 → 1674.14] So, anyway, I want this future where everyone is inspecting the taint.
[1674.40 → 1674.54] Right?
[1674.54 → 1676.02] They're trying to find – they're chasing the taint.
[1676.36 → 1688.62] They're like, look, we have to figure out – because if we go to court, the last thing that I want, if I'm a CEO, I don't want to go to court and find out that suddenly I actually did have the taint.
[1688.86 → 1689.20] Right?
[1689.48 → 1690.60] Because I – and I didn't know.
[1690.68 → 1691.90] Like, we used some product.
[1692.12 → 1693.78] We licensed some, you know, thing.
[1694.04 → 1695.74] And the taint – we ended up with the taint.
[1696.02 → 1696.28] Right?
[1696.40 → 1697.38] This reminds me of –
[1697.38 → 1699.90] We thought we were getting a coding assistant, but all we ended up with was taint.
[1700.38 → 1700.50] Right?
[1700.58 → 1701.74] You just do, like, a smell test.
[1702.06 → 1702.14] Right?
[1702.14 → 1702.30] Yeah.
[1702.42 → 1703.40] It's a sniff test.
[1703.72 → 1703.86] Yeah.
[1704.26 → 1705.68] This reminds me of the Wheel of Time.
[1705.96 → 1707.46] You know, Saladin was also tainted.
[1707.78 → 1708.64] You know what I'm talking about, TJ?
[1709.08 → 1714.62] I was thinking – I was literally like, should we just have a 15-minute conversation about Wheel of Time in the middle of this?
[1714.70 → 1715.92] That would be good for stand-up.
[1716.26 → 1716.58] Like –
[1716.58 → 1717.82] Classic stand-up.
[1717.82 → 1727.14] No, this actually is pretty interesting because there has to be a certain level of copyrighted material that the courts are going to say is okay to be within an LLM.
[1728.20 → 1738.30] Because there's just, like, some way that – like, there's just going to be things that you will never know because someone stole some work that you could not possibly ever attribute and somehow got into it, and therefore that's that.
[1738.38 → 1744.32] And now they're saying, okay, well, you can have some percentage, but we have to prove – you have to be able to prove that you're below some threshold.
[1745.16 → 1745.98] That's the hard part.
[1745.98 → 1747.46] It won't necessarily be a percentage.
[1747.66 → 1752.06] It'll just be, like, some number of people will sue you, and you'll make licensing deals with them, right?
[1752.36 → 1756.74] Because what's going to happen is its – copyright assertion is, like, on an individual basis, right?
[1756.74 → 1764.32] It's like Disney goes and sues them, and then, you know, assuming that they get a similar decision to Thomson Reuters, then one of two things happens.
[1764.80 → 1771.30] Either all of that training data is pulled, and they have to rerun the models, or they have to pay Disney.
[1771.30 → 1781.68] And my guess – and I could be wrong about this, but at least for the big companies like Disney, what they're going to do is they're going to get a little of that sweet, sweet molar, right?
[1781.72 → 1783.48] So you're not going to have to pull Disney stuff.
[1783.82 → 1787.14] You're going to have to pay Disney their little AI tax, right?
[1787.14 → 1791.94] And OpenAI is going to be taking some of those – you know, they won't be venture capital rounds by this point.
[1792.06 → 1794.46] Presumably they'll be a public-traded company or something like that.
[1794.80 → 1805.38] Once they figure out how to turn themselves out from a nonprofit or whatever, they'll have to pay some of that – you know, all of that AI income or investment money, which is what they currently are running on investment money.
[1805.46 → 1807.14] They'll have to pay that to their copyright people.
[1807.14 → 1815.58] By the way, I would like to say that Trash is doing the most classic stand-up behaviour, clearly typing on his keyboard.
[1816.34 → 1817.88] I'm typing in the chat.
[1818.26 → 1819.46] Well, shut up.
[1820.40 → 1821.78] Okay, Trash, shut up.
[1821.82 → 1822.94] We're working here, okay?
[1823.14 → 1823.88] Focus up.
[1823.88 → 1825.34] I'm listening.
[1826.04 → 1828.70] He is director of community outreach.
[1829.22 → 1830.28] That is his job.
[1830.38 → 1831.06] He's doing it.
[1831.18 → 1831.60] He's doing it.
[1831.62 → 1835.58] You lost me at the Main conversation, and then my head just starts spiralling into like 50 memes.
[1835.98 → 1836.14] Yeah.
[1836.14 → 1840.28] Well, the good news is that we can move on from the taint, okay?
[1840.34 → 1843.74] We do not have to continue down this road to this logical conclusion.
[1843.96 → 1844.84] Instead, we can pivot.
[1845.80 → 1846.18] All right.
[1846.36 → 1848.02] You guys ready for topic two?
[1848.76 → 1849.02] Sure.
[1849.26 → 1854.96] Which is kind of really just like an extension of topic one, and so that is going to be this right here.
[1855.72 → 1862.12] Shopify has announced from the tip pity of the top pity that everybody must use AI.
[1862.12 → 1871.30] Toby effectively went on to say that he presented about it and called action for an invitation for everyone to be able to use it, but that was not enough.
[1871.30 → 1881.58] And then he goes on to say that people who used AI are not just 10x the engineer, but in fact 100x the engineer in work being done.
[1881.64 → 1881.92] That's right.
[1881.96 → 1889.80] For every single three-and-a-half-day work they've done, they've done an equivalent of a year before they started using AI, which is quite a wild amount of work.
[1889.80 → 1895.80] And then finally, he lists out all the things that he expects you to be doing when you're using AI.
[1896.00 → 1899.34] For those that don't know what GSD is, GSD just is a simple get shit done.
[1899.66 → 1900.70] That's what it means.
[1900.96 → 1902.58] German Shepherd Dog, actually.
[1902.60 → 1903.44] German Shepherd Dog.
[1903.44 → 1905.78] Not going to lie, I was very confused when I saw GSD.
[1906.00 → 1907.26] It's a German Shepherd Dog.
[1907.38 → 1908.92] If you had one, you would understand.
[1909.06 → 1912.70] I do not have one, so that's probably why I'm missing a little bit of the context here.
[1912.70 → 1919.44] But Shopify has officially stated that you are to use it no matter what in every single one of these cases.
[1919.96 → 1930.86] Now, I don't know if there's any sort of – they didn't really list any sort of negative cases where you do not have to use it, but it went from a suggestion you should do this to now this is a requirement from here on out at Shopify.
[1930.86 → 1943.62] And one of their justifications was actually found in this previous paragraph, which is, in a company growing 20% to 40% year over year, you must improve by at least that every year to re-qualify.
[1944.32 → 1957.34] So you need to improve – I'm not sure on what metric they're exactly saying or how they're suggesting that they rate this improvement, but you must improve at the same rate the company is in fact growing monetarily.
[1957.34 → 1960.02] Which makes no sense, by the way.
[1960.08 → 1964.78] I literally have no idea what that is even supposed to mean in practice, and I'm not sure if he does either.
[1965.16 → 1968.98] TJ, I'll defend that line, even though I don't agree 100%.
[1968.98 → 1977.66] Pretty sure they're just saying the market for Shopify-style sites is growing by this much all the time.
[1978.16 → 1985.44] So if we don't keep getting better, someone else is just going to snatch those up because those are like brand-new customers on the internet making Shopify sites.
[1985.44 → 1994.50] They're like in their vertical, the like TAM is just growing by 20% to 40% every year, and they're like, we're going to get surpassed by this.
[1994.76 → 1997.00] If we're not doing this, then we're just going to get trashed.
[1997.14 → 1997.94] That's what I thought they were saying.
[1997.94 → 2006.06] Okay, so the must mean the company because if you look at the thing, it says in a company growing 20% to 40% year over year, you must improve at least by –
[2006.06 → 2006.52] Oh, maybe I misunderstood.
[2006.92 → 2007.94] Yeah, that's what I'm saying.
[2008.10 → 2009.14] It just doesn't make sense.
[2009.46 → 2011.94] Is this a royal you or are you the engineer?
[2011.94 → 2016.32] I was reading it as like you, like Shopify's got to keep doing this because the market's growing there.
[2016.42 → 2017.02] No, no, no.
[2017.02 → 2017.22] But I see what you're saying.
[2017.30 → 2020.78] You're saying, yo, person, you got to be 40% better.
[2021.00 → 2027.16] Yes, because the rest of it is all about the individual using it, getting X percent better, all this kind of stuff.
[2027.70 → 2038.60] But TJ, I know you had some contrarian viewpoints on this, but I will say that one thing that I just really dislike about all of this kind of conversation is not that saying that AI will make you faster.
[2038.60 → 2044.80] For definitely a set of people in every single profession, AI is going to be a huge assist to their current skill set.
[2045.16 → 2048.02] I don't think that's – I don't think anyone can argue that.
[2048.34 → 2056.18] I think the big problem is that we sell it as 100x because I just do not believe that three and a half days is worth a year of work ever for anything.
[2056.42 → 2059.96] Like you could generate shaders for a year and a half – or for three and a half days.
[2060.02 → 2060.48] TJ could.
[2060.60 → 2065.22] And I would out shader his shaders if you gave me a full year, 40 hours a week, every single week.
[2065.30 → 2066.16] TJ is raising his hand.
[2066.22 → 2067.62] TJ, question.
[2067.62 → 2067.88] Thank you.
[2068.00 → 2068.96] Thanks for calling on me.
[2069.34 → 2072.24] Have you worked with some of the people at these large companies?
[2072.60 → 2072.86] Yes.
[2072.96 → 2073.78] In fact, I have.
[2073.86 → 2075.10] I worked with Trash Dev.
[2075.20 → 2076.68] As you can see right below me.
[2077.90 → 2078.82] Am I below you?
[2079.26 → 2080.78] Well, yes, in this one.
[2080.86 → 2082.12] No, I'm below you.
[2082.22 → 2082.50] Okay.
[2082.62 → 2083.14] You get it.
[2083.32 → 2084.28] You're below me.
[2085.14 → 2085.46] Oh.
[2085.72 → 2088.06] He's down the lower floor because you're in the C-suite.
[2088.22 → 2089.74] Let's talk about that taint again.
[2089.94 → 2090.20] Yeah.
[2090.40 → 2090.78] That taint.
[2091.90 → 2092.26] Okay.
[2092.94 → 2094.54] I think 100x – I'm with you.
[2094.92 → 2096.08] 100x seems wild.
[2096.08 → 2099.66] But I will say I've worked with people that haven't gotten anything done in a year, too.
[2099.84 → 2101.38] So that's the only thing.
[2101.38 → 2104.04] It all depends on, like, what your baseline is, right?
[2104.14 → 2106.14] Like, my baseline is not like your baseline.
[2106.38 → 2108.30] Like, they need to, like, have some metrics.
[2108.40 → 2110.66] Is it going to be lines of code, feature ships?
[2110.68 → 2111.08] Oh, yeah.
[2111.36 → 2111.96] You know what I mean?
[2112.00 → 2113.90] Like, you can't – I don't know.
[2113.90 → 2115.24] So I have a lot of thoughts here.
[2115.46 → 2116.32] So I'm just going to keep going.
[2116.52 → 2116.68] Yeah.
[2116.80 → 2121.24] So one of the bullet points they have – and honestly, I thought I read the whole thing.
[2121.30 → 2124.02] But then when you started reading it, Prime, I completely missed everything you said when
[2124.02 → 2124.52] I was reading it.
[2124.68 → 2125.58] So I guess I don't know how to read it.
[2125.58 → 2126.82] So you, in fact, did not read it.
[2126.88 → 2128.02] Would you like me to put it back up for you?
[2128.02 → 2129.74] I, in fact, did not read it when you started.
[2129.74 → 2130.78] I was like, oh, we had said that.
[2130.78 → 2138.00] So one of the bullet points was before asking for headcount, you have to prove that you
[2138.00 → 2141.42] need it with, like, your current use of AI, which kind of blows my mind.
[2141.50 → 2145.26] So, like, if we go back to the water tower, right, you know, I basically came in there
[2145.26 → 2147.22] and, like, bombed your project in, like, 30 minutes, right?
[2147.22 → 2149.54] The bug still exists that you did, but yes.
[2149.80 → 2155.30] So the way I kind of see it is, like, an AI is kind of more or less an intern junior dev
[2155.30 → 2160.70] where they're effectively a net negative until they onboard, get familiar with the code base,
[2160.78 → 2162.20] you know, level up a little bit, right?
[2162.72 → 2167.96] So you spend most of your time babysitting, code reviewing that AI slash intern.
[2168.40 → 2175.16] So you end up losing productivity on your side to make up, to have the AI or intern do its work.
[2175.58 → 2178.28] So that is, like, kind of counterintuitive.
[2178.44 → 2182.74] That would be the argument, I feel like, to, you know, you would need headcount
[2182.74 → 2185.10] because you need someone to basically be a full-time prompt engineer
[2185.10 → 2189.84] and basically code review the AI stuff, right, before it just keeps shipping more stuff.
[2189.94 → 2190.54] Does that make sense?
[2191.78 → 2194.26] Yeah, but, I mean, there are a couple flaws to all this.
[2194.44 → 2200.16] Like, first off, on the TJ side of things, the people that don't ship in a year,
[2200.38 → 2202.90] do you want them shipping to begin with, right?
[2202.96 → 2204.94] Like, do you want them 100x more effective?
[2205.08 → 2206.02] Like, no, you don't.
[2206.36 → 2209.66] And so second, even with this intern thing, people always equate it to an intern, right?
[2209.74 → 2210.82] AI is just like an intern.
[2211.12 → 2215.24] The thing about an intern is that day one, you're like, here's what you need to do, intern,
[2215.32 → 2217.16] and you sink a bunch of hours into it.
[2217.16 → 2223.00] And then day two, day three, day four, by, you know, by month, you're like, okay, hey,
[2223.08 → 2223.90] go do this project.
[2224.04 → 2226.68] And then they come back in two weeks, and they have a pretty good set of project.
[2226.78 → 2228.54] And you're like, okay, here are a couple different things you need to do.
[2228.80 → 2230.90] There are no gains with an AI.
[2231.04 → 2235.46] The AI is as good as it is day one as it will be on day 60 right now,
[2235.50 → 2238.80] unless there's some theoretical huge breakthrough that we have yet to see on the horizon.
[2238.80 → 2241.84] That's going to make it from a junior engineer into some magnificent senior engineer.
[2241.84 → 2245.98] But right now, barring extreme breakthrough, you get the same thing.
[2246.08 → 2248.62] Whereas with interns, you get some sort of multiplier effect,
[2248.64 → 2250.96] or at least some sort of additive effect to the work,
[2251.00 → 2253.12] where they get to run individually because they have autonomy.
[2254.20 → 2254.52] Potentially.
[2255.42 → 2258.64] I think, to be fair to the guy who wrote this is Toby, right?
[2258.86 → 2259.32] Yeah, Toby.
[2260.10 → 2263.36] Okay, so to be fair to Toby, if he's following his own guidelines,
[2263.68 → 2268.40] he probably, he might not have even proofread this particular memo, right?
[2268.40 → 2271.54] He might have just asked the AI for something that encourages the company to use AI,
[2271.66 → 2272.24] and then hit send.
[2272.60 → 2275.78] Because he was 100x, he didn't have time to read any of it.
[2276.02 → 2278.18] So it's possible that Toby doesn't know what it says.
[2278.46 → 2280.26] And frankly, that would be likely,
[2280.40 → 2282.44] because this is also the guy who sent out the
[2282.44 → 2286.78] premature optimization is the root of all evil, we should ignore tweet.
[2287.22 → 2290.66] So like, you know, just if you could go back like last year,
[2290.66 → 2293.80] and he was talking about how everyone needs to start thinking seriously about performance.
[2293.94 → 2295.66] And like, we need to use a lot of AIs,
[2295.76 → 2297.86] and we need to think seriously about performance all the time.
[2297.86 → 2300.82] They don't even, it's like, what are you even talking about?
[2300.88 → 2307.46] These are completely conflicting statements you're giving to your, you know, your engineers here.
[2307.74 → 2313.06] So I don't know, like, it almost reads like a thing where it's just like,
[2313.12 → 2316.04] yeah, I asked AI for a thing that would encourage AI use.
[2316.14 → 2318.10] I hit send, and then, you know, it leaked.
[2318.46 → 2319.72] Oops, or something.
[2320.36 → 2322.94] But do you think there's a world where this actually is, I mean,
[2322.94 → 2328.30] do you think in our world, this is actually a fine thing to mandate from the top down?
[2329.72 → 2333.92] Not if you trust your employees at all, because at least for me,
[2333.92 → 2337.08] if I was running a company of that size, I would say,
[2337.84 → 2339.96] the kind of memo I would send is like,
[2340.08 → 2341.66] look, AI is a new thing.
[2341.80 → 2343.38] We would like people to evaluate it.
[2343.46 → 2345.70] If you find it useful, we'll pay for it.
[2345.70 → 2348.40] So go ahead and adopt AI practices if you think they're helping you.
[2348.48 → 2350.40] If you don't think they're helping you, you don't have to.
[2350.74 → 2353.28] I trust my engineers to make intelligent decisions,
[2353.48 → 2355.18] or, you know, designers would be the other people,
[2355.30 → 2356.92] presumably, who are using like this for prototyping.
[2357.28 → 2361.56] I trust you to figure out whether this is going to help you or hurt you,
[2361.58 → 2362.96] and to act accordingly.
[2363.70 → 2366.70] Saying, like, you have to use it everywhere just sounds like
[2366.70 → 2368.34] you don't really believe you have any good people.
[2368.90 → 2370.62] Because if you did, you wouldn't be saying this,
[2370.64 → 2372.38] because you'd trust them to make that decision.
[2372.38 → 2375.18] So you would mandate the evaluation of AI.
[2375.44 → 2377.16] You wouldn't mandate the usage.
[2377.96 → 2378.36] Exactly.
[2378.50 → 2381.78] You'd just say, look, you know, I would like our company to look at this.
[2382.16 → 2383.46] Please, everyone take, you know,
[2383.70 → 2385.56] basically, it's more of giving them permission, right?
[2385.58 → 2388.80] Saying, look, everyone, it takes some time to evaluate AI.
[2389.18 → 2392.90] I would like you all to try it, see how it goes,
[2393.28 → 2396.34] maybe write it up a little bit so we know for each of our things,
[2396.38 → 2397.34] like, how did it go?
[2397.44 → 2399.34] Like, who had success, who had failures?
[2399.42 → 2400.60] And then I'll look at it,
[2400.60 → 2403.14] and we'll maybe make some decisions about what we want to do going forward.
[2403.22 → 2403.30] Right?
[2403.32 → 2404.56] That seems like a sane thing.
[2404.74 → 2407.14] Just like, ah, AI is big at 20%.
[2407.14 → 2408.02] Everyone use it.
[2408.06 → 2411.12] Like, I don't know, it just kind of comes off as, like,
[2411.16 → 2413.22] almost, like, frantic or, like, you know,
[2413.24 → 2415.44] I had too much coffee and, or I don't know.
[2416.22 → 2418.28] There is another interesting bullet point where it says,
[2418.36 → 2420.90] AI usage will be a part of your performance review.
[2420.96 → 2421.28] Exactly.
[2421.64 → 2425.08] Like, how do you, let's just say I ship all the time,
[2425.12 → 2426.06] even without the help of AI.
[2426.06 → 2430.10] Will I get a bad performance score because I didn't make an AI commit?
[2430.10 → 2432.98] Or, like, how do you even prove that I didn't use AI or used AI?
[2433.00 → 2433.30] Obviously.
[2433.42 → 2433.58] Yeah.
[2433.68 → 2436.42] If you would read the other bullet point is that you're going to share your
[2436.42 → 2438.42] prompts in the Slack room.
[2438.96 → 2442.78] And so it's probably prompts shared is really your impact to the AI usage
[2442.78 → 2443.54] among the company.
[2443.98 → 2446.28] My prompts would be so bad, just like yours, Prime.
[2446.48 → 2447.28] Like, can you just make this work?
[2447.28 → 2448.40] I'm going to kill your grandmother.
[2450.12 → 2452.34] You guys are not even Chaos Orb maxing.
[2452.44 → 2453.32] Like, it's pretty embarrassing.
[2453.48 → 2454.46] I've seen both of your prompts.
[2454.66 → 2456.62] That should actually, I'm going to start doing that.
[2457.68 → 2458.84] That part's pretty embarrassing.
[2458.84 → 2463.64] Dude, I would love to see a room full of, like, shared prompts.
[2463.64 → 2466.38] And it just gets more and more degenerate.
[2466.90 → 2468.70] Like, as you read, you're like, oh, gosh, what?
[2469.42 → 2470.46] What with their mom?
[2470.84 → 2472.30] It just gets worse and worse.
[2473.00 → 2475.46] Hey, if anyone's out there listening right now, all of my prompts,
[2475.54 → 2477.00] you have to pay me to train off of them.
[2477.12 → 2478.70] So if you're in the audience listening right now,
[2478.72 → 2481.20] you better close your eyes and never hear me do a prompt again
[2481.20 → 2483.24] because those are mine for $1,000.
[2483.44 → 2484.40] Contract of adhesion.
[2484.98 → 2485.62] Yeah, got them.
[2485.82 → 2486.30] Got them.
[2486.30 → 2488.58] I've got some bad news for you guys on that
[2488.58 → 2491.42] because the Copyright Office recently released their guidance
[2491.42 → 2493.26] that says that anything that comes out of an AI
[2493.26 → 2495.72] where all you did was put in a prompt is not copyrightable.
[2496.04 → 2497.48] So everyone can just take that for free.
[2497.60 → 2497.92] You can't even.
[2498.14 → 2499.12] Don't you know anything about law, idiots?
[2499.12 → 2501.10] But this is the prompt he's crafting.
[2501.44 → 2502.76] Okay, this is not the output.
[2503.72 → 2504.30] They don't need it.
[2504.36 → 2505.62] They can just take what it generated.
[2507.08 → 2507.86] Oh, I see what you're saying.
[2507.94 → 2510.28] But to get the peak performance,
[2510.36 → 2511.58] you've got to be using Chaos Orbs,
[2511.64 → 2513.08] and I'm copyrighting that strategy.
[2513.08 → 2515.28] So that's just too bad, Casey.
[2515.60 → 2516.30] Fair enough.
[2518.50 → 2521.04] I will say, okay, in –
[2521.04 → 2528.88] I will say that I have seen people use different LLM tools quite a bit.
[2529.10 → 2532.48] Let's say different levels of efficacy, right?
[2532.52 → 2536.12] And knowing when to use them and how to use them is not always intuitive.
[2536.12 → 2539.90] And also, like, sometimes it breaks your expectations
[2539.90 → 2541.98] and they're better at some stuff than you think they were going to be
[2541.98 → 2543.46] or not at other things.
[2543.66 → 2547.62] So I do think there's something of saying, like, what you're saying, Casey,
[2547.74 → 2549.30] of, like, having people try it.
[2549.48 → 2551.72] But even, like, saying an extended, like,
[2551.78 → 2554.58] you guys really need to, like, try it a lot of times to see
[2554.58 → 2557.46] because it is quite a bit different
[2557.46 → 2559.46] and, like, reaching for it in unexpected places.
[2559.52 → 2561.36] Like, one of the ones that I used –
[2561.36 → 2565.10] I told Prime this one, like, I was doing some awesome WE.
[2565.72 → 2568.26] Like, not it's awesome, but, like, awesome is the name of it.
[2568.88 → 2571.46] And they have all of their docs and everything online,
[2571.46 → 2575.30] but it wasn't, like, in a format that I could pull into my editor
[2575.30 → 2576.48] to get autocomplete and everything.
[2576.86 → 2579.82] So I just, like, listed five URLs
[2579.82 → 2585.04] and told the LLM to generate from those, like, properly formatted stuff
[2585.04 → 2585.88] and put it in.
[2586.10 → 2590.86] And it would have taken me, like, hours and hours probably to do it, right?
[2590.90 → 2593.10] And it did it within two minutes, right?
[2593.14 → 2597.82] And then it, like, literally 10xed how fast I could do the rest of the things
[2597.82 → 2599.96] because, like, now my editor works and I get autocomplete
[2599.96 → 2602.24] and I can discover everything, and I'm getting squiggles
[2602.24 → 2603.30] and all this other stuff, right?
[2604.10 → 2608.44] And I feel like if you're not trying LLMs out for different things,
[2608.44 → 2614.60] you would never think to, like, paste a URL and just be like, fix this, right?
[2614.76 → 2618.24] So there is something there were, like,
[2618.26 → 2619.84] I don't know exactly how that's going to play out
[2619.84 → 2622.26] or where that's going to end, but there are things like,
[2622.98 → 2625.50] okay, you do actually need to try and use it
[2625.50 → 2626.86] if you want to get stuff out of it.
[2626.94 → 2629.44] I don't think, at least right now, you can just be like,
[2629.52 → 2632.72] I use it once a week, and it really affects my, like, workflow.
[2632.84 → 2636.04] That just doesn't, doesn't, it doesn't work for anything.
[2636.30 → 2636.76] You know what I mean?
[2636.76 → 2640.18] Nothing you just use occasionally is really going to, you know, I don't know.
[2640.42 → 2641.76] Except for ayahuasca, apparently.
[2641.88 → 2642.38] That's what I heard.
[2642.56 → 2644.94] Yeah, that's a one-time use change right there.
[2645.10 → 2646.18] Yeah, that's, yeah.
[2646.76 → 2648.40] That's a vibe, TJ, okay?
[2648.46 → 2649.30] Yeah, exactly.
[2649.58 → 2650.26] That's what I heard.
[2652.42 → 2654.92] Yeah, I generally, I generally agree with that.
[2654.98 → 2656.84] There are a few things that when you use it, you're like, man,
[2656.88 → 2658.86] that is actually extremely convenient
[2658.86 → 2661.74] because I would have spent, like, two hours trying to come up with a Vim macro
[2661.74 → 2666.50] to solve that problem and then attempt a regex to edit HTML
[2666.50 → 2669.84] and, like, go down all the same routes everybody else has ever gone down
[2669.84 → 2673.08] in the history of mankind just to try to fix that exact problem.
[2673.14 → 2674.50] There are things that are fantastic.
[2674.86 → 2676.66] And so you don't, you definitely don't know that
[2676.66 → 2678.92] if you haven't AI'd hard enough.
[2678.92 → 2685.68] So that's my only defence of, like, sort of the mandate aspect.
[2686.10 → 2688.54] Now, would I put it in your performance review?
[2689.62 → 2690.78] Nah, that ain't for me.
[2690.86 → 2694.52] Like, I don't want to, I barely want to measure anything in performance review.
[2694.60 → 2696.66] I would do performance reviews mostly off vibes.
[2696.84 → 2699.34] Like, I just hire managers who you can trust
[2699.34 → 2701.24] and hire managers of those managers who you can trust,
[2701.28 → 2702.24] however many levels you need.
[2702.28 → 2703.84] I don't know how big Shopify is.
[2703.98 → 2706.64] And then you just, like, fire people if people get the bad vibes.
[2706.64 → 2712.12] You know, I don't want to measure if they're doing a good job or not.
[2712.34 → 2714.08] You're a trillionaire, okay?
[2714.34 → 2716.62] Everyone desperately wants to know how to measure
[2716.62 → 2718.02] if you're making good software or not.
[2718.44 → 2720.20] And people aren't.
[2720.42 → 2721.42] So there you go.
[2721.60 → 2722.64] That's...
[2722.64 → 2728.54] Maybe we use the AIs to evaluate whether you have been using AI
[2728.54 → 2732.70] and then we can give you a completely objective
[2732.70 → 2735.68] in the sense that it's done by a machine performance review.
[2735.84 → 2736.56] There we go.
[2736.64 → 2737.66] We could also take...
[2737.66 → 2738.74] I'm going to poison the data.
[2738.96 → 2740.34] Yeah, we could also take an AI
[2740.34 → 2743.34] and throw all your Slack messages into it
[2743.34 → 2744.70] with all of your coworkers
[2744.70 → 2746.66] and then be like, give me his vibe score.
[2746.94 → 2747.72] One out of ten.
[2747.84 → 2749.24] How's he vibing with his teammates?
[2749.70 → 2751.02] People that aren't, get them out.
[2751.60 → 2753.22] Is there going to be a thing where, like,
[2753.32 → 2755.76] the trust and safety division of companies,
[2755.92 → 2757.56] which is, like, you know, during the day,
[2757.56 → 2759.60] they're, like, trying to go through
[2759.60 → 2761.84] all of that company's public comment boards
[2761.84 → 2764.72] and pull out all the like, things that are, you know,
[2765.02 → 2768.24] horribly demented that people are posting.
[2768.46 → 2771.54] And all of those are going to just get sent to the AI team
[2771.54 → 2773.54] where now those will be tried as prompts
[2773.54 → 2777.50] to see if they encourage the AI to do better work, right?
[2777.50 → 2780.12] So it's just, like, it's this pipeline directly from,
[2780.32 → 2783.24] we had to stop 13-year-olds from seeing this
[2783.24 → 2786.54] to this is what we feed directly to the AI
[2786.54 → 2788.08] because it works really well
[2788.08 → 2789.68] when, you know,
[2790.68 → 2792.40] buttstank33's comments
[2792.40 → 2795.28] that we had to remove from the public forum
[2795.28 → 2796.50] are all...
[2796.50 → 2799.28] are getting it set up for the day.
[2799.28 → 2801.18] Dang.
[2802.14 → 2804.20] Well, I am just curious
[2804.20 → 2805.40] how this is going to go in a year
[2805.40 → 2806.20] because I'm just very...
[2807.08 → 2808.48] I think the performance reviews...
[2808.48 → 2809.14] Because out of everything,
[2809.24 → 2810.90] it's just, like, mandating people use AI
[2810.90 → 2812.68] in some sort of capacity at your job.
[2812.74 → 2815.12] It's like, okay, sure, maybe you could do that.
[2815.16 → 2817.48] But mandating it as a measurement of performance,
[2817.74 → 2820.94] I don't know how that's ever going to be acceptable
[2820.94 → 2822.96] or any measurable or anything.
[2823.64 → 2825.52] It just seems like you should always measure people
[2825.52 → 2826.32] how you've measured them.
[2826.36 → 2827.90] And if they're getting better relatively,
[2828.04 → 2828.68] then it's like, okay,
[2828.68 → 2829.48] this person's better.
[2829.70 → 2830.18] The end.
[2830.78 → 2831.32] It doesn't make...
[2831.32 → 2832.58] And if the AI was doing its job,
[2832.66 → 2834.30] it will show up in that metric, right?
[2834.38 → 2836.76] It's like if the person is using AI effectively,
[2836.98 → 2838.90] they should be having good output.
[2839.10 → 2840.92] So you wouldn't really need to also check
[2840.92 → 2841.80] if they used it, right?
[2842.22 → 2842.48] Correct.
[2842.56 → 2844.26] If they were doing 100X...
[2844.26 → 2844.68] You'd know.
[2845.06 → 2846.00] You would know.
[2846.16 → 2848.50] Like, it will show up somehow.
[2849.66 → 2850.66] Your features...
[2850.66 → 2851.70] Things should just get done
[2851.70 → 2852.78] really, really quickly
[2852.78 → 2855.76] because someone has 100 years worth of human work
[2855.76 → 2857.92] in one year of time at your job.
[2858.68 → 2861.04] There's a lot of that thing, too,
[2861.12 → 2862.18] where people keep saying
[2862.18 → 2863.40] these ridiculous multipliers,
[2863.66 → 2864.96] but their output doesn't appear
[2864.96 → 2865.82] to change externally.
[2866.28 → 2866.84] So you're just like,
[2867.16 → 2868.88] I don't see, like, Shopify
[2868.88 → 2870.88] producing 100X more stuff,
[2870.98 → 2872.14] so what happened?
[2872.26 → 2872.56] You know, like,
[2872.62 → 2873.62] what's going on exactly?
[2874.12 → 2874.88] So it's kind of weird.
[2874.96 → 2876.02] Like, shouldn't we see it?
[2876.26 → 2877.28] 100X would be like,
[2877.48 → 2878.68] you've rewritten all the software
[2878.68 → 2879.92] in the world this year,
[2880.14 → 2881.02] but just your company.
[2881.36 → 2882.62] That would be 100X, right?
[2882.70 → 2884.76] Yeah, you don't have any external dependencies.
[2884.76 → 2885.24] Right?
[2885.24 → 2886.76] Like, you just rewrote the stack.
[2887.24 → 2888.68] I wish the like,
[2889.38 → 2890.98] in my experience,
[2891.78 → 2893.64] it's been much more, like,
[2893.94 → 2895.04] instead of saying,
[2895.42 → 2897.66] like, I'm 10X better at whatever,
[2898.56 → 2900.72] like, AI has let me do things
[2900.72 → 2902.86] that I would normally not do at all.
[2903.02 → 2904.34] Like, because it would just take too long
[2904.34 → 2906.08] to, like, learn some base knowledge
[2906.08 → 2906.78] about this thing.
[2906.82 → 2907.32] So I can say, like,
[2907.34 → 2908.40] hey, do these.
[2908.56 → 2909.64] Oh, that's not really right.
[2909.72 → 2910.42] Kind of go here.
[2910.76 → 2911.52] Okay, now, like,
[2911.58 → 2912.28] summarize this.
[2912.50 → 2912.84] Well, right.
[2913.08 → 2913.46] And, like,
[2914.14 → 2915.22] I could start something
[2915.22 → 2916.48] and, like, complete it
[2916.48 → 2918.16] that I would not have started otherwise.
[2918.26 → 2919.20] But it's not like the AI, like,
[2919.28 → 2920.46] wrote all of it or anything.
[2920.54 → 2920.98] You know what I mean?
[2921.00 → 2921.54] It's that, like,
[2921.56 → 2923.36] it enabled me to do that.
[2923.46 → 2925.10] Which is, there's no comparison.
[2925.22 → 2926.82] It's not like 100X more or less.
[2926.86 → 2927.32] It's just like,
[2927.66 → 2928.00] oh, yeah,
[2928.02 → 2928.78] I could do this project
[2928.78 → 2929.78] that I couldn't have done before.
[2929.88 → 2930.44] In a sense,
[2930.76 → 2931.94] that's a million X.
[2932.10 → 2932.98] But in a sense,
[2933.02 → 2933.80] it's plus one.
[2934.24 → 2934.46] Like,
[2934.84 → 2935.62] okay,
[2935.96 → 2937.00] so I feel like
[2937.00 → 2939.16] that would be such a better framing
[2939.16 → 2940.76] for a lot of these, like, CEOs.
[2940.94 → 2941.14] It's like,
[2941.18 → 2941.72] you can take on
[2941.72 → 2942.74] a more ambitious project.
[2942.92 → 2943.52] You can try something
[2943.52 → 2944.26] in a new area.
[2944.40 → 2945.00] We can try and, like,
[2945.02 → 2945.70] fix something that
[2945.70 → 2947.08] we didn't know how to fix before
[2947.08 → 2947.70] because you can, like,
[2948.48 → 2949.38] ask the LLM
[2949.38 → 2950.82] how to solve DNS problems
[2950.82 → 2952.02] that you couldn't figure out
[2952.02 → 2952.78] on your own before
[2952.78 → 2954.76] that were 37 replies deep
[2954.76 → 2955.24] in a forum.
[2955.76 → 2956.10] Sweet.
[2956.52 → 2957.28] Now you can solve that.
[2957.60 → 2957.92] But, like,
[2957.92 → 2958.58] it doesn't make me
[2958.58 → 2960.42] 1,000 or 100X better.
[2960.54 → 2962.04] I can't write all of my software
[2962.04 → 2963.16] for the year in a week.
[2963.56 → 2963.62] Like,
[2963.66 → 2964.56] that's not happening.
[2965.56 → 2967.50] And it's something in which,
[2967.66 → 2967.80] yeah,
[2967.80 → 2968.98] because when we did the Towers
[2968.98 → 2969.96] of Moratoria stuff,
[2970.30 → 2971.54] there were a few things
[2971.54 → 2972.88] in which I could have spent
[2972.88 → 2974.40] a day or two
[2974.40 → 2975.62] just reading the Love Docs
[2975.62 → 2977.20] and just purely experimenting
[2977.20 → 2977.88] on how to draw,
[2977.98 → 2978.34] how to, like,
[2978.40 → 2979.42] do the things right
[2979.42 → 2980.86] in that.
[2980.94 → 2981.94] Or we could just generate
[2981.94 → 2982.58] a bunch of stuff
[2982.58 → 2983.36] and then try to fix it
[2983.36 → 2983.86] as we go.
[2984.12 → 2985.08] Which, in some sense,
[2985.12 → 2986.04] just means you understand
[2986.04 → 2986.90] the basics of it.
[2986.98 → 2988.22] I understand how to use it.
[2988.26 → 2988.76] I don't understand
[2988.76 → 2989.50] how it works.
[2989.90 → 2991.56] Which I think is a fine thing
[2991.56 → 2992.22] when you're attempting
[2992.22 → 2992.88] to learn something
[2992.88 → 2993.62] for the first time.
[2993.62 → 2995.86] Or you want to iterate
[2995.86 → 2996.58] on an idea
[2996.58 → 2997.30] and you want to see
[2997.30 → 2998.30] different versions of it.
[2998.48 → 2998.50] Like,
[2998.60 → 3001.34] it's important to be able,
[3001.66 → 3002.74] it's a valuable thing
[3002.74 → 3003.90] to be able to try something out,
[3004.00 → 3004.36] throw it away,
[3004.44 → 3005.00] try something new,
[3005.12 → 3005.50] throw it away,
[3005.58 → 3006.08] try something new,
[3006.16 → 3006.58] throw it away.
[3006.80 → 3007.74] Oh, that one was good.
[3008.44 → 3009.34] Oh, that could have taken me
[3009.34 → 3010.74] three months to figure out before.
[3011.16 → 3011.68] But this way,
[3011.74 → 3012.60] it took me three days.
[3012.86 → 3013.40] Okay, cool.
[3013.58 → 3014.44] Then maybe I delete
[3014.44 → 3015.08] all the code
[3015.08 → 3015.96] and now I write it
[3015.96 → 3017.42] from scratch by hand
[3017.42 → 3018.56] like a Luddite
[3018.56 → 3019.02] or something.
[3019.02 → 3022.14] But that iteration speed
[3022.14 → 3023.48] is still important and useful,
[3023.58 → 3023.96] especially,
[3024.62 → 3024.84] like,
[3025.04 → 3026.26] you're not going to guess
[3026.26 → 3027.76] product market fit right
[3027.76 → 3028.58] on the first one
[3028.58 → 3029.28] for anything,
[3029.36 → 3029.98] basically, right?
[3030.00 → 3030.18] Like,
[3030.22 → 3030.80] if you are,
[3030.90 → 3032.90] then you are also
[3032.90 → 3034.26] already a trillionaire,
[3034.36 → 3034.52] right?
[3034.52 → 3035.50] If you can be the guy
[3035.50 → 3036.16] that always guesses
[3036.16 → 3036.86] everything right
[3036.86 → 3038.24] for PMF,
[3038.60 → 3039.18] congrats.
[3039.44 → 3040.26] Go work at one
[3040.26 → 3040.86] of these countries
[3040.86 → 3041.78] or companies
[3041.78 → 3042.98] and deploy billions
[3042.98 → 3043.94] of dollars of capital
[3043.94 → 3044.62] and, like,
[3044.78 → 3045.66] succeed where others
[3045.66 → 3046.04] have failed.
[3046.46 → 3047.32] That's not me.
[3047.40 → 3047.76] I can't.
[3047.80 → 3048.52] I have to try it
[3048.52 → 3048.98] to find out
[3048.98 → 3049.48] if it's good.
[3050.44 → 3050.84] Correct.
[3051.04 → 3051.18] Hey,
[3051.22 → 3051.92] speaking of that,
[3052.04 → 3052.64] we're going to do,
[3052.84 → 3053.90] because we're way over time,
[3053.96 → 3054.40] but we're going to just
[3054.40 → 3055.18] do one quick thing.
[3055.54 → 3056.20] There's this video
[3056.20 → 3057.12] going around right now
[3057.12 → 3057.56] on the internet.
[3057.74 → 3058.12] Oh, sorry.
[3058.34 → 3059.38] Which is Quake
[3059.38 → 3061.26] and Quake
[3061.26 → 3062.50] was entirely generated
[3062.50 → 3062.94] by an AI.
[3063.06 → 3063.88] This entire gameplay
[3063.88 → 3064.94] that you're seeing right now
[3064.94 → 3065.98] is generated by an AI.
[3066.22 → 3067.64] It was 100% trained
[3067.64 → 3068.62] on only Quake
[3068.62 → 3070.14] and now it made Quake
[3070.14 → 3071.50] and you can run around
[3071.50 → 3073.04] and shoot bad guys,
[3073.14 → 3073.58] sort of.
[3073.88 → 3074.70] If you turn your back
[3074.70 → 3074.94] on them,
[3075.00 → 3076.36] they never existed
[3076.36 → 3077.02] to begin with.
[3077.38 → 3078.08] Sometimes they die
[3078.08 → 3078.60] standing up
[3078.60 → 3079.20] because that's just
[3079.20 → 3079.82] the way it goes.
[3080.30 → 3080.68] But, you know,
[3080.80 → 3082.56] it is all generated.
[3082.92 → 3083.32] So, Casey,
[3083.46 → 3084.12] and speaking kind of
[3084.12 → 3086.06] of this fast iteration cycle,
[3086.26 → 3087.04] hey, we're going to be able
[3087.04 → 3087.78] to move really,
[3087.86 → 3088.42] really quickly.
[3088.74 → 3089.84] Is there something
[3089.84 → 3091.34] to this kind of technology?
[3091.66 → 3092.24] Let's just say
[3092.24 → 3093.64] it can improve by 10x
[3093.64 → 3094.62] or 100x
[3094.62 → 3095.18] or some of these
[3095.18 → 3096.18] big magical numbers.
[3096.56 → 3097.60] Do you think there's actually
[3097.60 → 3098.70] a real value
[3098.70 → 3099.72] in the gaming market
[3099.72 → 3100.72] to be able to see
[3100.72 → 3102.16] or experience features
[3102.16 → 3103.30] not yet created
[3103.30 → 3104.90] as fast as a prompt
[3104.90 → 3105.44] just to see
[3105.44 → 3106.28] if they're good enough?
[3106.28 → 3107.08] Or is being able
[3107.08 → 3107.60] to just simply
[3107.60 → 3108.80] see something play out
[3108.80 → 3111.42] actually real
[3111.42 → 3113.08] to you being able
[3113.08 → 3113.70] to say that's
[3113.70 → 3114.28] a good mechanic?
[3115.50 → 3116.46] So, you know,
[3116.54 → 3117.28] I'm really the wrong
[3117.28 → 3118.02] person to ask.
[3118.12 → 3118.58] That's a very
[3118.58 → 3120.18] designer-specific question.
[3120.56 → 3121.96] And the best I could say
[3121.96 → 3124.24] is in that form,
[3124.62 → 3125.52] like the form
[3125.52 → 3127.48] that they're doing there,
[3127.74 → 3128.72] probably not.
[3128.82 → 3130.22] Because when I've seen
[3130.22 → 3131.20] great designers work,
[3131.22 → 3132.04] and there are very few
[3132.04 → 3132.32] of them,
[3132.38 → 3133.76] but I've had the privilege
[3133.76 → 3135.14] of seeing a couple
[3135.14 → 3135.92] people who I would
[3135.92 → 3136.40] consider to be
[3136.40 → 3137.50] perfect designers
[3137.50 → 3138.80] do what they do.
[3140.34 → 3141.32] There's a lot
[3141.32 → 3142.24] to the specifics.
[3143.14 → 3143.32] You know,
[3143.42 → 3145.50] like a great example
[3145.50 → 3145.88] would be
[3145.88 → 3147.44] go listen to...
[3147.44 → 3147.74] So,
[3148.14 → 3149.14] I mentioned
[3149.14 → 3149.86] Dragon Sweeper
[3149.86 → 3150.34] one time
[3150.34 → 3151.22] on Prime Stream.
[3151.82 → 3152.26] There's a game
[3152.26 → 3153.18] called Dragon Sweeper
[3153.18 → 3154.40] made by Daniel Benmergui.
[3154.70 → 3155.44] He's also the person
[3155.44 → 3156.22] who made Storyteller,
[3156.32 → 3157.10] which was a big hit.
[3157.30 → 3158.40] One of Netflix's top
[3158.40 → 3160.34] games on their service,
[3160.40 → 3160.72] actually.
[3161.94 → 3162.32] Trash.
[3162.32 → 3164.94] And he's
[3164.94 → 3166.02] an excellent designer.
[3166.40 → 3167.96] And you can go listen to...
[3167.96 → 3168.94] There's a podcast of him
[3168.94 → 3169.74] where he describes
[3169.74 → 3171.02] how he designed the game.
[3171.62 → 3172.70] And he designed it
[3172.70 → 3173.22] very quickly.
[3173.34 → 3174.52] It didn't take him
[3174.52 → 3175.08] very long,
[3175.80 → 3177.00] all things considered.
[3177.56 → 3178.60] But the kinds of things
[3178.60 → 3179.46] he had to work out
[3179.46 → 3180.76] to make the game fun
[3180.76 → 3181.90] and to experiment with
[3181.90 → 3182.60] are things like
[3182.60 → 3183.84] exactly what is
[3183.84 → 3184.44] the algorithm
[3184.44 → 3185.26] that determines
[3185.26 → 3186.66] how these pieces
[3186.66 → 3188.06] are placed on the board
[3188.06 → 3188.78] when the board
[3188.78 → 3189.36] is generated
[3189.36 → 3190.22] or things like that.
[3190.96 → 3192.06] And it's just like...
[3192.06 → 3193.32] So a thing that improves
[3193.32 → 3194.48] that iteration time
[3194.48 → 3195.68] would be useful.
[3196.18 → 3196.80] This thing that
[3196.80 → 3198.06] Microsoft is showing
[3198.06 → 3199.78] doesn't seem like
[3199.78 → 3200.72] it gets at that.
[3201.00 → 3201.88] That seems like
[3201.88 → 3203.26] a very superficial thing
[3203.26 → 3203.98] that they've created
[3203.98 → 3205.50] that wouldn't really help
[3205.50 → 3206.34] any real designer
[3206.34 → 3207.36] because we all kind of
[3207.36 → 3208.10] already know
[3208.10 → 3209.34] what that thing...
[3209.34 → 3210.14] Like, if you're just
[3210.14 → 3211.10] training a thing on Quake,
[3211.16 → 3211.42] it's like,
[3211.54 → 3212.44] what are the design changes
[3212.44 → 3213.24] I'm going to make
[3213.24 → 3213.88] that this thing
[3213.88 → 3215.38] can represent properly
[3215.38 → 3216.56] that actually give me
[3216.56 → 3217.20] that information?
[3217.66 → 3218.78] They're the kinds of things
[3218.78 → 3219.40] that we would call
[3219.40 → 3220.54] a producer knob
[3220.54 → 3221.28] in music.
[3221.48 → 3222.28] A thing where you
[3222.28 → 3223.04] just kind of like
[3223.04 → 3224.12] pretend it does something
[3224.12 → 3225.08] so that higher-ups
[3225.08 → 3226.08] can feel like
[3226.08 → 3226.90] they did something.
[3227.26 → 3228.34] That's what this looks like.
[3228.38 → 3229.00] It's like something
[3229.00 → 3229.90] that some higher-up
[3229.90 → 3230.44] could say like,
[3230.66 → 3232.32] make the shield more green
[3232.32 → 3233.64] and the AI doesn't like,
[3233.72 → 3234.32] oh, that was great.
[3234.38 → 3234.94] And everyone's like,
[3235.28 → 3235.96] thanks, Bob.
[3236.00 → 3236.64] You really turned
[3236.64 → 3237.38] the project around.
[3237.48 → 3238.16] And then they go back
[3238.16 → 3239.48] to doing the real work, right?
[3239.96 → 3241.54] So I do think
[3241.54 → 3243.06] there's room for something
[3243.06 → 3244.90] that helps game designers iterate.
[3245.28 → 3247.20] This doesn't look like it to me,
[3247.28 → 3248.72] but if you really wanted to know,
[3248.72 → 3249.50] you'd have to ask
[3249.50 → 3251.00] some of those top-notch game designers.
[3251.14 → 3251.80] Ask Jonathan Blow,
[3251.92 → 3252.74] ask Daniel Ben-Argue,
[3252.88 → 3253.72] ask some of the people
[3253.72 → 3254.78] who are really excellent
[3254.78 → 3255.94] at doing this kind of stuff
[3255.94 → 3257.32] and see what they say
[3257.32 → 3258.10] because I don't really know.
[3259.16 → 3259.36] Okay.
[3259.60 → 3260.60] Yeah, because I think
[3260.60 → 3261.08] one of the things
[3261.08 → 3261.72] that's probably missing
[3261.72 → 3262.36] from this context
[3262.36 → 3263.16] is that they did this
[3263.16 → 3264.12] other kind of release of it,
[3264.18 → 3265.24] which is Copilot for Your Games,
[3265.32 → 3266.08] or I forget what they call it.
[3266.12 → 3266.70] It could have been Copilot.
[3266.80 → 3268.12] We should talk about that.
[3268.20 → 3269.10] That's a completely different thing.
[3269.18 → 3269.46] Okay, well,
[3269.50 → 3269.88] they would just,
[3270.00 → 3270.54] it's the exact,
[3270.66 → 3271.22] well, in my head,
[3271.26 → 3271.94] it's the same thing,
[3271.98 → 3273.40] meaning that it takes your game,
[3273.60 → 3274.24] records thousands,
[3274.24 → 3275.22] you record thousands
[3275.22 → 3276.38] upon thousands of hours
[3278.72 → 3279.32] and then you create
[3279.32 → 3280.06] a model off of it
[3280.06 → 3281.14] that can generate your game
[3281.14 → 3281.82] and then you're supposed
[3281.82 → 3282.96] to add in these features
[3282.96 → 3284.42] by kind of prompt maxing it
[3284.42 → 3285.38] into being like,
[3285.48 → 3286.28] okay, I want my game,
[3286.34 → 3287.56] but I want like surfboards
[3287.56 → 3289.52] and then it makes surfboards
[3289.52 → 3289.72] and you go,
[3289.80 → 3290.78] oh, that wasn't fun looking.
[3290.94 → 3292.44] So then you add in new stuff.
[3292.52 → 3293.76] I think that's the entire goal
[3293.76 → 3294.42] of this project
[3294.42 → 3294.88] because I believe
[3294.88 → 3295.46] this is along
[3295.46 → 3296.44] the exact same veins,
[3296.50 → 3297.80] which is how close
[3297.80 → 3299.80] can they accurately portray your game,
[3300.26 → 3300.90] make it playable
[3300.90 → 3302.38] and then add new features
[3302.38 → 3303.00] that don't require
[3303.00 → 3303.74] any programming,
[3303.86 → 3304.68] just a simple prompt.
[3305.06 → 3307.10] But the Copilot for Games 1
[3307.10 → 3307.66] was the thing
[3307.66 → 3308.90] that's like Clippy for games.
[3309.10 → 3309.68] Like it comes up
[3309.68 → 3310.66] and you ask it like,
[3310.94 → 3312.60] where's the wood on this map?
[3312.64 → 3313.06] And they're like,
[3313.12 → 3314.08] oh, you have to go
[3314.08 → 3314.82] towards the forest
[3314.82 → 3315.48] to the northeast.
[3315.96 → 3316.78] Like, you know what I mean?
[3317.26 → 3318.48] That's what Copilot for Gaming is.
[3318.54 → 3319.40] Copilot has a lot
[3319.40 → 3320.16] of meetings these days.
[3320.52 → 3321.02] There was some
[3321.02 → 3322.50] Microsoft Game Studio AI
[3322.50 → 3323.18] that is doing
[3323.18 → 3323.86] what I'm describing,
[3324.26 → 3325.44] which is thousands of hours
[3325.44 → 3326.26] recording of a game
[3326.26 → 3326.78] and then attempting
[3326.78 → 3327.92] to replay it and show you
[3327.92 → 3329.24] like this is your game
[3329.24 → 3329.72] on features.
[3330.24 → 3331.46] Yes, that was a different thing,
[3331.52 → 3331.78] I think,
[3331.86 → 3332.72] not Copilot for Gaming,
[3332.72 → 3333.64] but I could be wrong about that.
[3333.70 → 3334.26] Maybe you're right.
[3334.40 → 3335.24] Maybe they're both called
[3335.24 → 3336.24] Copilot for some reason.
[3336.24 → 3337.48] I mean, everything's called
[3337.48 → 3338.50] Copilot for Microsoft.
[3339.22 → 3339.84] That's a good point.
[3340.06 → 3340.98] Maybe I've got the name wrong.
[3341.08 → 3342.20] Maybe it had a better name.
[3342.36 → 3342.60] I don't know.
[3342.70 → 3343.06] They probably have three
[3343.06 → 3345.28] separate AI game related things
[3345.28 → 3346.78] called Microsoft Copilot for Games.
[3346.88 → 3347.64] You're probably right.
[3347.70 → 3349.78] Or Copilot for Microsoft Games
[3349.78 → 3351.54] and Games for Copilot Microsoft.
[3351.96 → 3352.52] And so they're all
[3352.52 → 3353.30] different products.
[3354.02 → 3355.74] Yes, by Xbox.
[3356.16 → 3356.74] By Xbox.
[3357.06 → 3357.62] For Windows.
[3357.88 → 3358.92] You need the Game Pass
[3358.92 → 3359.42] for that one.
[3359.42 → 3361.50] As many R as you can in there.
[3361.82 → 3362.00] Yeah.
[3362.78 → 3363.04] Yeah.
[3363.24 → 3363.64] So anyway,
[3363.86 → 3365.36] that's all I could really say
[3365.36 → 3366.30] about this is I don't,
[3366.30 → 3367.30] I don't,
[3367.30 → 3369.24] the specifics are what matter
[3369.24 → 3370.50] to real game designers.
[3371.00 → 3373.16] And so this is just not specifics.
[3373.28 → 3374.86] These are more cosmetic changes
[3374.86 → 3376.24] that they're showing here.
[3376.76 → 3378.30] Things that the game designer
[3378.30 → 3379.02] would not really,
[3379.22 → 3380.12] like they're thinking about
[3380.12 → 3381.50] exactly what will happen
[3381.50 → 3383.00] if I change a little bit
[3383.00 → 3383.94] the way we respond
[3383.94 → 3385.18] to like this mouse down
[3385.18 → 3385.90] or what,
[3385.98 → 3387.06] like that's what they're working at
[3387.06 → 3387.50] to feel,
[3387.58 → 3388.40] to get the feel for it.
[3388.40 → 3389.84] Everyone knows what it looks like
[3389.84 → 3390.94] if you walk around in 3D.
[3391.04 → 3391.68] It's not hard,
[3391.78 → 3393.12] like that's not a prototyping problem
[3393.12 → 3394.48] we've ever had, right?
[3394.70 → 3396.20] Not for a very long time anyway.
[3396.74 → 3397.32] So it just,
[3397.68 → 3398.90] this doesn't seem to be
[3398.90 → 3400.42] at the level of experimentation
[3400.42 → 3401.90] that they're working at typically
[3401.90 → 3403.00] in my experience.
[3403.10 → 3403.48] But again,
[3403.74 → 3404.58] they'd have to say,
[3404.70 → 3405.08] not me.
[3405.16 → 3406.22] I'm the wrong one to answer.
[3406.80 → 3406.98] Okay.
[3407.20 → 3407.44] Okay.
[3407.60 → 3408.78] So maybe in 10 years
[3408.78 → 3410.00] this thing could be outstanding,
[3410.18 → 3412.72] but today from your brief survey
[3412.72 → 3413.70] it does not look like
[3413.70 → 3415.20] it necessarily fulfills the need.
[3415.72 → 3417.26] It might even be outstanding today
[3417.26 → 3418.76] if they were training it more
[3418.76 → 3419.74] on the kinds of things
[3419.74 → 3420.30] that game designers
[3420.30 → 3421.42] actually work on,
[3421.90 → 3422.10] right?
[3422.70 → 3423.06] Like,
[3423.14 → 3424.34] like the nitty-gritty,
[3424.66 → 3424.84] right?
[3424.90 → 3425.18] Like I said,
[3425.22 → 3426.46] this is very producer,
[3426.52 → 3426.64] right?
[3426.64 → 3427.38] It's very much like
[3427.38 → 3428.44] should the Valkyrie
[3428.44 → 3429.80] jump a little higher
[3429.80 → 3431.32] because it looks cool
[3431.32 → 3431.66] or something,
[3431.72 → 3431.86] right?
[3431.86 → 3433.26] Like it's not asking
[3433.26 → 3434.02] the question of like
[3434.02 → 3434.98] what are the gameplay
[3434.98 → 3436.46] ramifications of the Valkyrie?
[3436.52 → 3437.18] It can't tell you,
[3437.52 → 3437.64] oh,
[3437.78 → 3438.84] the Valkyrie will win
[3438.84 → 3440.04] 20% more matches,
[3440.18 → 3440.96] right?
[3441.04 → 3441.16] Yeah.
[3441.50 → 3442.42] And that is the thing
[3442.42 → 3443.34] that they could be doing.
[3443.50 → 3443.62] So,
[3443.74 → 3444.02] you know,
[3444.02 → 3445.84] maybe they'll get there soon
[3445.84 → 3446.88] because that's the kind of thing,
[3446.94 → 3447.10] you know,
[3447.14 → 3448.98] just we analyze billions of matches
[3448.98 → 3450.08] and then we can tell you like,
[3450.16 → 3450.26] oh,
[3450.28 → 3450.58] if you,
[3450.70 → 3452.14] if you change the way
[3452.14 → 3453.40] that this thing worked
[3453.40 → 3454.54] where you have to go to,
[3454.70 → 3454.84] you know,
[3454.84 → 3455.56] the Black King bar
[3455.56 → 3457.54] costs 20% more at the shop,
[3457.68 → 3459.66] it has this effect on games.
[3459.84 → 3460.38] That's something
[3460.38 → 3461.36] that game designers could use
[3461.36 → 3462.90] for like balancing multiplayer,
[3463.06 → 3463.52] like let's say,
[3463.58 → 3463.74] right?
[3464.08 → 3465.64] This doesn't look like that to me.
[3466.00 → 3466.68] Maybe it does that
[3466.68 → 3467.64] and they just didn't say so,
[3467.72 → 3468.60] but it doesn't look like
[3468.60 → 3469.58] it could do those sorts of things.
[3469.58 → 3470.28] I could be wrong.
[3471.04 → 3471.40] Okay.
[3471.66 → 3472.82] Was that a Data 2 reference?
[3472.82 → 3473.76] It was.
[3474.04 → 3474.32] Oh,
[3474.44 → 3474.88] sick.
[3474.96 → 3475.24] All right.
[3475.46 → 3475.72] Nice.
[3476.56 → 3476.92] Nice.
[3477.26 → 3478.44] That's TJ's love language.
[3478.54 → 3478.64] Dude,
[3478.66 → 3479.42] we can go play that
[3479.42 → 3480.20] at burn convention.
[3480.34 → 3480.88] That'll be sick.
[3481.00 → 3481.74] At burn convention.
[3481.92 → 3482.78] We'll all bank Roche.
[3482.88 → 3484.06] That's the only term I know.
[3484.40 → 3484.66] Sweet.
[3484.76 → 3484.94] Okay,
[3485.00 → 3485.18] cool.
[3485.46 → 3485.62] Yeah.
[3485.76 → 3486.28] That's all I needed.
[3486.52 → 3486.98] All right.
[3487.34 → 3488.84] We'll make no further jokes
[3488.84 → 3489.84] about the burn convention
[3489.84 → 3490.92] because they're all inappropriate
[3490.92 → 3491.58] from here on out.
[3491.58 → 3492.28] Remember that React Miami?
[3493.12 → 3494.44] The whole like Burning Man thing
[3494.44 → 3495.16] kept coming up.
[3495.84 → 3497.62] React Miami is the burn convention
[3497.62 → 3499.76] of developer conferences.
[3500.44 → 3500.84] Josh,
[3500.84 → 3501.38] I don't even know
[3501.38 → 3502.08] what you're talking about.
[3502.08 → 3502.82] You know what?
[3502.96 → 3503.28] Okay,
[3503.48 → 3503.90] never mind.
[3503.92 → 3505.76] It was funny a year ago.
[3505.78 → 3507.08] Stop talking about
[3507.08 → 3508.04] burn convention.
[3509.72 → 3510.28] All right.
[3510.32 → 3510.52] Hey,
[3511.56 → 3512.42] what's your sign-off?
[3514.58 → 3514.98] Who?
[3516.06 → 3516.50] Anybody.
[3516.66 → 3517.32] You got to start going.
[3517.48 → 3517.62] My name's CJ.
[3517.94 → 3518.88] My name's Casey.
[3519.30 → 3519.70] Trash.
[3520.08 → 3520.66] And the name.
[3521.34 → 3521.92] Take us away,
[3522.06 → 3522.12] Prime.
[3522.64 → 3523.56] Why would you just
[3523.56 → 3524.26] interrupt me like that?
[3524.30 → 3524.80] It was coming out.
[3524.92 → 3525.12] Dude,
[3525.18 → 3525.88] that was so good.
[3525.96 → 3526.30] It was like,
[3526.36 → 3526.64] bam,
[3526.70 → 3526.96] bam,
[3527.02 → 3527.30] bam.
[3527.30 → 3528.14] I was coming in
[3528.14 → 3529.14] and it ruined everything.
[3529.14 → 3529.20] It ruined everything.
[3529.42 → 3529.98] To take that out.
[3530.04 → 3531.30] If you're here watching the video now,
[3531.40 → 3532.04] Flip took it out.
[3532.40 → 3533.18] Flip took it out.
[3533.34 → 3534.00] It did not exist.
[3534.70 → 3535.68] Join us next week
[3535.68 → 3537.04] for another predictable disaster
[3537.04 → 3538.20] here on the standup.
[3538.24 → 3538.80] Next week,
[3538.86 → 3540.36] we have a great topic
[3540.36 → 3541.12] which you have suggested
[3541.12 → 3541.96] in the docs, Casey.
[3542.18 → 3542.50] Yes,
[3542.54 → 3543.06] we use docs,
[3543.14 → 3543.44] by the way.
[3543.54 → 3544.08] We're the standup.
[3544.14 → 3544.74] I actually wanted to,
[3544.82 → 3545.68] I think that's just hilarious
[3545.68 → 3546.54] because I think I have
[3546.54 → 3547.64] like an internal itch
[3547.64 → 3549.14] that one day,
[3549.26 → 3550.00] if I ever was forced
[3550.00 → 3550.94] to work for an evil company,
[3550.94 → 3552.12] I would do the exact same thing.
[3552.68 → 3553.34] And so it's just like,
[3553.40 → 3554.28] I have always wanted the
[3554.46 → 3555.30] yeah.
[3556.20 → 3556.56] Anyway.
[3556.70 → 3556.84] Okay.
[3557.00 → 3557.32] What?
[3557.56 → 3557.78] Yeah.
[3558.04 → 3559.04] Read the docs, loser.
[3559.44 → 3559.76] We'll scratch that itch
[3559.76 → 3560.20] next Wednesday.
[3560.72 → 3562.00] The name is the foreshadow agenda.
